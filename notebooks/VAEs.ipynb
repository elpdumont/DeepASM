{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "\n",
        "# Python packages for data, stats\n",
        "import numpy as np\n",
        "import ast\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from dask import delayed\n",
        "import pandas as pd\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Setting the random seed for various libraries\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "bq_client = bigquery.Client()\n",
        "storage_client = storage.Client()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "kpAe30-2HnoS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716150749685,
          "user_tz": 240,
          "elapsed": 1161,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "kpAe30-2HnoS",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\"TRAINING\", \"VALIDATION\", \"TESTING\", \"TRAINING_AND_VALIDATION\"]\n",
        "\n",
        "project = \"hmh-em-deepasm\"\n",
        "samples_dataset = \"samples_250bp\"\n",
        "ml_dataset = \"ml_250bp_db7e6e4\"\n",
        "ml_mode = 'TESTING'\n",
        "ml_nb_datapoints_for_testing = 200000\n",
        "metrics = ['asm', 'asm_not_corrected']\n",
        "bucket = \"hmh_deepasm\"\n",
        "label_var = 'asm_not_corrected'\n",
        "padding_value = 0\n",
        "batch_size = 1024\n",
        "\n",
        "# Make sure this is the same as in the config file\n",
        "samples_dic = {'TRAINING': ['gm12878',\n",
        "                                  'CD14',\n",
        "                                  'fibroblast',\n",
        "                                  'A549',\n",
        "                                  'spleen_female_adult',\n",
        "                                  'HeLa_S3'],\n",
        "            'VALIDATION': ['mammary_epithelial',\n",
        "                                       'sk_n_sh',\n",
        "                                       'CD34'],\n",
        "            'TESTING': ['HepG2',\n",
        "                                 'righ_lobe_liver',\n",
        "                                 't_cell_male_adult']}\n",
        "\n",
        "dataset_types = [\"TRAINING\", \"VALIDATION\", \"TESTING\"]\n"
      ],
      "metadata": {
        "id": "-4TqE6r2HhJe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716151888913,
          "user_tz": 240,
          "elapsed": 143,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "-4TqE6r2HhJe",
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_classes(dic_data, device):\n",
        "    class_weight_dic = {\"TRAINING\": {}, \"TRAINING_AND_VALIDATION\": {}}\n",
        "    for data in [\"TRAINING\", \"TRAINING_AND_VALIDATION\"]:\n",
        "        labels = dic_data[data][\"labels\"]\n",
        "        weights = np.round(\n",
        "            compute_class_weight(\"balanced\", classes=[0, 1], y=labels), 2\n",
        "        )\n",
        "        class_weight_dic[data][\"class_weight\"] = {0: weights[0], 1: weights[1]}\n",
        "        scale_pos_weight = len(labels[labels == 0]) / len(labels[labels == 1])\n",
        "        class_weight_dic[data][\"scale_pos_weight\"] = scale_pos_weight\n",
        "        class_weight_dic[data][\"weight_tensor\"] = torch.tensor(\n",
        "            weights, dtype=torch.float32\n",
        "        ).to(device)\n",
        "    return class_weight_dic\n",
        "\n",
        "def evaluate_model(labels, predictions):\n",
        "  confusion = confusion_matrix(labels, predictions)\n",
        "  report = classification_report(labels, predictions, output_dict=True)\n",
        "  sum_f1 = np.round(report['0.0']['f1-score'] + report['1.0']['f1-score'],3)\n",
        "  report = classification_report(labels, predictions, output_dict=False)\n",
        "  report = classification_report(labels, predictions, digits = 3)\n",
        "  return sum_f1, confusion, report"
      ],
      "metadata": {
        "id": "RZZQq5P3IBMO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716151890406,
          "user_tz": 240,
          "elapsed": 182,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "RZZQq5P3IBMO",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic_data = {\n",
        "        **{dataset: {} for dataset in dataset_types},\n",
        "        **{\"TRAINING_AND_VALIDATION\": {}},\n",
        "    }\n",
        "\n",
        "max_sequence_length = 0\n",
        "for dataset in dataset_types:\n",
        "    print(f\"Processing {dataset} dataset...\")\n",
        "    quoted_samples = \",\".join([f\"'{sample}'\" for sample in samples_dic[dataset]])\n",
        "    print(f\"Importing the samples: {quoted_samples}\")\n",
        "    query = f\"\"\"\n",
        "        SELECT * EXCEPT (region_sup, clustering_index, region_nb_cpg, cpgs_w_padding)\n",
        "        FROM {project}.{ml_dataset}.features_wo_hmm\n",
        "        WHERE\n",
        "            cpg_directional_fm IS NOT NULL AND\n",
        "            {label_var} IS NOT NULL AND\n",
        "            sample IN ({quoted_samples})\n",
        "        \"\"\"\n",
        "    if ml_mode == \"TESTING\":\n",
        "        print(\"In testing mode. Adding a limit to the import.\")\n",
        "        query += f\"LIMIT {ml_nb_datapoints_for_testing}\"\n",
        "    df = bq_client.query(query).to_dataframe()\n",
        "    print(f\"Number of rows in DF: {len(df):,}\")\n",
        "    dic_data[dataset][\"labels\"] = df[label_var].astype(int)\n",
        "    dic_data[dataset][\"region_info\"] = df[\n",
        "        [\"asm\", \"sample\", \"chr\", \"region_inf\", \"nb_cpg_found\", \"nb_reads\"]\n",
        "    ]\n",
        "    dic_data[dataset][\"1d_seq\"] = df[\"cpg_directional_fm\"].apply(\n",
        "        lambda x: ast.literal_eval(x.strip('\"'))\n",
        "    )\n",
        "    current_max_sequence_length = max(dic_data[dataset][\"1d_seq\"].apply(len))\n",
        "    print(\n",
        "        f\"Max sequence length in the dataset: {current_max_sequence_length}\"\n",
        "    )\n",
        "    if current_max_sequence_length > max_sequence_length:\n",
        "        max_sequence_length = current_max_sequence_length\n",
        "\n",
        "for data in [\"labels\", \"region_info\", \"1d_seq\"]:  # '1d_seq', '2d_seq',\n",
        "    dic_data[\"TRAINING_AND_VALIDATION\"][data] = pd.concat(\n",
        "        [dic_data[\"TRAINING\"][data], dic_data[\"VALIDATION\"][data]]\n",
        "    )\n",
        "\n",
        "print(\"Adding a dummy row of variables to adjust the max sequence length\")\n",
        "zero_sequence = [0] * max_sequence_length  # This is the sequence to append\n",
        "for dataset in dic_data.keys():\n",
        "    # Append a new sequence of zeros to the '1d_seq' array\n",
        "    dic_data[dataset][\"1d_seq\"] = pd.concat(\n",
        "        [dic_data[dataset][\"1d_seq\"], pd.Series([zero_sequence])], ignore_index=True\n",
        "    )\n",
        "    # Assuming you need to add a corresponding new label\n",
        "    dic_data[dataset][\"labels\"] = pd.concat(\n",
        "        [dic_data[dataset][\"labels\"], pd.Series([0])], ignore_index=True\n",
        "    )\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing Data Loader for: {dataset}\")\n",
        "    sequences = [list(row) for row in dic_data[dataset][\"1d_seq\"]]\n",
        "    labels = [int(label) for label in dic_data[dataset][\"labels\"]]\n",
        "    print(f\"Unique labels: {np.unique(labels)}\")\n",
        "    # Convert sequences to tensors and pad them\n",
        "    padded_sequences = torch.nn.utils.rnn.pad_sequence(\n",
        "        [torch.tensor(s) for s in sequences],\n",
        "        batch_first=True,\n",
        "        padding_value=padding_value,\n",
        "    )\n",
        "    #padded_sequences = padded_sequences.squeeze(-1)\n",
        "    print(f\"Shape of the Tensor for the data: {padded_sequences.shape}\")\n",
        "    # Convert labels to a tensor\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    print(f\"Shape of the Tensor for the labels: {labels.shape}\")\n",
        "    # Create DataLoader for batch processing\n",
        "    data = TensorDataset(padded_sequences, labels)\n",
        "    dic_data[dataset][\"dataloader\"] = DataLoader(\n",
        "        data, batch_size=batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "print(\"Computing class weights\")\n",
        "class_weight_dic = compute_classes(dic_data, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ket-TOdVIoHa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716151953180,
          "user_tz": 240,
          "elapsed": 61626,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b7d19b90-04cc-4c36-a8f9-d58859f34600"
      },
      "id": "ket-TOdVIoHa",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TRAINING dataset...\n",
            "Importing the samples: 'gm12878','CD14','fibroblast','A549','spleen_female_adult','HeLa_S3'\n",
            "In testing mode. Adding a limit to the import.\n",
            "Number of rows in DF: 200,000\n",
            "Max sequence length in the dataset: 56\n",
            "Processing VALIDATION dataset...\n",
            "Importing the samples: 'mammary_epithelial','sk_n_sh','CD34'\n",
            "In testing mode. Adding a limit to the import.\n",
            "Number of rows in DF: 200,000\n",
            "Max sequence length in the dataset: 57\n",
            "Processing TESTING dataset...\n",
            "Importing the samples: 'HepG2','righ_lobe_liver','t_cell_male_adult'\n",
            "In testing mode. Adding a limit to the import.\n",
            "Number of rows in DF: 200,000\n",
            "Max sequence length in the dataset: 56\n",
            "Adding a dummy row of variables to adjust the max sequence length\n",
            "Processing Data Loader for: TRAINING\n",
            "Unique labels: [0 1]\n",
            "Shape of the Tensor for the data: torch.Size([200001, 57])\n",
            "Shape of the Tensor for the labels: torch.Size([200001])\n",
            "Processing Data Loader for: VALIDATION\n",
            "Unique labels: [0 1]\n",
            "Shape of the Tensor for the data: torch.Size([200001, 57])\n",
            "Shape of the Tensor for the labels: torch.Size([200001])\n",
            "Processing Data Loader for: TESTING\n",
            "Unique labels: [0 1]\n",
            "Shape of the Tensor for the data: torch.Size([200001, 57])\n",
            "Shape of the Tensor for the labels: torch.Size([200001])\n",
            "Processing Data Loader for: TRAINING_AND_VALIDATION\n",
            "Unique labels: [0 1]\n",
            "Shape of the Tensor for the data: torch.Size([400001, 57])\n",
            "Shape of the Tensor for the labels: torch.Size([400001])\n",
            "Computing class weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "rR44DVgRJnbV1QKDw5nm3CvN",
      "metadata": {
        "tags": [],
        "id": "rR44DVgRJnbV1QKDw5nm3CvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716155879389,
          "user_tz": 240,
          "elapsed": 591697,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c737e162-83e9-47a6-d10e-76ac41f45328"
      },
      "source": [
        "# Hyperparameters\n",
        "input_dim = max_sequence_length\n",
        "hidden_dim = 2000\n",
        "latent_dim = 50\n",
        "num_epochs = 60\n",
        "\n",
        "\n",
        "# class VAE(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "#         super(VAE, self).__init__()\n",
        "\n",
        "#         # Encoder layers\n",
        "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "#         self.fc_mu = nn.Linear(hidden_dim, latent_dim)  # Mean of the latent space\n",
        "#         self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # Log variance of the latent space\n",
        "\n",
        "#         # Decoder layers\n",
        "#         self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "#         self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "#     def encode(self, x):\n",
        "#         h1 = F.relu(self.fc1(x))\n",
        "#         return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "#     def reparameterize(self, mu, logvar):\n",
        "#         std = torch.exp(0.5 * logvar)\n",
        "#         eps = torch.randn_like(std)\n",
        "#         return mu + eps * std\n",
        "\n",
        "#     def decode(self, z):\n",
        "#         h3 = F.relu(self.fc3(z))\n",
        "#         return self.fc4(h3)  # Raw logits, no sigmoid\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         mu, logvar = self.encode(x)\n",
        "#         z = self.reparameterize(mu, logvar)\n",
        "#         return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout_rate=0.5):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)  # Mean of the latent space\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)  # Log variance of the latent space\n",
        "\n",
        "        # Decoder layers\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return self.fc4(h3)  # Raw logits, no sigmoid\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def get_latent_representation(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        return self.reparameterize(mu, logvar)\n",
        "\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy_with_logits(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "def train(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(dataloader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    return train_loss / len(dataloader.dataset)\n",
        "\n",
        "# Sample usage (assuming `dataset` is your dataset)\n",
        "dataloader = dic_data['TRAINING'][\"dataloader\"]\n",
        "model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=0.005,\n",
        "            weight_decay=1e-3,\n",
        "        )\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    train_loss = train(model, dataloader, optimizer)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}')\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "Epoch 1, Loss: 3.6885\n",
            "epoch: 1\n",
            "Epoch 2, Loss: 3.3176\n",
            "epoch: 2\n",
            "Epoch 3, Loss: 3.3181\n",
            "epoch: 3\n",
            "Epoch 4, Loss: 3.3222\n",
            "epoch: 4\n",
            "Epoch 5, Loss: 3.3206\n",
            "epoch: 5\n",
            "Epoch 6, Loss: 3.3201\n",
            "epoch: 6\n",
            "Epoch 7, Loss: 3.3210\n",
            "epoch: 7\n",
            "Epoch 8, Loss: 3.3189\n",
            "epoch: 8\n",
            "Epoch 9, Loss: 3.3184\n",
            "epoch: 9\n",
            "Epoch 10, Loss: 3.3180\n",
            "epoch: 10\n",
            "Epoch 11, Loss: 3.3181\n",
            "epoch: 11\n",
            "Epoch 12, Loss: 3.3151\n",
            "epoch: 12\n",
            "Epoch 13, Loss: 3.3142\n",
            "epoch: 13\n",
            "Epoch 14, Loss: 3.3128\n",
            "epoch: 14\n",
            "Epoch 15, Loss: 3.3109\n",
            "epoch: 15\n",
            "Epoch 16, Loss: 3.3124\n",
            "epoch: 16\n",
            "Epoch 17, Loss: 3.3084\n",
            "epoch: 17\n",
            "Epoch 18, Loss: 3.3083\n",
            "epoch: 18\n",
            "Epoch 19, Loss: 3.3058\n",
            "epoch: 19\n",
            "Epoch 20, Loss: 3.3063\n",
            "epoch: 20\n",
            "Epoch 21, Loss: 3.3061\n",
            "epoch: 21\n",
            "Epoch 22, Loss: 3.3035\n",
            "epoch: 22\n",
            "Epoch 23, Loss: 3.3014\n",
            "epoch: 23\n",
            "Epoch 24, Loss: 3.3037\n",
            "epoch: 24\n",
            "Epoch 25, Loss: 3.3013\n",
            "epoch: 25\n",
            "Epoch 26, Loss: 3.2979\n",
            "epoch: 26\n",
            "Epoch 27, Loss: 3.2955\n",
            "epoch: 27\n",
            "Epoch 28, Loss: 3.2955\n",
            "epoch: 28\n",
            "Epoch 29, Loss: 3.2923\n",
            "epoch: 29\n",
            "Epoch 30, Loss: 3.2949\n",
            "epoch: 30\n",
            "Epoch 31, Loss: 3.2930\n",
            "epoch: 31\n",
            "Epoch 32, Loss: 3.2933\n",
            "epoch: 32\n",
            "Epoch 33, Loss: 3.2919\n",
            "epoch: 33\n",
            "Epoch 34, Loss: 3.2917\n",
            "epoch: 34\n",
            "Epoch 35, Loss: 3.2915\n",
            "epoch: 35\n",
            "Epoch 36, Loss: 3.2916\n",
            "epoch: 36\n",
            "Epoch 37, Loss: 3.2910\n",
            "epoch: 37\n",
            "Epoch 38, Loss: 3.2903\n",
            "epoch: 38\n",
            "Epoch 39, Loss: 3.2901\n",
            "epoch: 39\n",
            "Epoch 40, Loss: 3.2912\n",
            "epoch: 40\n",
            "Epoch 41, Loss: 3.2879\n",
            "epoch: 41\n",
            "Epoch 42, Loss: 3.2893\n",
            "epoch: 42\n",
            "Epoch 43, Loss: 3.2890\n",
            "epoch: 43\n",
            "Epoch 44, Loss: 3.2902\n",
            "epoch: 44\n",
            "Epoch 45, Loss: 3.2894\n",
            "epoch: 45\n",
            "Epoch 46, Loss: 3.2898\n",
            "epoch: 46\n",
            "Epoch 47, Loss: 3.2891\n",
            "epoch: 47\n",
            "Epoch 48, Loss: 3.2899\n",
            "epoch: 48\n",
            "Epoch 49, Loss: 3.2889\n",
            "epoch: 49\n",
            "Epoch 50, Loss: 3.2882\n",
            "epoch: 50\n",
            "Epoch 51, Loss: 3.2893\n",
            "epoch: 51\n",
            "Epoch 52, Loss: 3.2875\n",
            "epoch: 52\n",
            "Epoch 53, Loss: 3.2885\n",
            "epoch: 53\n",
            "Epoch 54, Loss: 3.2880\n",
            "epoch: 54\n",
            "Epoch 55, Loss: 3.2880\n",
            "epoch: 55\n",
            "Epoch 56, Loss: 3.2873\n",
            "epoch: 56\n",
            "Epoch 57, Loss: 3.2873\n",
            "epoch: 57\n",
            "Epoch 58, Loss: 3.2889\n",
            "epoch: 58\n",
            "Epoch 59, Loss: 3.2884\n",
            "epoch: 59\n",
            "Epoch 60, Loss: 3.2882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_latent_representations(model, dataloader):\n",
        "#     model.eval()\n",
        "#     latent_vectors = []\n",
        "#     labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for data, target in dataloader:\n",
        "#             data = data.to(device)\n",
        "#             target = target.to(device)\n",
        "#             mu, _ = model.encode(data)\n",
        "#             latent_vectors.append(mu.cpu())\n",
        "#             labels.append(target.cpu())\n",
        "#     return torch.cat(latent_vectors), torch.cat(labels)\n",
        "\n",
        "# def extract_latent_vectors(model, dataloader):\n",
        "#   # Returns tensors\n",
        "#     model.eval()\n",
        "#     latent_vectors = []\n",
        "#     labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for data, target in dataloader:\n",
        "#             data = data.to(device)\n",
        "#             latent_vector = model.get_latent_representation(data)\n",
        "#             latent_vectors.append(latent_vector.cpu())\n",
        "#             labels.append(target.cpu())\n",
        "#     return torch.cat(latent_vectors), torch.cat(labels)\n",
        "\n",
        "def extract_latent_vectors(model, dataloader):\n",
        "    model.eval()\n",
        "    latent_vectors = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data = data.to(device)\n",
        "            latent_vector = model.get_latent_representation(data)\n",
        "            latent_vectors.append(latent_vector.cpu().numpy())\n",
        "            labels.append(target.cpu().numpy())\n",
        "\n",
        "    latent_vectors_np = np.concatenate(latent_vectors, axis=0)\n",
        "    labels_np = np.concatenate(labels, axis=0)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_latent_vectors = pd.DataFrame(latent_vectors_np)\n",
        "    df_latent_vectors['label'] = labels_np\n",
        "\n",
        "    return df_latent_vectors\n",
        "\n",
        "# Assuming dataloader is the DataLoader for your entire dataset\n",
        "latent_vectors_training = extract_latent_vectors(model, dic_data['TRAINING_AND_VALIDATION'][\"dataloader\"])\n",
        "latent_vectors_testing = extract_latent_vectors(model, dic_data['TESTING'][\"dataloader\"])"
      ],
      "metadata": {
        "id": "M65u_yT8OoIn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716155890688,
          "user_tz": 240,
          "elapsed": 11307,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "M65u_yT8OoIn",
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(labels, predictions):\n",
        "  confusion = confusion_matrix(labels, predictions)\n",
        "  report = classification_report(labels, predictions, output_dict=True)\n",
        "  sum_f1 = np.round(report['0']['f1-score'] + report['1']['f1-score'],3)\n",
        "  report = classification_report(labels, predictions, output_dict=False)\n",
        "  report = classification_report(labels, predictions, digits = 3)\n",
        "  return sum_f1, confusion, report"
      ],
      "metadata": {
        "id": "8g67k9UZf5-4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716154294328,
          "user_tz": 240,
          "elapsed": 9,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "8g67k9UZf5-4",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#var_for_training = 'hs_features'\n",
        "params = {\n",
        "    'max_depth': 100,\n",
        "    'eta': 0.001, # learning rate\n",
        "    'objective': 'binary:logistic',\n",
        "    'nthread': 8,\n",
        "    'eval_metric': ['logloss'],\n",
        "    'subsample': 0.5,\n",
        "    #'scale_pos_weight': 0.02  # Adjust based on your exact dataset\n",
        "}\n",
        "\n",
        "train_labels = latent_vectors_training['label'].astype(int)\n",
        "scale_pos_weight = len(train_labels[train_labels == 0]) / len(train_labels[train_labels == 1])\n",
        "print(f\"Scale pos weight: {scale_pos_weight}\")\n",
        "\n",
        "model_w_params = XGBClassifier(scale_pos_weight=scale_pos_weight, **params)\n",
        "model_w_params.fit(latent_vectors_training.drop(columns=['label']), train_labels)\n",
        "\n",
        "labels = latent_vectors_testing['label'].astype(int)\n",
        "predictions = model_w_params.predict(latent_vectors_testing.drop(columns=['label']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlwpjjhkjYQq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716155932592,
          "user_tz": 240,
          "elapsed": 41906,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c27b05c7-d4ff-4878-9e48-b34926c1866e"
      },
      "id": "IlwpjjhkjYQq",
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scale pos weight: 90.86977491961414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#sum_f1, confusion, report = evaluate_model(np.array(labels), predictions)\n",
        "#print(f\"SUm of F1s: {sum_f1}\\nConfusion matrix: {confusion}\\nReport:{report}\")\n",
        "\n",
        "confusion = confusion_matrix(np.array(labels), predictions)\n",
        "report = classification_report(np.array(labels), predictions, output_dict=True)\n",
        "\n",
        "print(confusion)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYTrUxLits0G",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716155932592,
          "user_tz": 240,
          "elapsed": 10,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a245015d-ff19-4f69-d368-2f92ba4cb614"
      },
      "id": "KYTrUxLits0G",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[196105    220]\n",
            " [  3652     24]]\n",
            "{'0': {'precision': 0.9817177871113403, 'recall': 0.9988794091430027, 'f1-score': 0.9902242464944128, 'support': 196325}, '1': {'precision': 0.09836065573770492, 'recall': 0.006528835690968444, 'f1-score': 0.012244897959183673, 'support': 3676}, 'accuracy': 0.980640096799516, 'macro avg': {'precision': 0.5400392214245227, 'recall': 0.5027041224169856, 'f1-score': 0.5012345722267982, 'support': 200001}, 'weighted avg': {'precision': 0.9654817642168074, 'recall': 0.980640096799516, 'f1-score': 0.972249075944188, 'support': 200001}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum_f1 = np.round(report['0']['f1-score'] + report['1']['f1-score'],3)\n",
        "print(sum_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1wNlczwxrl7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716155950099,
          "user_tz": 240,
          "elapsed": 161,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c2eac40b-c76d-4862-ab36-1b1b4fba0402"
      },
      "id": "X1wNlczwxrl7",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 100)\n",
        "        self.fc2 = nn.Linear(100, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Hyperparameters for the classifier\n",
        "num_classes = 2  # Assuming binary classification\n",
        "classifier_input_dim = latent_dim  # Latent dimension from VAE\n",
        "classifier_learning_rate = 1e-3\n",
        "classifier_num_epochs = 10\n",
        "\n",
        "classifier = Classifier(classifier_input_dim, num_classes).to(device)\n",
        "classifier_optimizer = torch.optim.AdamW(\n",
        "            classifier.parameters(),\n",
        "            lr=0.001,\n",
        "            weight_decay=1e-3,\n",
        "        )\n",
        "criterion = nn.BCEWithLogitsLoss(\n",
        "        pos_weight=class_weight_dic[\"TRAINING_AND_VALIDATION\"][\"weight_tensor\"][1]\n",
        "    )\n",
        "\n",
        "# Convert latent vectors and labels to DataLoader\n",
        "latent_dataset = TensorDataset(latent_vectors, labels)\n",
        "latent_dataloader = DataLoader(latent_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training loop for the classifier\n",
        "def train_classifier(classifier, dataloader, optimizer, criterion):\n",
        "    classifier.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device).long()\n",
        "        optimizer.zero_grad()\n",
        "        output = classifier(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    return train_loss / len(dataloader.dataset)\n",
        "\n",
        "for epoch in range(classifier_num_epochs):\n",
        "    train_loss = train_classifier(classifier, latent_dataloader, classifier_optimizer, criterion)\n",
        "    print(f'Classifier Epoch {epoch+1}, Loss: {train_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "OV8WsYbpOrCY",
        "executionInfo": {
          "status": "error",
          "timestamp": 1716147336398,
          "user_tz": 240,
          "elapsed": 168,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "89e2e4b2-0d2f-42e5-8f39-89b00ab739b1"
      },
      "id": "OV8WsYbpOrCY",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target size (torch.Size([1024])) must be the same as input size (torch.Size([1024, 2]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-2c94f2f4c920>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Classifier Epoch {epoch+1}, Loss: {train_loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-2c94f2f4c920>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(classifier, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    726\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1024])) must be the same as input size (torch.Size([1024, 2]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sumf1, report, confusion = evaluate_model(\n",
        "        classifier, dic_data[\"TESTING\"][\"dataloader\"], criterion, device\n",
        "    )"
      ],
      "metadata": {
        "id": "DxVraPeyP87y"
      },
      "id": "DxVraPeyP87y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "VAEs"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}