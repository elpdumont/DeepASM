{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "scof3hgF43Tp",
      "metadata": {
        "id": "scof3hgF43Tp"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8BRDOlmpe7tT1CXQO9WnnLdz",
      "metadata": {
        "id": "8BRDOlmpe7tT1CXQO9WnnLdz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import ast\n",
        "\n",
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import sklearn\n",
        "import xgboost\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SPh5MmLo55L_",
      "metadata": {
        "id": "SPh5MmLo55L_"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "YlWTer6N4mq8",
      "metadata": {
        "id": "YlWTer6N4mq8"
      },
      "outputs": [],
      "source": [
        "# GCP authorization\n",
        "bq_client = bigquery.Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "QUrXGc2I4tQj",
      "metadata": {
        "id": "QUrXGc2I4tQj"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "PROJECT_ID = \"hmh-em-deepasm\"\n",
        "BQ_ML_DATASET = \"ml_250bp_3\" # Ali: hg19_250_ml #ml_250bp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y9r-GShN4ztj",
      "metadata": {
        "id": "y9r-GShN4ztj"
      },
      "outputs": [],
      "source": [
        "# dic_data = {'train': {'samples': ['gm12878',\n",
        "#                                   'CD14',\n",
        "#                                   'fibroblast',\n",
        "#                                   'A549',\n",
        "#                                   'spleen_female_adult',\n",
        "#                                   'HeLa_S3']},\n",
        "#             'validation': {'samples': ['mammary_epithelial',\n",
        "#                                        'sk_n_sh',\n",
        "#                                        'CD34']},\n",
        "#             'test': {'samples': ['HepG2',\n",
        "#                                  'righ_lobe_liver',\n",
        "#                                  't_cell_male_adult']}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "mlzLMBUX42RY",
      "metadata": {
        "id": "mlzLMBUX42RY"
      },
      "outputs": [],
      "source": [
        "query = f\"SELECT * FROM {PROJECT_ID}.{BQ_ML_DATASET}.TRAINING WHERE cpg_directional_fm IS NOT NULL AND asm IS NOT NULL\"\n",
        "\n",
        "# Execute Query and store as DF\n",
        "df_train = bq_client.query(query).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "NriQW0UX46Rm",
      "metadata": {
        "id": "NriQW0UX46Rm"
      },
      "outputs": [],
      "source": [
        "query = f\"SELECT * FROM {PROJECT_ID}.{BQ_ML_DATASET}.VALIDATION WHERE cpg_directional_fm IS NOT NULL AND asm IS NOT NULL\"\n",
        "\n",
        "# Execute Query and store as DF\n",
        "df_validation = bq_client.query(query).to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Q0kkaziD515U",
      "metadata": {
        "id": "Q0kkaziD515U"
      },
      "outputs": [],
      "source": [
        "query = f\"SELECT * FROM {PROJECT_ID}.{BQ_ML_DATASET}.TESTING WHERE cpg_directional_fm IS NOT NULL AND asm IS NOT NULL\"\n",
        "\n",
        "# Execute Query and store as DF\n",
        "df_test = bq_client.query(query).to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Njon6oXF6v0W",
      "metadata": {
        "id": "Njon6oXF6v0W"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cMSkpWkh6xh6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "cMSkpWkh6xh6",
        "outputId": "f124a9fe-c388-4b9c-a7c4-768721411a8b"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trh7lCH27Ul6",
      "metadata": {
        "id": "trh7lCH27Ul6"
      },
      "outputs": [],
      "source": [
        "df_train.columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "auwkniO8CE8b",
      "metadata": {
        "id": "auwkniO8CE8b"
      },
      "source": [
        "### Getting Tabular data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YXccD8_xATcC",
      "metadata": {
        "id": "YXccD8_xATcC"
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['cpg_directional_fm', \"cpgs_w_padding\", 'sample', 'chr', 'clustering_index', 'region_inf', 'region_sup', 'region_nb_cpg']\n",
        "df_train_tabular = df_train.drop(columns=columns_to_drop)\n",
        "df_validation_tabular = df_validation.drop(columns=columns_to_drop)\n",
        "df_test_tabular = df_test.drop(columns=columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7plRVnln7rYi",
      "metadata": {
        "id": "7plRVnln7rYi"
      },
      "outputs": [],
      "source": [
        "df_X_train_tabular = df_train_tabular.drop(\"asm\", axis=1)\n",
        "df_X_test_tabular = df_test_tabular.drop(\"asm\", axis=1)\n",
        "df_X_validation_tabular = df_validation_tabular.drop(\"asm\", axis=1)\n",
        "\n",
        "# df_X_train_tabular['cpg_fm'] = df_X_train_tabular['cpg_fm'].apply(lambda x: np.mean(x))\n",
        "# df_X_test_tabular['cpg_fm'] = df_X_test_tabular['cpg_fm'].apply(lambda x: np.mean(x))\n",
        "# df_X_validation_tabular['cpg_fm'] = df_X_validation_tabular['cpg_fm'].apply(lambda x: np.mean(x))\n",
        "\n",
        "df_y_train_tabular = df_train_tabular[\"asm\"]\n",
        "df_y_test_tabular = df_test_tabular[\"asm\"]\n",
        "df_y_validation_tabular = df_validation_tabular[\"asm\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Mrv0F69ECIE8",
      "metadata": {
        "id": "Mrv0F69ECIE8"
      },
      "source": [
        "### Getting Imagery Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X0VNB5TDLemz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "X0VNB5TDLemz",
        "outputId": "fdff6b29-42fd-4d1f-e188-25f557d2bfd6"
      },
      "outputs": [],
      "source": [
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "vCpAS_ekCMpl",
      "metadata": {
        "id": "vCpAS_ekCMpl"
      },
      "outputs": [],
      "source": [
        "imagery_var = 'cpgs_w_padding' # 'cpg_directional_fm' OR 'cpgs_w_padding'\n",
        "imagery_cols = ['asm', imagery_var]\n",
        "df_train_imagery = df_train[imagery_cols]\n",
        "df_test_imagery = df_test[imagery_cols]\n",
        "df_validation_imagery = df_validation[imagery_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rqIxAl58L_bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "rqIxAl58L_bd",
        "outputId": "ef98e51f-7959-4ab0-b4a3-86bfbe59537e"
      },
      "outputs": [],
      "source": [
        "df_train_imagery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "PD-UIYMdM2Ia",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD-UIYMdM2Ia",
        "outputId": "542b3f78-3383-444a-e060-4bdd27cb37b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-9450828e9ef2>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_imagery[imagery_var] = df_train_imagery[imagery_var].apply(\n"
          ]
        }
      ],
      "source": [
        "# Function to convert data strings to numpy arrays\n",
        "df_train_imagery[imagery_var] = df_train_imagery[imagery_var].apply(\n",
        "        lambda x: ast.literal_eval(x.strip('\"'))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "vtpi8uyeqJXN",
      "metadata": {
        "id": "vtpi8uyeqJXN"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "\n",
        "X = df_train_imagery.drop('asm', axis=1)\n",
        "y = df_train_imagery['asm']\n",
        "\n",
        "# Initialize the RandomOverSampler object\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "# Fit and apply the transform\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "# Convert X_resampled back to a DataFrame\n",
        "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "\n",
        "# Add the resampled target column back to the DataFrame\n",
        "X_resampled_df['asm'] = y_resampled\n",
        "\n",
        "# Now df_train_imagery is the balanced dataset\n",
        "df_train_imagery = X_resampled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "Shc6tNg7pa2H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shc6tNg7pa2H",
        "outputId": "99c5f8c9-aeda-4d36-9d0d-7c7407facd1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "asm\n",
              "0    633665\n",
              "1    633665\n",
              "Name: count, dtype: Int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# random oversampling\n",
        "df_train_imagery['asm'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "WPyD4bTuAeov",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPyD4bTuAeov",
        "outputId": "65d7dfd1-1801-4810-b8de-5d10fc22ac22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-16193aca6174>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_test_imagery[imagery_var] = df_test_imagery[imagery_var].apply(\n",
            "<ipython-input-11-16193aca6174>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_validation_imagery[imagery_var] = df_validation_imagery[imagery_var].apply(\n"
          ]
        }
      ],
      "source": [
        "df_test_imagery[imagery_var] = df_test_imagery[imagery_var].apply(\n",
        "        lambda x: ast.literal_eval(x.strip('\"'))\n",
        "    )\n",
        "df_validation_imagery[imagery_var] = df_validation_imagery[imagery_var].apply(\n",
        "        lambda x: ast.literal_eval(x.strip('\"'))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "_p5_jXEjNNTA",
      "metadata": {
        "id": "_p5_jXEjNNTA"
      },
      "outputs": [],
      "source": [
        "df_X_train_imagery = df_train_imagery.drop(\"asm\", axis=1)\n",
        "df_X_test_imagery = df_test_imagery.drop('asm', axis=1)\n",
        "df_X_validation_imagery = df_validation_imagery.drop(\"asm\", axis=1)\n",
        "\n",
        "df_y_train_imagery = df_train_imagery.drop(imagery_var, axis=1)\n",
        "df_y_test_imagery = df_test_imagery.drop(imagery_var, axis=1)\n",
        "df_y_validation_imagery = df_validation_imagery.drop(imagery_var, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f-8N9arbTDDa",
      "metadata": {
        "id": "f-8N9arbTDDa"
      },
      "outputs": [],
      "source": [
        "#np.stack(df_X_t rain_imagery[imagery_var].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1wG6VC5nQp8v",
      "metadata": {
        "id": "1wG6VC5nQp8v"
      },
      "outputs": [],
      "source": [
        "df_X_train_imagery[imagery_var] = df_X_train_imagery[imagery_var].apply(np.array)\n",
        "df_X_validation_imagery[imagery_var] = df_X_validation_imagery[imagery_var].apply(np.array)\n",
        "df_X_test_imagery[imagery_var] = df_X_test_imagery[imagery_var].apply(np.array)\n",
        "\n",
        "# Step 2: Stack the arrays\n",
        "all_arrays_train = np.stack(df_X_train_imagery[imagery_var].values)\n",
        "all_arrays_val = np.stack(df_X_validation_imagery[imagery_var].values)\n",
        "all_arrays_test = np.stack(df_X_test_imagery[imagery_var].values)\n",
        "\n",
        "# Step 3: Convert to a PyTorch tensor\n",
        "X_train = torch.tensor(all_arrays_train)\n",
        "X_test = torch.tensor(all_arrays_test)\n",
        "X_val = torch.tensor(all_arrays_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "u9JR44bjRk1E",
      "metadata": {
        "id": "u9JR44bjRk1E"
      },
      "outputs": [],
      "source": [
        "y_train = torch.tensor(df_y_train_imagery.iloc[:, 0].values)\n",
        "y_test = torch.tensor(df_y_test_imagery.iloc[:, 0].values)\n",
        "y_val = torch.tensor(df_y_validation_imagery.iloc[:, 0].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fIasb88uR3rY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIasb88uR3rY",
        "outputId": "2a4597c8-0050-4f53-b139-0390f75c9bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1267330, 10, 20]) torch.Size([198085, 10, 20]) torch.Size([239833, 10, 20]) torch.Size([1267330]) torch.Size([198085]) torch.Size([239833])\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7h8vpbJf6Pb5",
      "metadata": {
        "id": "7h8vpbJf6Pb5"
      },
      "source": [
        "# Baseline Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DHY4ixCG6SL1",
      "metadata": {
        "id": "DHY4ixCG6SL1"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5tEEVAWc8vCN",
      "metadata": {
        "id": "5tEEVAWc8vCN"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pkf_wSMUbNUj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkf_wSMUbNUj",
        "outputId": "2d335893-0196-418e-d5bd-203b422dbc1c"
      },
      "outputs": [],
      "source": [
        "df_X_train_tabular.columns.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jb2PXkFb6R4x",
      "metadata": {
        "id": "Jb2PXkFb6R4x"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "RFClassifier_ = RandomForestClassifier(class_weight= 'balanced')\n",
        "RFClassifier_.fit(df_X_train_tabular, df_y_train_tabular)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15HbNDuz8wVD",
      "metadata": {
        "id": "15HbNDuz8wVD"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mtfANz3E8ycq",
      "metadata": {
        "id": "mtfANz3E8ycq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = RFClassifier_.predict(df_X_validation_tabular)\n",
        "print(confusion_matrix(df_y_validation_tabular, y_pred))\n",
        "print(sklearn.metrics.classification_report(df_y_validation_tabular, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "glzoUt2F6UI7",
      "metadata": {
        "id": "glzoUt2F6UI7"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N2kgT-iZ92Ba",
      "metadata": {
        "id": "N2kgT-iZ92Ba"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gdYN4GE09awG",
      "metadata": {
        "id": "gdYN4GE09awG"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "Scaler_logistic_regression = StandardScaler()\n",
        "Scaler_logistic_regression.fit(df_X_train_tabular)\n",
        "\n",
        "df_X_train_scaled = Scaler_logistic_regression.transform(df_X_train_tabular)\n",
        "df_X_validation_scaled = Scaler_logistic_regression.transform(df_X_validation_tabular)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thmbxzxR93lR",
      "metadata": {
        "id": "thmbxzxR93lR"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6OHrAR6l6WH9",
      "metadata": {
        "id": "6OHrAR6l6WH9"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR_ = LogisticRegression(class_weight= 'balanced')\n",
        "LR_.fit(df_X_train_scaled, df_y_train_tabular)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cBBejlkO9675",
      "metadata": {
        "id": "cBBejlkO9675"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9HV3aj5U9_1R",
      "metadata": {
        "id": "9HV3aj5U9_1R"
      },
      "outputs": [],
      "source": [
        "y_pred = LR_.predict(df_X_validation_scaled)\n",
        "print(sklearn.metrics.classification_report(df_y_validation_tabular, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u2EKujFJ6W-P",
      "metadata": {
        "id": "u2EKujFJ6W-P"
      },
      "source": [
        "## XGboost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OZgOtDbO-eUe",
      "metadata": {
        "id": "OZgOtDbO-eUe"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hlM9MsNV6YU1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "hlM9MsNV6YU1",
        "outputId": "43611e2e-95b7-403d-8e20-7803be044e0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [19:54:43] WARNING: /workspace/src/learner.cc:742: \n",
            "Parameters: { \"class_weight\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              class_weight=&#x27;balanced&#x27;, colsample_bylevel=None,\n",
              "              colsample_bynode=None, colsample_bytree=None, device=None,\n",
              "              early_stopping_rounds=None, enable_categorical=False,\n",
              "              eval_metric=None, feature_types=None, gamma=None,\n",
              "              grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              class_weight=&#x27;balanced&#x27;, colsample_bylevel=None,\n",
              "              colsample_bynode=None, colsample_bytree=None, device=None,\n",
              "              early_stopping_rounds=None, enable_categorical=False,\n",
              "              eval_metric=None, feature_types=None, gamma=None,\n",
              "              grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              class_weight='balanced', colsample_bylevel=None,\n",
              "              colsample_bynode=None, colsample_bytree=None, device=None,\n",
              "              early_stopping_rounds=None, enable_categorical=False,\n",
              "              eval_metric=None, feature_types=None, gamma=None,\n",
              "              grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, ...)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "XGBClassifier_ = XGBClassifier(class_weight= 'balanced')\n",
        "XGBClassifier_.fit(df_X_train_tabular, df_y_train_tabular)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C1bgUEX7-g2S",
      "metadata": {
        "id": "C1bgUEX7-g2S"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ca9uuSNc-iTy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca9uuSNc-iTy",
        "outputId": "69b6bea3-b4c0-43c5-b6bc-c11bdc5cfbf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      0.99    237014\n",
            "         1.0       0.60      0.17      0.26      2819\n",
            "\n",
            "    accuracy                           0.99    239833\n",
            "   macro avg       0.80      0.58      0.63    239833\n",
            "weighted avg       0.99      0.99      0.99    239833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = XGBClassifier_.predict(df_X_validation_tabular)\n",
        "print(sklearn.metrics.classification_report(df_y_validation_tabular, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yjGwvZc_6bkZ",
      "metadata": {
        "id": "yjGwvZc_6bkZ"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Co3PNlD1_GtC",
      "metadata": {
        "id": "Co3PNlD1_GtC"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "oABX-5936cjj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oABX-5936cjj",
        "outputId": "cf47b3f9-dfdf-4825-c5f3-8152dd5b632f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of regions assessed for ASM: 1267330\n",
            "Regions with ASM found: 633665 (50.00% of total)\n",
            "\n",
            "Weight for class 0: 1.00\n",
            "Weight for class 1: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Getting class weights\n",
        "df = pd.concat([df_train_imagery])\n",
        "neg, pos = np.bincount(df['asm'])\n",
        "total = neg + pos\n",
        "print('Number of regions assessed for ASM: {}\\nRegions with ASM found: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))\n",
        "\n",
        "weight_for_0 = (1 / neg)*(total)/2.0\n",
        "weight_for_1 = (1 / pos)*(total)/2.0\n",
        "\n",
        "class_weight_asm = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5uMTsfcYKjxN",
      "metadata": {
        "id": "5uMTsfcYKjxN"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evy-K9c6Kl4D",
      "metadata": {
        "id": "evy-K9c6Kl4D"
      },
      "outputs": [],
      "source": [
        "class SimpleCNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 1))\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(1, 1))\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(1, 1))\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(in_features=1280, out_features=64)  # Adjusted the input features after calculations\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # returns logits\n",
        "\n",
        "    def predict(self, x):\n",
        "        logit = self.forward(x)\n",
        "        probability = torch.sigmoid(logit)\n",
        "        return probability > 0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cBiEpc-KmeP",
      "metadata": {
        "id": "6cBiEpc-KmeP"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VPrIsGQcKngH",
      "metadata": {
        "id": "VPrIsGQcKngH"
      },
      "outputs": [],
      "source": [
        "# Creating the PyTorch model\n",
        "CNN_ = SimpleCNNModel()\n",
        "\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "optimizer = torch.optim.AdamW(CNN_.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Taking the class weights into account\n",
        "pos_weight = torch.tensor(class_weight_asm[1], dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Device Handling (GPU usage if possible)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Passing the objects to the device\n",
        "CNN_ = CNN_.to(device)\n",
        "X_train = X_train.to(device)\n",
        "X_train = X_train.float()\n",
        "X_train = X_train.unsqueeze(1)\n",
        "X_test = X_test.to(device)\n",
        "X_test = X_test.float()\n",
        "X_test = X_test.unsqueeze(1)\n",
        "X_val = X_val.to(device)\n",
        "X_val = X_val.float()\n",
        "X_val = X_val.unsqueeze(1)\n",
        "y_train = y_train.to(device)\n",
        "y_train = y_train.float()\n",
        "y_test = y_test.to(device)\n",
        "y_test = y_test.float()\n",
        "y_val = y_val.to(device)\n",
        "y_val = y_val.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QjoqLxc4Uk1K",
      "metadata": {
        "id": "QjoqLxc4Uk1K"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 64  # You can adjust this size depending on your GPU memory\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xfpCl82XWTdc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "xfpCl82XWTdc",
        "outputId": "67aa9182-7fa4-4632-aa81-d9221fc78608"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y_true):\n",
        "    predicted = torch.round(torch.sigmoid(y_pred))\n",
        "    correct = (predicted == y_true).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "num_epochs = 250\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    CNN_.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = CNN_(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = running_accuracy / len(train_loader.dataset)\n",
        "\n",
        "    # Validation loss\n",
        "    CNN_.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = CNN_(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy /= len(val_loader.dataset)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zxFYEGe8Ogof",
      "metadata": {
        "id": "zxFYEGe8Ogof"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-5IUkYNASu6w",
      "metadata": {
        "id": "-5IUkYNASu6w"
      },
      "outputs": [],
      "source": [
        "# import classification report\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V-DuqKWRLTOM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-DuqKWRLTOM",
        "outputId": "25ccba09-16b6-4835-bd2a-a6920f4df070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9983    0.9208    0.9580    237014\n",
            "         1.0     0.1156    0.8709    0.2041      2819\n",
            "\n",
            "    accuracy                         0.9202    239833\n",
            "   macro avg     0.5570    0.8958    0.5811    239833\n",
            "weighted avg     0.9880    0.9202    0.9491    239833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# classification report\n",
        "y_pred = CNN_(X_val).squeeze()\n",
        "y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "print(classification_report(y_val.detach().numpy(), y_pred.detach().numpy(), digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZGDJ99SISaFZ",
      "metadata": {
        "id": "ZGDJ99SISaFZ"
      },
      "source": [
        "# CNN 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nz_zTHhGSgor",
      "metadata": {
        "id": "nz_zTHhGSgor"
      },
      "outputs": [],
      "source": [
        "class SecondCNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SecondCNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(2, 2), padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(2, 2), padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # Recalculate the output dimensions:\n",
        "        # Pooling now correctly outputs (5 x 10) after first, and (3 x 5) after second\n",
        "        self.fc1 = nn.Linear(in_features=32 * 3 * 5, out_features=64)  # Recalculated input features\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        logit = self.forward(x)\n",
        "        probability = torch.sigmoid(logit)\n",
        "        return probability > 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RBb0QX0STbCm",
      "metadata": {
        "id": "RBb0QX0STbCm"
      },
      "outputs": [],
      "source": [
        "CNN_ = SecondCNNModel()\n",
        "\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "optimizer = torch.optim.AdamW(CNN_.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Taking the class weights into account\n",
        "pos_weight = torch.tensor(class_weight_asm[1], dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Device Handling (GPU usage if possible)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZTc5Rj4OS1EK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTc5Rj4OS1EK",
        "outputId": "9132cd74-69ef-4447-8993-9b918267f990"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y_true):\n",
        "    predicted = torch.round(torch.sigmoid(y_pred))\n",
        "    correct = (predicted == y_true).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "num_epochs = 250\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    CNN_.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = CNN_(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = running_accuracy / len(train_loader.dataset)\n",
        "\n",
        "    # Validation loss\n",
        "    CNN_.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = CNN_(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy /= len(val_loader.dataset)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08yA-90SV2RN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08yA-90SV2RN",
        "outputId": "1b3f4a46-cf50-427c-f2a6-840aaba430e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9982    0.9183    0.9566    237014\n",
            "         1.0     0.1118    0.8645    0.1980      2819\n",
            "\n",
            "    accuracy                         0.9177    239833\n",
            "   macro avg     0.5550    0.8914    0.5773    239833\n",
            "weighted avg     0.9878    0.9177    0.9477    239833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# classification report\n",
        "y_pred = CNN_(X_val).squeeze()\n",
        "y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "print(classification_report(y_val.detach().numpy(), y_pred.detach().numpy(), digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2Sp0pXPa8Wk2",
      "metadata": {
        "id": "2Sp0pXPa8Wk2"
      },
      "source": [
        "# 3rd CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zfC739038ZCE",
      "metadata": {
        "id": "zfC739038ZCE"
      },
      "outputs": [],
      "source": [
        "class ThirdCNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThirdCNNModel, self).__init__()\n",
        "        # Using 3x3 kernels now, with padding=1 to maintain dimensions\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Calculating the output dimensions:\n",
        "        # First Convolution: (10 x 20) -> (10 x 20) with padding=1\n",
        "        # First Pooling: (10 x 20) -> (5 x 10)\n",
        "        # Second Convolution: (5 x 10) -> (5 x 10) with padding=1\n",
        "        # Second Pooling: (5 x 10) -> (2 x 5)\n",
        "        self.fc1 = nn.Linear(in_features=32 * 2 * 5, out_features=64)  # Updated dimensions to 2 x 5\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        logit = self.forward(x)\n",
        "        probability = torch.sigmoid(logit)\n",
        "        return probability > 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0J9i8xDK8o_G",
      "metadata": {
        "id": "0J9i8xDK8o_G"
      },
      "outputs": [],
      "source": [
        "CNN_ = ThirdCNNModel()\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "optimizer = torch.optim.AdamW(CNN_.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Taking the class weights into account\n",
        "pos_weight = torch.tensor(class_weight_asm[1], dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Device Handling (GPU usage if possible)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rW3F0ENd8z5J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW3F0ENd8z5J",
        "outputId": "e17abf55-035a-4c43-c87f-3f3ff11559cb"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y_true):\n",
        "    predicted = torch.round(torch.sigmoid(y_pred))\n",
        "    correct = (predicted == y_true).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "num_epochs = 250\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    CNN_.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = CNN_(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = running_accuracy / len(train_loader.dataset)\n",
        "\n",
        "    # Validation loss\n",
        "    CNN_.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    val_accuracy = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = CNN_(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_accuracy += calculate_accuracy(outputs, labels) * inputs.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy /= len(val_loader.dataset)\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q6rs1tHh830n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6rs1tHh830n",
        "outputId": "3870e37b-4b82-4c93-a446-2e3dc11b9718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9983    0.9174    0.9561    237014\n",
            "         1.0     0.1108    0.8652    0.1964      2819\n",
            "\n",
            "    accuracy                         0.9168    239833\n",
            "   macro avg     0.5545    0.8913    0.5763    239833\n",
            "weighted avg     0.9878    0.9168    0.9472    239833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# classification report\n",
        "y_pred = CNN_(X_val).squeeze()\n",
        "y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "print(classification_report(y_val.detach().numpy(), y_pred.detach().numpy(), digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3jIJ2KYZXYGL",
      "metadata": {
        "id": "3jIJ2KYZXYGL"
      },
      "source": [
        "# ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "aUWcEP1gXbAD",
      "metadata": {
        "id": "aUWcEP1gXbAD"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, in_channels=1, patch_size=2, emb_size=128, img_size=(10, 20), num_heads=4, depth=6, num_classes=1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (img_size[0] // patch_size) * (img_size[1] // patch_size) + \\\n",
        "                      (img_size[0] % patch_size != 0) + (img_size[1] % patch_size != 0)\n",
        "        self.patch_embedding = nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, emb_size))  # Modified line\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=emb_size, nhead=num_heads),\n",
        "            num_layers=depth\n",
        "        )\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        self.fc = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) != 4:\n",
        "          raise ValueError(\"Expected input to have 4 dimensions [B, C, H, W], got {}\".format(x.shape))\n",
        "\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 4, 1, 3, 5).contiguous()\n",
        "        x = x.view(B, -1, self.patch_size * self.patch_size * C)\n",
        "\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.position_embeddings\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "\n",
        "        return self.fc(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        logit = self.forward(x)\n",
        "        probability = torch.sigmoid(logit)\n",
        "        return probability > 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Row3CYtvXmuo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Row3CYtvXmuo",
        "outputId": "5e0644d4-51d2-4b28-93e2-9a114b76ea3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BCEWithLogitsLoss()"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating the PyTorch model\n",
        "ViT_ = VisionTransformer(in_channels=1)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "optimizer = torch.optim.AdamW(ViT_.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Taking the class weights into account\n",
        "pos_weight = torch.tensor(class_weight_asm[1], dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Device Handling (GPU usage if possible)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Passing the objects to the device\n",
        "ViT_.to(device)\n",
        "criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4IODjLnyhzik",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IODjLnyhzik",
        "outputId": "7b5bebe2-57e7-450d-f177-a339fc4e3876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "CWqQ57UKtt1n",
      "metadata": {
        "id": "CWqQ57UKtt1n"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.to(device)\n",
        "X_train = X_train.float()\n",
        "X_train = X_train.unsqueeze(1)\n",
        "X_test = X_test.to(device)\n",
        "X_test = X_test.float()\n",
        "X_test = X_test.unsqueeze(1)\n",
        "X_val = X_val.to(device)\n",
        "X_val = X_val.float()\n",
        "X_val = X_val.unsqueeze(1)\n",
        "y_train = y_train.to(device)\n",
        "y_train = y_train.float()\n",
        "y_test = y_test.to(device)\n",
        "y_test = y_test.float()\n",
        "y_val = y_val.to(device)\n",
        "y_val = y_val.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "muH7QSCtYWnP",
      "metadata": {
        "id": "muH7QSCtYWnP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 16\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yAYT9DT7YBJC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAYT9DT7YBJC",
        "outputId": "968f0557-c53d-4b25-b880-64e2a4d9409a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "Epoch 2/50\n",
            "Epoch 3/50\n",
            "Epoch 4/50\n",
            "Epoch 5/50\n"
          ]
        }
      ],
      "source": [
        "def calculate_accuracy(y_pred, y_true):\n",
        "    predicted = torch.round(torch.sigmoid(y_pred))\n",
        "    correct = (predicted == y_true).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ViT_.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = ViT_(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yW-z8FcScbkM",
      "metadata": {
        "id": "yW-z8FcScbkM"
      },
      "outputs": [],
      "source": [
        "torch.save(ViT_, 'ViT_oversampled.pth')\n",
        "torch.save(ViT_.state_dict(), 'ViT_oversampled_state_dict.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vwvzblwR9DYY",
      "metadata": {
        "id": "vwvzblwR9DYY"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "ViT_ = torch.load('ViT_oversampled.pth', map_location=torch.device('cpu'))\n",
        "\n",
        "ViT_.load_state_dict(torch.load('ViT_oversampled_state_dict.pth', map_location=torch.device('cpu')))\n",
        "ViT_.eval()\n",
        "\n",
        "# Create a DataLoader for your validation dataset\n",
        "batch_size = 32  # You can adjust the batch size according to your system's capability\n",
        "dataset = TensorDataset(X_val, y_val)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize lists to store true labels and predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Evaluate in batches\n",
        "for X_batch, y_batch in data_loader:\n",
        "    with torch.no_grad():  # No need to compute gradients\n",
        "        y_pred = ViT_(X_batch).squeeze()\n",
        "        y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "        all_preds.extend(y_pred.detach().numpy())\n",
        "        all_labels.extend(y_batch.detach().numpy())\n",
        "\n",
        "# Classification report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(all_labels, all_preds, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bkbUeqaGOhvl",
      "metadata": {
        "id": "bkbUeqaGOhvl"
      },
      "source": [
        "# 2D RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bosfsc4Oj6q",
      "metadata": {
        "id": "6bosfsc4Oj6q"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(in_channels, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input to (batch_size, seq_len, in_channels)\n",
        "        batch_size, _, height, width = x.size()\n",
        "        x = x.view(batch_size, 1, height * width)\n",
        "\n",
        "        # Set initial hidden state\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, 0, :]\n",
        "\n",
        "        # Fully connected layer to get the final output\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "in_channels = 1  # Number of input channels (e.g., grayscale image)\n",
        "hidden_size = 64  # Size of the hidden state\n",
        "num_layers = 2  # Number of RNN layers\n",
        "num_classes = 1  # Number of output classes\n",
        "RNN_ = RNN(in_channels, hidden_size, num_layers, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V9h7TZ32HW-l",
      "metadata": {
        "id": "V9h7TZ32HW-l"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and the optimizer\n",
        "optimizer = torch.optim.AdamW(RNN_.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "# Taking the class weights into account\n",
        "pos_weight = torch.tensor(class_weight_asm[1], dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Device Handling (GPU usage if possible)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Passing the objects to the device\n",
        "RNN_.to(device)\n",
        "criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fEBY1NmqPmPH",
      "metadata": {
        "id": "fEBY1NmqPmPH"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y_true):\n",
        "    predicted = torch.round(torch.sigmoid(y_pred))\n",
        "    correct = (predicted == y_true).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    RNN_.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = RNN_(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print losses\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y0ecrarOHPCg",
      "metadata": {
        "id": "Y0ecrarOHPCg"
      },
      "outputs": [],
      "source": [
        "torch.save(RNN_, 'path/to/save/ViT_.pth')\n",
        "\n",
        "# Or, save only the model's state dictionary\n",
        "torch.save(RNN_.state_dict(), 'path/to/save/RNN_state_dict.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vRaTkL4yHEVJ",
      "metadata": {
        "id": "vRaTkL4yHEVJ"
      },
      "outputs": [],
      "source": [
        "# classification report\n",
        "RNN_.eval()\n",
        "y_pred = RNN_(X_val).squeeze()\n",
        "y_pred = torch.round(torch.sigmoid(y_pred))\n",
        "print(classification_report(y_val.detach().numpy(), y_pred.detach().numpy(), digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98PxxNwr6dUh",
      "metadata": {
        "id": "98PxxNwr6dUh"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UPQQheWHP7um",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "UPQQheWHP7um",
        "outputId": "e6875b1e-b8d4-4f66-de04-84988f5a7936"
      },
      "outputs": [],
      "source": [
        "train_scalar_features = np.array(df_X_train_tabular)\n",
        "val_scalar_features = np.array(df_X_validation_tabular)\n",
        "test_scalar_features = np.array(df_X_test_tabular)\n",
        "\n",
        "train_scalar_features_torch = torch.tensor(train_scalar_features)\n",
        "test_scalar_features_torch = torch.tensor(test_scalar_features)\n",
        "val_scalar_features_torch = torch.tensor(val_scalar_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T_N-hNpj6eXU",
      "metadata": {
        "id": "T_N-hNpj6eXU"
      },
      "outputs": [],
      "source": [
        "class RNN_MODEL(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(RNN_MODEL, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      h0 = torch.randn(self.layer_dim, x.size(0), self.hidden_dim)\n",
        "      out, hn = self.rnn(x, h0.detach())\n",
        "      out = self.fc(out[:, -1, :])\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5L-hz6cPdcD",
      "metadata": {
        "id": "y5L-hz6cPdcD"
      },
      "outputs": [],
      "source": [
        "RNN = RNN_MODEL(train_scalar_features.shape[1], 100, 5, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vdxAiO18PfgO",
      "metadata": {
        "id": "vdxAiO18PfgO"
      },
      "outputs": [],
      "source": [
        "train_scalar_features_RNN = train_scalar_features_torch.unsqueeze(1) # N, L, H_in\n",
        "test_scalar_features_RNN = test_scalar_features_torch.unsqueeze(1) # N, L, H_in\n",
        "val_scalar_features_RNN = val_scalar_features_torch.unsqueeze(1) # N, L, H_in\n",
        "\n",
        "print(train_scalar_features_RNN.shape)\n",
        "print(test_scalar_features_RNN.shape)\n",
        "print(val_scalar_features_RNN.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tg3WiwFxPjOB",
      "metadata": {
        "id": "Tg3WiwFxPjOB"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(RNN.parameters(), lr=0.001, weight_decay=0.001)\n",
        "pos_weight = torch.tensor(class_weight_asm[1])\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u3zMdJcWPlES",
      "metadata": {
        "id": "u3zMdJcWPlES"
      },
      "outputs": [],
      "source": [
        "# Device handling\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "RNN.to(device)\n",
        "train_scalar_features_RNN = train_scalar_features_RNN.to(device)\n",
        "train_labels_torch = train_labels_torch.to(device)\n",
        "test_scalar_features_RNN = test_scalar_features_RNN.to(device)\n",
        "test_labels_torch = test_labels_torch.to(device)\n",
        "val_scalar_features_RNN = val_scalar_features_RNN.to(device)\n",
        "val_labels_torch = val_labels_torch.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VeZ8lFbaPnDq",
      "metadata": {
        "id": "VeZ8lFbaPnDq"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "train_loss  = []\n",
        "\n",
        "trainloader = DataLoader(torch.utils.data.TensorDataset(train_scalar_features_RNN, train_labels_torch.unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "testloader = DataLoader(torch.utils.data.TensorDataset(test_scalar_features_RNN, test_labels_torch.unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "valloader = DataLoader(torch.utils.data.TensorDataset(val_scalar_features_RNN, val_labels_torch.unsqueeze(1)), batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Iterate through each epoch\n",
        "for epoch in range(1000):\n",
        "  for batch in trainloader:\n",
        "    # Get the inputs and labels\n",
        "    inputs, labels = batch\n",
        "    # Forward pass\n",
        "    outputs = RNN(inputs)\n",
        "    # Calculate loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    train_loss.append(loss.item())\n",
        "\n",
        "\n",
        "    # Zero gradients, backward pass, update weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y8SSHz1s6fk-",
      "metadata": {
        "id": "y8SSHz1s6fk-"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hYr_dk7v6hRP",
      "metadata": {
        "id": "hYr_dk7v6hRP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "loZUOnGs6h2N",
      "metadata": {
        "id": "loZUOnGs6h2N"
      },
      "source": [
        "## ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1QZqAVU6i1A",
      "metadata": {
        "id": "Q1QZqAVU6i1A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "zo1y-VR4_A8c",
      "metadata": {
        "id": "zo1y-VR4_A8c"
      },
      "source": [
        "# Fine-tuning Baseline models and choosing the best one"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F6ksg-NKJ5hb",
      "metadata": {
        "id": "F6ksg-NKJ5hb"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RWXKMM64ojUk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWXKMM64ojUk",
        "outputId": "86ac23c2-2b0c-4491-b67d-259e15fb107b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 150 sqrt 10 2 False 0.1686506617011372\n",
            "90 40 sqrt 5 4 True 0.3577549271636675\n",
            "None 90 sqrt 5 2 False 0.25362318840579706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None 110 auto 10 4 False 0.3666046703864435\n",
            "10 90 sqrt 10 1 False 0.1671716506389145\n",
            "90 30 sqrt 10 2 True 0.29967909158232536\n",
            "40 40 sqrt 2 2 False 0.3304592644729896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 140 auto 2 4 False 0.36686134544667964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 50 auto 10 2 False 0.1699912687218752\n",
            "50 40 sqrt 2 4 False 0.3544510083520065\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(100, 140, 'auto', 2, 4, False, 0.36686134544667964)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "grid = {\n",
        "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "    'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# pick 10 random hyperparameter combinations\n",
        "n = 10\n",
        "random_grid = {k: np.random.choice(v, n) for k, v in grid.items()}\n",
        "\n",
        "# fit logistic regression with each hyperparameter combination\n",
        "results = []\n",
        "for i in range(n):\n",
        "    max_depth = random_grid['max_depth'][i]\n",
        "    n_estimators = random_grid['n_estimators'][i]\n",
        "    max_features = random_grid['max_features'][i]\n",
        "    min_samples_split = random_grid['min_samples_split'][i]\n",
        "    min_samples_leaf = random_grid['min_samples_leaf'][i]\n",
        "    bootstrap = random_grid['bootstrap'][i]\n",
        "\n",
        "\n",
        "    # fit logistic regression\n",
        "    model = RandomForestClassifier(class_weight='balanced', max_depth=max_depth, n_estimators=n_estimators, max_features=max_features, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, bootstrap=bootstrap)\n",
        "    model.fit(df_X_train_tabular, df_y_train_tabular)\n",
        "\n",
        "    # evaluate model on f_1 score\n",
        "    y_pred = model.predict(df_X_validation_tabular)\n",
        "    f1 = f1_score(df_y_validation_tabular, y_pred)\n",
        "\n",
        "    # save results\n",
        "    print(max_depth, n_estimators, max_features, min_samples_split, min_samples_leaf, bootstrap, f1)\n",
        "    results.append((max_depth, n_estimators, max_features, min_samples_split, min_samples_leaf, bootstrap, f1))\n",
        "\n",
        "# get the best hyperparameters\n",
        "best_hyperparameters = max(results, key=lambda x: x[6])\n",
        "best_hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QpLZnY9ZsgiJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpLZnY9ZsgiJ",
        "outputId": "04314a00-b28e-4519-d5b6-cece4d151763"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 140, 'auto', 2, 4, False, 0.36686134544667964)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_hyperparameters = max(results, key=lambda x: x[6])\n",
        "best_hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g5fI48Xne9a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5fI48Xne9a2",
        "outputId": "14783845-e6e6-4542-b608-1ea921477bad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      0.99    237014\n",
            "         1.0       0.46      0.30      0.36      2819\n",
            "\n",
            "    accuracy                           0.99    239833\n",
            "   macro avg       0.73      0.65      0.68    239833\n",
            "weighted avg       0.99      0.99      0.99    239833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "best_model = RandomForestClassifier(class_weight='balanced', max_depth=best_hyperparameters[0] , n_estimators=best_hyperparameters[1], max_features=best_hyperparameters[2], min_samples_split=best_hyperparameters[3], min_samples_leaf=best_hyperparameters[4], bootstrap=best_hyperparameters[5])\n",
        "best_model.fit(df_X_train_tabular, df_y_train_tabular)\n",
        "y_pred = best_model.predict(df_X_validation_tabular)\n",
        "print(classification_report(df_y_validation_tabular, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stLLJUN9XiJS",
      "metadata": {
        "id": "stLLJUN9XiJS"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79Pg9LeEXjzE",
      "metadata": {
        "id": "79Pg9LeEXjzE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']}\n",
        "\n",
        "lr_ = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# pick 10 random hyperparameter combinations\n",
        "n = 10\n",
        "random_grid = {k: np.random.choice(v, n) for k, v in grid.items()}\n",
        "\n",
        "# fit logistic regression with each hyperparameter combination\n",
        "results = []\n",
        "for i in range(n):\n",
        "    C = random_grid['C'][i]\n",
        "    penalty = random_grid['penalty'][i]\n",
        "    solver = random_grid['solver'][i]\n",
        "\n",
        "    # fit logistic regression\n",
        "    model = LogisticRegression(C=C, penalty=penalty, solver=solver)\n",
        "    model.fit(df_X_train_scaled, df_y_train_tabular)\n",
        "\n",
        "    # evaluate model on f_1 score\n",
        "    y_pred = model.predict(df_X_validation_scaled)\n",
        "    f1 = f1_score(df_y_validation_tabular, y_pred)\n",
        "\n",
        "    # save results\n",
        "    print(C, penalty, solver, f1)\n",
        "    results.append((C, penalty, solver, f1))\n",
        "\n",
        "# get the best hyperparameters\n",
        "best_hyperparameters = max(results, key=lambda x: x[3])\n",
        "best_hyperparameters\n",
        "\n",
        "# defining best_model\n",
        "best_model = LogisticRegression(C=best_hyperparameters[0], penalty=best_hyperparameters[1], solver=best_hyperparameters[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OTwX6PhuyOBj",
      "metadata": {
        "id": "OTwX6PhuyOBj"
      },
      "outputs": [],
      "source": [
        "# get classificaiton report of best_model\n",
        "best_model.fit(df_X_train_scaled, df_y_train_tabular)\n",
        "y_pred = best_model.predict(df_X_validation_scaled)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(df_y_validation_tabular, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xfhB9Y1HYvdI",
      "metadata": {
        "id": "xfhB9Y1HYvdI"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aJwlnMePYxIs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJwlnMePYxIs",
        "outputId": "da7b4596-4649-48b0-e37c-9c813417940b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 90 0.01 10 0.8 1.0 0.1 0.024509803921568627\n",
            "70 150 10.0 5 1.0 0.8 0.01 0.0\n",
            "50 20 1.0 10 1.0 1.0 0.1 0.2669809299335762\n",
            "None 30 0.1 5 0.6 0.6 0.001 0.13856960408684546\n",
            "30 20 0.01 10 0.8 0.8 0.1 0.0\n",
            "100 70 1.0 1 0.6 1.0 0.001 0.25920408597573946\n",
            "30 70 1.0 5 1.0 1.0 0.001 0.2681797128300139\n",
            "50 130 1.0 5 1.0 0.6 0.1 0.26986365773389753\n",
            "30 70 0.1 1 0.8 0.6 0.01 0.21195172210774213\n",
            "70 10 10.0 5 0.8 0.6 0.0 0.05950500263296471\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      0.99    237014\n",
            "         1.0       0.40      0.20      0.27      2819\n",
            "\n",
            "    accuracy                           0.99    239833\n",
            "   macro avg       0.70      0.60      0.63    239833\n",
            "weighted avg       0.98      0.99      0.98    239833\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "grid = {'max_depth': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        "        'n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150],\n",
        "        'learning_rate': [0.001, 0.01, 0.1, 1, 10],\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'reg_alpha': [0, 0.001, 0.01, 0.1]}\n",
        "\n",
        "xgb = XGBClassifier(class_weight='balanced')\n",
        "\n",
        "# pick 10 random hyperparameter combinations\n",
        "n = 10\n",
        "random_grid = {k: np.random.choice(v, n) for k, v in grid.items()}\n",
        "\n",
        "# fit logistic regression with each hyperparameter combination\n",
        "results = []\n",
        "for i in range(n):\n",
        "    max_depth = random_grid['max_depth'][i]\n",
        "    n_estimators = random_grid['n_estimators'][i]\n",
        "    learning_rate = random_grid['learning_rate'][i]\n",
        "    min_child_weight = random_grid['min_child_weight'][i]\n",
        "    subsample = random_grid['subsample'][i]\n",
        "    colsample_bytree = random_grid['colsample_bytree'][i]\n",
        "    reg_alpha = random_grid['reg_alpha'][i]\n",
        "\n",
        "    # fit logistic regression\n",
        "    model = XGBClassifier(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate,\n",
        "                          min_child_weight=min_child_weight, subsample=subsample, colsample_bytree=colsample_bytree,\n",
        "                          reg_alpha=reg_alpha)\n",
        "    model.fit(df_X_train_tabular, df_y_train_tabular)\n",
        "\n",
        "    # evaluate model on f_1 score\n",
        "    y_pred = model.predict(df_X_validation_tabular)\n",
        "    f1 = f1_score(df_y_validation_tabular, y_pred)\n",
        "\n",
        "    # save results\n",
        "    print(max_depth, n_estimators, learning_rate, min_child_weight, subsample, colsample_bytree, reg_alpha, f1)\n",
        "    results.append((max_depth, n_estimators, learning_rate, min_child_weight, subsample, colsample_bytree, reg_alpha, f1))\n",
        "\n",
        "# get the best hyperparameters\n",
        "best_hyperparameters = max(results, key=lambda x: x[7])\n",
        "best_hyperparameters\n",
        "\n",
        "# get classificaiton report\n",
        "best_model = XGBClassifier(max_depth=best_hyperparameters[0], n_estimators=best_hyperparameters[1],\n",
        "                           learning_rate=best_hyperparameters[2], min_child_weight=best_hyperparameters[3],\n",
        "                           subsample=best_hyperparameters[4], colsample_bytree=best_hyperparameters[5],\n",
        "                           reg_alpha=best_hyperparameters[6])\n",
        "best_model.fit(df_X_train_tabular, df_y_train_tabular)\n",
        "y_pred = best_model.predict(df_X_validation_tabular)\n",
        "print(classification_report(df_y_validation_tabular, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ML_Ali_NewData",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
