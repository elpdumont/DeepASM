{
  "cells": [
    {
      "cell_type": "code",
      "id": "kOsB9DwleTwIfALHx8EyzQns",
      "metadata": {
        "tags": [],
        "id": "kOsB9DwleTwIfALHx8EyzQns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715256222666,
          "user_tz": 240,
          "elapsed": 30040,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "504f3228-f912-4654-8927-50af9e0baa53"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import io\n",
        "import logging\n",
        "import ast\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "import dask.dataframe as dd\n",
        "#from dask.distributed import Client\n",
        "from google.cloud import bigquery\n",
        "from dask import delayed\n",
        "import pandas as pd\n",
        "from sklearn.utils import check_random_state\n",
        "from scipy.stats import entropy\n",
        "\n",
        "import concurrent.futures\n",
        "import ast\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "# Python packages for data, stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "# ML\n",
        "import sklearn\n",
        "from sklearn.decomposition import PCA, KernelPCA, NMF, TruncatedSVD\n",
        "from sklearn.manifold import TSNE, LocallyLinearEmbedding, SpectralEmbedding\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# To get the time\n",
        "from datetime import datetime\n",
        "\n",
        "# To write on the same line\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Setting the random seed for various libraries\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Plotly\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Print different versions\n",
        "print(sys.version)\n",
        "print(\"Numpy version:\", np.__version__)\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"Seaborn version:\", sns.__version__)\n",
        "print(\"Sklearn version:\", sklearn.__version__)\n",
        "\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "Numpy version: 1.25.2\n",
            "Pandas version: 2.0.3\n",
            "Seaborn version: 0.13.1\n",
            "Sklearn version: 1.2.2\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "PROJECT_ID = \"hmh-em-deepasm\"\n",
        "BQ_ML_DATASET = \"ml_250bp_3\" # Ali: hg19_250_ml #ml_250bp\n",
        "bq_client = bigquery.Client()"
      ],
      "metadata": {
        "id": "x-4HFUbbeag0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715256226301,
          "user_tz": 240,
          "elapsed": 360,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "x-4HFUbbeag0",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['TRAINING', 'VALIDATION', 'TESTING']\n",
        "seq_var = 'cpg_directional_fm'\n",
        "dic_data = {dataset: {} for dataset in datasets}\n",
        "\n",
        "for dataset in datasets:\n",
        "  print(f\"Processing {dataset} dataset...\")\n",
        "  query = f\"\"\"\n",
        "    SELECT asm, {seq_var}\n",
        "    FROM {PROJECT_ID}.{BQ_ML_DATASET}.{dataset}\n",
        "    WHERE {seq_var} IS NOT NULL AND asm IS NOT NULL\n",
        "    \"\"\"\n",
        "\n",
        "  dic_data[dataset]['raw'] = bq_client.query(query).to_dataframe()\n",
        "  dic_data[dataset]['raw'][seq_var] = dic_data[dataset]['raw'][seq_var].apply(\n",
        "        lambda x: ast.literal_eval(x.strip('\"'))\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp8FLmJyefxt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715256258092,
          "user_tz": 240,
          "elapsed": 30751,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7e2d2610-cf4b-42e0-dd85-4d9a7103f87e"
      },
      "id": "Zp8FLmJyefxt",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TRAINING dataset...\n",
            "Processing VALIDATION dataset...\n",
            "Processing TESTING dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding_value = 0.0\n",
        "batch_size = 5000  # Batch size for training\n",
        "max_sequence = 0\n",
        "\n",
        "for dataset in datasets:\n",
        "  print(f\"Processing: {dataset}\")\n",
        "  sequences = [list(row) for row in dic_data[dataset]['raw'][seq_var]]\n",
        "  labels = list(dic_data[dataset]['raw']['asm'])\n",
        "\n",
        "  if len(max(sequences, key=len)) > max_sequence:\n",
        "    max_sequence = len(max(sequences, key=len))\n",
        "    print(f\" Max sequence length: {max_sequence}\")\n",
        "\n",
        "    # Convert sequences to tensors and pad them\n",
        "\n",
        "  # Convert sequences to tensors and pad them\n",
        "  padded_sequences = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in sequences],\n",
        "                                                    batch_first=True,\n",
        "                                                    padding_value=padding_value)\n",
        "\n",
        "\n",
        "  # Convert labels to a tensor\n",
        "  labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "  if dataset == 'TRAINING':\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=[0, 1], y=labels.numpy())\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "  # Create DataLoader for batch processing\n",
        "  data = TensorDataset(padded_sequences, labels)\n",
        "  dic_data[dataset]['dataloader'] = DataLoader(data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "fsqzN7Ifvu32",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715256274769,
          "user_tz": 240,
          "elapsed": 14925,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f97c54d-fadc-4541-8e90-713cea57511f"
      },
      "id": "fsqzN7Ifvu32",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: TRAINING\n",
            " Max sequence length: 38\n",
            "Processing: VALIDATION\n",
            "Processing: TESTING\n",
            " Max sequence length: 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example data\n",
        "# sequences = [\n",
        "#     [1.5, 2.3, 3.1],\n",
        "#     [0.1, 0.2],\n",
        "#     [1.0, 2.1, 3.0, 4.1]\n",
        "# ]\n",
        "# labels = [0, 1, 0]\n",
        "\n",
        "# # Convert sequences to tensors and pad them\n",
        "# padded_sequences = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in sequences], batch_first=True, padding_value=0.0)\n",
        "\n",
        "# # Convert labels to a tensor\n",
        "# labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# # Create DataLoader for batch processing\n",
        "# dataset = TensorDataset(padded_sequences, labels)\n",
        "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# # Compute class weights\n",
        "# class_weights = compute_class_weight('balanced', classes=[0, 1], y=labels.numpy())\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "# # Define the BCELoss with class weights correctly applied\n",
        "# criterion = nn.BCELoss(weight=class_weights[1])\n",
        "\n",
        "# # Define a Transformer model\n",
        "# class TransformerModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(TransformerModel, self).__init__()\n",
        "#         self.encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=2, dim_feedforward=128)\n",
        "#         self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "#         self.position_embeddings = nn.Embedding(100, 32)  # Assuming max sequence length is 100\n",
        "#         self.fc = nn.Linear(32, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.unsqueeze(-1).repeat(1, 1, 32)  # Extend features to d_model size\n",
        "#         seq_length, N = x.shape[1], x.shape[0]\n",
        "#         positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).repeat(N, 1)\n",
        "#         x += self.position_embeddings(positions)\n",
        "#         x = x.permute(1, 0, 2)  # Reshape x to [seq_length, batch, features]\n",
        "#         x = self.transformer_encoder(x)\n",
        "#         x = self.fc(x[-1, :, :])  # Take the last sequence output\n",
        "#         return torch.sigmoid(x.view(-1))  # Ensure output is flat\n",
        "\n",
        "# # Instantiate model and move it to the selected device\n",
        "# transformer_model = TransformerModel().to(device)\n",
        "\n",
        "# # Define optimizer\n",
        "# optimizer_transformer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(10):\n",
        "#     for data, targets in dataloader:\n",
        "#         data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "#         # Training Transformer\n",
        "#         transformer_model.zero_grad()\n",
        "#         outputs = transformer_model(data)\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer_transformer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTWzkJ4lc0n_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715116309408,
          "user_tz": 240,
          "elapsed": 153,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b19a07ab-8024-4b94-dbc0-527699ec7c1a"
      },
      "id": "GTWzkJ4lc0n_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.5338258743286133\n",
            "Epoch 2, Loss: 0.2836953401565552\n",
            "Epoch 3, Loss: 0.46044814586639404\n",
            "Epoch 4, Loss: 0.6938964128494263\n",
            "Epoch 5, Loss: 0.21642085909843445\n",
            "Epoch 6, Loss: 0.1471812129020691\n",
            "Epoch 7, Loss: 0.4084697663784027\n",
            "Epoch 8, Loss: 0.2152155339717865\n",
            "Epoch 9, Loss: 0.08698108792304993\n",
            "Epoch 10, Loss: 0.17779676616191864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# # Check for GPU\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(f\"Using device: {device}\")\n",
        "\n",
        "# # Example data\n",
        "# sequences = [\n",
        "#     [1.5, 2.3, 3.1],\n",
        "#     [0.1, 0.2],\n",
        "#     [1.0, 2.1, 3.0, 4.1]\n",
        "# ]\n",
        "# labels = [0, 1, 0]\n",
        "\n",
        "# # Convert sequences to tensors and pad them\n",
        "# padded_sequences = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in sequences], batch_first=True, padding_value=0.0)\n",
        "\n",
        "# # Convert labels to a tensor\n",
        "# labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# # Create DataLoader for batch processing\n",
        "# dataset = TensorDataset(padded_sequences, labels)\n",
        "# dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# # Compute class weights\n",
        "# class_weights = compute_class_weight('balanced', classes=[0, 1], y=labels.numpy())\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "# # Define the BCELoss with class weights correctly applied\n",
        "# criterion = nn.BCELoss(weight=class_weights[1])\n",
        "\n",
        "# # Define an RNN model\n",
        "# class RNNModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(RNNModel, self).__init__()\n",
        "#         self.rnn = nn.LSTM(input_size=1, hidden_size=64, batch_first=True)\n",
        "#         self.fc = nn.Linear(64, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x, _ = self.rnn(x)\n",
        "#         x = self.fc(x[:, -1, :])  # Take the output of the last sequence step\n",
        "#         return torch.sigmoid(x.view(-1))  # Ensure output is flat\n",
        "\n",
        "# # Instantiate model and move it to the selected device\n",
        "# rnn_model = RNNModel().to(device)\n",
        "\n",
        "# # Define optimizer\n",
        "# optimizer_rnn = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(10):\n",
        "#     for data, targets in dataloader:\n",
        "#         data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "#         # Training RNN\n",
        "#         rnn_model.zero_grad()\n",
        "#         outputs = rnn_model(data.unsqueeze(-1))  # Add a feature dimension\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer_rnn.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# # Prediction can be done by calling rnn_model(data) after training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_QEtthpttj4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715113240398,
          "user_tz": 240,
          "elapsed": 162,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "b42552af-69e1-401c-d3e8-c44236be7c5a"
      },
      "id": "C_QEtthpttj4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch 1, Loss: 0.957861065864563\n",
            "Epoch 2, Loss: 1.1194591522216797\n",
            "Epoch 3, Loss: 0.923400342464447\n",
            "Epoch 4, Loss: 0.9046634435653687\n",
            "Epoch 5, Loss: 0.8847229480743408\n",
            "Epoch 6, Loss: 0.8639021515846252\n",
            "Epoch 7, Loss: 1.1244492530822754\n",
            "Epoch 8, Loss: 1.1254537105560303\n",
            "Epoch 9, Loss: 0.8033512830734253\n",
            "Epoch 10, Loss: 1.1244105100631714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import TensorDataset, DataLoader\n",
        "# from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# # Device configuration\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Hyperparameters\n",
        "# input_size = 32    # Number of features (change this to make it divisible by num_heads)\n",
        "# hidden_size = 128  # Number of hidden units in the RNN/Transformer\n",
        "# num_layers = 2     # Number of layers in the RNN/Transformer\n",
        "# num_heads = 8      # Number of attention heads (for the Transformer)\n",
        "# batch_size = 64    # Batch size\n",
        "# num_epochs = 100   # Number of training epochs\n",
        "# lr = 0.001         # Learning rate\n",
        "\n",
        "# # Example dataset\n",
        "# sequences = [[0.1, 0.2, 0.3], [0.4, 0.5], [0.6, 0.7, 0.8, 0.9]]\n",
        "# labels = [0, 1, 0]\n",
        "\n",
        "# # Convert data to PyTorch tensors\n",
        "# seq_lengths = [len(seq) for seq in sequences]\n",
        "# seq_tensor = torch.tensor([seq + [0.0] * (max(seq_lengths) - len(seq)) for seq in sequences], dtype=torch.float32)\n",
        "# label_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# # Create a TensorDataset and DataLoader\n",
        "# dataset = TensorDataset(seq_tensor, label_tensor)\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # RNN Model\n",
        "# class RNNModel(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_layers):\n",
        "#         super(RNNModel, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.num_layers = num_layers\n",
        "#         self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_size, 2)  # Binary classification\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Pack padded batch of sequences for RNN module\n",
        "#         packed_input = nn.utils.rnn.pack_padded_sequence(x, [len(seq) for seq in x], batch_first=True, enforce_sorted=False)\n",
        "\n",
        "#         # Forward propagate RNN\n",
        "#         out, _ = self.rnn(packed_input)\n",
        "\n",
        "#         # Unpack padding\n",
        "#         out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "\n",
        "#         # Get the last output from each sequence\n",
        "#         out = out[torch.arange(out.size(0)), [len(seq) - 1 for seq in x]]\n",
        "\n",
        "#         # Pass through fully connected layer\n",
        "#         out = self.fc(out)\n",
        "#         return out\n",
        "\n",
        "# # Transformer Model\n",
        "# class TransformerModel(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_layers, num_heads):\n",
        "#         super(TransformerModel, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.num_layers = num_layers\n",
        "#         self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(hidden_size, num_heads), num_layers)\n",
        "#         self.fc = nn.Linear(hidden_size, 2)  # Binary classification\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Add dummy dimension for transformer\n",
        "#         x = x.unsqueeze(1)\n",
        "\n",
        "#         # Forward propagate Transformer Encoder\n",
        "#         out = self.transformer_encoder(x)\n",
        "\n",
        "#         # Get the last output from each sequence\n",
        "#         out = out[:, 0]\n",
        "\n",
        "#         # Pass through fully connected layer\n",
        "#         out = self.fc(out)\n",
        "#         return out\n",
        "\n",
        "\n",
        "# # Function to handle imbalanced dataset\n",
        "# def weighted_binary_cross_entropy(output, target, weights=None):\n",
        "#     if weights is None:\n",
        "#         weights = torch.tensor([1.0, 1.0])\n",
        "#     loss = nn.CrossEntropyLoss(weight=weights)\n",
        "#     return loss(output, target)\n",
        "\n",
        "# # Instantiate and train the model\n",
        "# model = TransformerModel(input_size, hidden_size, num_layers, num_heads).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# criterion = weighted_binary_cross_entropy\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     for batch_seqs, batch_labels in dataloader:\n",
        "#         batch_seqs = batch_seqs.to(device)\n",
        "#         batch_labels = batch_labels.to(device)\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(batch_seqs)\n",
        "#         loss = criterion(outputs, batch_labels)\n",
        "\n",
        "#         # Backward and optimize\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Compute accuracy and F1-score\n",
        "#     y_true = label_tensor.tolist()\n",
        "#     y_pred = torch.max(outputs, 1)[1].cpu().tolist()\n",
        "#     acc = accuracy_score(y_true, y_pred)\n",
        "#     f1 = f1_score(y_true, y_pred)\n",
        "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {acc:.4f}, F1-score: {f1:.4f}')\n",
        "\n",
        "# # Example usage\n",
        "# example_seq = [[0.1, 0.2, 0.3, 0.4]]\n",
        "# example_tensor = torch.tensor(example_seq, dtype=torch.float32).to(device)\n",
        "# output = model(example_tensor)\n",
        "# pred = torch.max(output, 1)[1].item()\n",
        "# print(f\"Predicted label for example sequence: {pred}\")"
      ],
      "metadata": {
        "id": "EvQxh4LTssHS"
      },
      "id": "EvQxh4LTssHS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Convert sequences to tensors and pad them\n",
        "# #padded_sequences = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in sequences], batch_first=True, padding_value=0.0)\n",
        "\n",
        "# # # Convert labels to a tensor\n",
        "# #labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# # # Create DataLoader for batch processing\n",
        "# # dataset = TensorDataset(padded_sequences, labels)\n",
        "# # dataloader = DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# # # Compute class weights\n",
        "# class_weights = compute_class_weight('balanced', classes=[0, 1], y=labels.numpy())\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "# # # Define the BCELoss with class weights correctly applied\n",
        "# criterion = nn.BCELoss(weight=class_weights[1])\n",
        "\n",
        "# # Define a Transformer model\n",
        "# class TransformerModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(TransformerModel, self).__init__()\n",
        "#         self.encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=2, dim_feedforward=128)\n",
        "#         self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "#         self.position_embeddings = nn.Embedding(100, 32)  # Assuming max sequence length is 100\n",
        "#         self.fc = nn.Linear(32, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.unsqueeze(-1).repeat(1, 1, 32)  # Extend features to d_model size\n",
        "#         seq_length, N = x.shape[1], x.shape[0]\n",
        "#         positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).repeat(N, 1)\n",
        "#         x += self.position_embeddings(positions)\n",
        "#         x = x.permute(1, 0, 2)  # Reshape x to [seq_length, batch, features]\n",
        "#         x = self.transformer_encoder(x)\n",
        "#         x = self.fc(x[-1, :, :])  # Take the last sequence output\n",
        "#         return torch.sigmoid(x.view(-1))  # Ensure output is flat\n",
        "\n",
        "# # # Instantiate model and move it to the selected device\n",
        "# transformer_model = TransformerModel().to(device)\n",
        "\n",
        "# # # Define optimizer\n",
        "# optimizer_transformer = torch.optim.Adam(transformer_model.parameters(), lr=0.001)\n",
        "\n",
        "# # # Training loop\n",
        "# for epoch in range(100):\n",
        "#     for data, targets in dataloader:\n",
        "#         data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "#         # Training Transformer\n",
        "#         transformer_model.zero_grad()\n",
        "#         outputs = transformer_model(data)\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer_transformer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "PQVvOF-tsuj5"
      },
      "id": "PQVvOF-tsuj5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\n",
        "    [1.5, 2.3, 3.1],\n",
        "    [0.1, 0.2],\n",
        "    [1.0, 2.1, 3.0, 4.1]\n",
        "]\n",
        "labels = [0, 1, 0]"
      ],
      "metadata": {
        "id": "aDkIPapz2xuN"
      },
      "id": "aDkIPapz2xuN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq1GCKxnpN9-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715179200176,
          "user_tz": 240,
          "elapsed": 129,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "378cfdba-6a73-4fa6-8f3f-4fc7dbda524b"
      },
      "id": "Nq1GCKxnpN9-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28.9716, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "d_model = 32  # Dimensionality of the model\n",
        "nhead = 4  # Number of heads in the multi-head attention models\n",
        "num_layers = 8  # Number of sub-encoder-layers in the transformer\n",
        "dim_feedforward = 128  # Size of the feedforward model in nn.TransformerEncoder\n",
        "max_seq_length = max_sequence\n",
        "learning_rate = 0.0005  # Learning rate for the optimizer\n",
        "num_epochs = 20  # Number of training epochs\n",
        "dropout = 0.3\n",
        "weight_decay = 0.01\n",
        "\n",
        "# # Define a Transformer model\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, max_seq_length, num_layers, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,  # Adding dropout to the encoder layer\n",
        "            layer_norm_eps=1e-6  # Using a smaller epsilon for layer normalization for more precise calculations\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
        "        self.position_embeddings = nn.Embedding(max_seq_length, d_model)  # Prepare position embeddings\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1).repeat(1, 1, d_model)  # Extend features to match d_model\n",
        "        seq_length, N = x.shape[1], x.shape[0]\n",
        "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).repeat(N, 1)\n",
        "        x += self.position_embeddings(positions)\n",
        "        x = x.permute(1, 0, 2)  # Reshape x to [seq_length, batch, features]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.fc(x[-1, :, :])  # Take the last sequence output\n",
        "        return torch.sigmoid(x.view(-1))  # Flatten the output for compatibility with target\n",
        "\n",
        "# Define loss function with class weights\n",
        "criterion = nn.BCELoss(weight=class_weights[1])\n",
        "\n",
        "# Instantiate model and move it to the selected device\n",
        "transformer_model = TransformerModel(d_model, nhead, dim_feedforward, max_seq_length, num_layers, dropout).to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer_transformer = torch.optim.Adam(transformer_model.parameters(),\n",
        "                                         lr=learning_rate,\n",
        "                                         weight_decay = weight_decay)\n",
        "\n",
        "# # Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for data, targets in dic_data['TRAINING']['dataloader']:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Training Transformer\n",
        "        transformer_model.zero_grad()\n",
        "        outputs = transformer_model(data)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer_transformer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I9vtmdBvjq5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715258600567,
          "user_tz": 240,
          "elapsed": 664420,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c83f0bd2-ed8c-4f6a-fcff-9da68a900f5d"
      },
      "id": "0I9vtmdBvjq5",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.7219479084014893\n",
            "Epoch 2, Loss: 2.489398956298828\n",
            "Epoch 3, Loss: 2.6095423698425293\n",
            "Epoch 4, Loss: 1.9272940158843994\n",
            "Epoch 5, Loss: 1.8746535778045654\n",
            "Epoch 6, Loss: 1.9068864583969116\n",
            "Epoch 7, Loss: 1.9541019201278687\n",
            "Epoch 8, Loss: 1.6243597269058228\n",
            "Epoch 9, Loss: 1.5730787515640259\n",
            "Epoch 10, Loss: 1.6436265707015991\n",
            "Epoch 11, Loss: 1.5704618692398071\n",
            "Epoch 12, Loss: 1.6391793489456177\n",
            "Epoch 13, Loss: 1.836776852607727\n",
            "Epoch 14, Loss: 1.6030900478363037\n",
            "Epoch 15, Loss: 1.7205359935760498\n",
            "Epoch 16, Loss: 1.6805227994918823\n",
            "Epoch 17, Loss: 1.609703540802002\n",
            "Epoch 18, Loss: 1.8406842947006226\n",
            "Epoch 19, Loss: 1.7668598890304565\n",
            "Epoch 20, Loss: 1.853647232055664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, criterion, device, threshold_prediction = 0.5):\n",
        "    model = model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in dataloader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Get the predictions\n",
        "            preds = (outputs > threshold_prediction).float()  # Threshold the output\n",
        "\n",
        "            # Update lists\n",
        "            all_preds.extend(preds.view(-1).cpu().numpy())\n",
        "            all_targets.extend(targets.view(-1).cpu().numpy())\n",
        "\n",
        "            # Update loss\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "\n",
        "    # Calculate the average loss\n",
        "    average_loss = total_loss / len(dataloader.dataset)\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(all_targets, all_preds, output_dict=True)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(\"Classification Report:\\n\")\n",
        "    print(classification_report(all_targets, all_preds))\n",
        "\n",
        "    # Additionally, print out the average loss\n",
        "    print(f\"Average Test Loss: {average_loss:.4f}\")\n",
        "\n",
        "    # Return the report as a dictionary for further analysis if needed\n",
        "    return report\n",
        "\n",
        "# Assuming you have your model, test_dataloader, and criterion defined, you can call:\n",
        "report = evaluate_model(transformer_model, dic_data['TESTING']['dataloader'], criterion, device, 0.15)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "4LlYwtn72-d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715258997753,
          "user_tz": 240,
          "elapsed": 5973,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7c8fdeee-c66b-4008-d56d-db0b43d3aa45"
      },
      "id": "4LlYwtn72-d8",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.97      0.98    193452\n",
            "         1.0       0.32      0.58      0.42      4633\n",
            "\n",
            "    accuracy                           0.96    198085\n",
            "   macro avg       0.66      0.78      0.70    198085\n",
            "weighted avg       0.97      0.96      0.97    198085\n",
            "\n",
            "Average Test Loss: 2.0233\n",
            "{'0.0': {'precision': 0.9898076416337286, 'recall': 0.9708661580133573, 'f1-score': 0.9802454058172975, 'support': 193452}, '1.0': {'precision': 0.3238152369526095, 'recall': 0.5825598963954242, 'f1-score': 0.4162553979025293, 'support': 4633}, 'accuracy': 0.9617840825908069, 'macro avg': {'precision': 0.656811439293169, 'recall': 0.7767130272043907, 'f1-score': 0.6982504018599134, 'support': 198085}, 'weighted avg': {'precision': 0.9742307791207285, 'recall': 0.9617840825908069, 'f1-score': 0.9670542721793687, 'support': 198085}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 3\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                           Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.delta = delta\n",
        "        self.best_score = None\n",
        "        self.epochs_no_improve = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.epochs_no_improve += 1\n",
        "            if self.epochs_no_improve >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.epochs_no_improve = 0\n",
        "\n",
        "        if self.early_stop and self.verbose:\n",
        "            print(\"Early stopping\")\n",
        "\n",
        "def train_model(model, device, dataloaders, criterion, optimizer, threshold_prediction = 0.5, num_epochs=10):\n",
        "    model = model.to(device)\n",
        "    early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['TRAINING', 'VALIDATION']:\n",
        "            if phase == 'TRAINING':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "\n",
        "            # Iterate over data.\n",
        "            for data, targets in dataloaders[phase]:\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "                if phase == 'TRAINING':\n",
        "                    loss, preds = train_step(model, data, targets, criterion, optimizer, threshold_prediction, device)\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(data)\n",
        "                        loss = criterion(outputs, targets)\n",
        "                        preds = (outputs > threshold_prediction).float()\n",
        "\n",
        "                running_loss += loss.item() * data.size(0)\n",
        "                all_preds.extend(preds.view(-1).cpu().numpy())\n",
        "                all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_f1 = f1_score(all_targets, all_preds, average=None)  # Calculate F1 for each class\n",
        "\n",
        "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} F1-Score (per class): {epoch_f1}')\n",
        "\n",
        "            if phase == 'VALIDATION':\n",
        "                early_stopping(epoch_loss)\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"Early stopping triggered.\")\n",
        "                    return\n",
        "\n",
        "def train_step(model, data, targets, criterion, optimizer, threshold_prediction, device):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, targets)\n",
        "    preds = (outputs > threshold_prediction).float()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, preds\n",
        "\n",
        "# Assuming you have your model, dataloaders, criterion, and optimizer defined, you can call:\n",
        "train_model(transformer_model,\n",
        "            device,\n",
        "            {'TRAINING': dic_data['TRAINING']['dataloader'], 'VALIDATION': dic_data['VALIDATION']['dataloader']},\n",
        "            criterion,\n",
        "            optimizer_transformer,\n",
        "            num_epochs = num_epochs,\n",
        "            )"
      ],
      "metadata": {
        "id": "Em_P5lNT77qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1715217664379,
          "user_tz": 240,
          "elapsed": 2556628,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7a94a489-ba10-4d3d-8dd3-6a92b1359f72"
      },
      "id": "Em_P5lNT77qh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Training Loss: 1.8361 F1-Score (per class): [9.91277549e-01 8.95977063e-04]\n",
            "Validation Loss: 1.1724 F1-Score (per class): [0.99408825 0.        ]\n",
            "Epoch 2/100\n",
            "Training Loss: 1.7535 F1-Score (per class): [9.91290989e-01 5.38599641e-04]\n",
            "Validation Loss: 1.1507 F1-Score (per class): [0.99408825 0.        ]\n",
            "Epoch 3/100\n",
            "Training Loss: 1.7528 F1-Score (per class): [0.99129251 0.00125606]\n",
            "Validation Loss: 1.1527 F1-Score (per class): [0.99408825 0.        ]\n",
            "Epoch 4/100\n",
            "Training Loss: 1.7393 F1-Score (per class): [9.91288636e-01 3.59034198e-04]\n",
            "Validation Loss: 1.1638 F1-Score (per class): [0.99408825 0.        ]\n",
            "Epoch 5/100\n",
            "Training Loss: 1.7503 F1-Score (per class): [9.91295723e-01 5.38889887e-04]\n",
            "Validation Loss: 1.1686 F1-Score (per class): [0.99408825 0.        ]\n",
            "Epoch 6/100\n",
            "Training Loss: 1.7365 F1-Score (per class): [9.91294934e-01 5.38841491e-04]\n",
            "Validation Loss: 1.1653 F1-Score (per class): [0.99408825 0.        ]\n",
            "Epoch 7/100\n",
            "Training Loss: 1.7520 F1-Score (per class): [0.99129576 0.        ]\n",
            "Validation Loss: 1.2011 F1-Score (per class): [0.99408825 0.        ]\n",
            "Early stopping\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "sequential models"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}