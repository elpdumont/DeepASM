{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71a3685-b41b-4dd7-8987-fb26729209a6",
   "metadata": {},
   "source": [
    "# DeepASM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca0fb9-e45a-4d26-be20-575214beec95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa06369-0a4f-4120-8b0f-a4dfcb5e7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To manipulate HDF5 files (RUN FOR ALL MODELS)\n",
    "!pip3 install --upgrade tables\n",
    "\n",
    "# Install Decision Forest models\n",
    "!pip3 install tensorflow_decision_forests==0.2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecb353-258f-451d-8c2a-2488137356b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb55e93-2391-4791-bca2-a69b4ec7a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Python packages for data, stats, and visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import random\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Kernel functions\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from numpy import asarray\n",
    "from matplotlib import pyplot\n",
    "from numpy import exp\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA, KernelPCA, NMF, TruncatedSVD\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding, SpectralEmbedding\n",
    "\n",
    "# To get the time\n",
    "from datetime import datetime\n",
    "\n",
    "# To write on the same line\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Decision tree algorithms\n",
    "import tensorflow_decision_forests as tfdf\n",
    " \n",
    "# Figure parameters\n",
    "mpl.rcParams['figure.figsize'] = (10, 10)\n",
    "mpl.rcParams['axes.titlesize'] = 15\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ebf71-130f-4ea0-8eaa-c27478ebb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print different versions\n",
    "print(sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddb628-6484-4f52-9386-ced2fb0f456a",
   "metadata": {},
   "source": [
    "## GCP Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb33deb-7b19-4795-acd3-4511eb464de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw data from bucket. False if you want to import the processed dataset\n",
    "IMPORT_RAW_FROM_BUCKET = True\n",
    "\n",
    "# Export data after it's been prepared\n",
    "EXPORT_PROCESSED_DATA = True\n",
    "\n",
    "# Import data after its features have been prepared\n",
    "IMPORT_PROCESSED_DATA = False\n",
    "PROCESSED_DATA_PATH = \"deepasm/notebook/250bp_500000rows_2022-03-20_23-24-20\"\n",
    "\n",
    "# Bucket name where the training datasets are\n",
    "DEEPASM_BUCKET=\"deepasm\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dbab41-0938-4ddf-83b3-f216430ac6de",
   "metadata": {},
   "source": [
    "## Dataset used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6a036-22a5-46ae-8095-25b43b70696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training on a small sample size (50k)\n",
    "TEST_RUN = True\n",
    "\n",
    "# Size of the genomic window (250bp, 500bp, 1000bp)\n",
    "GENOMIC_INTERVAL = 250\n",
    "\n",
    "# Number of rows to take into the dataset after import\n",
    "if TEST_RUN == True:\n",
    "    NB_ROWS_RAW_DATASET = int(500000) # The maximum is 5e6. We use 200k to test the code\n",
    "else:\n",
    "    NB_ROWS_RAW_DATASET = int(5e6) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb144c7-b3d9-4ca7-836b-027939589d48",
   "metadata": {},
   "source": [
    "## Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f672e-93d9-485a-9914-010650158ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# Gradient-Boosted algorithms\n",
    "\n",
    "GROWING_STRATEGY = \"BEST_FIRST_GLOBAL\"  # LOCAL (default). or BEST_FIRST_GLOBAL is the default\n",
    "NUM_TREES = 600 # Maxmimum number of decision trees. (default 300)\n",
    "MIN_EXAMPLES = 10 # Minimum number of examples in a node (default: 5)\n",
    "MAX_DEPTH = 12 # Maximum depth of the tree (default 6)\n",
    "SUBSAMPLE = 0.5 # Ratio of the dataset (sampling without replacement) used to train individual trees for the random sampling method (Default 1)\n",
    "SAMPLING_METHOD = \"RANDOM\"\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Parameters common to all models\n",
    "\n",
    "# Minimum correlation factor. Under that, remove features\n",
    "MIN_CORR = 0.05\n",
    "\n",
    "# Kernel values for probability estimates\n",
    "KERNEL_FM_NB_VALUES = 10\n",
    "KERNEL_FM_BANDWIDTH = 0.1\n",
    "KERNEL_COV_NB_MAX = 200\n",
    "KERNEL_COV_NB_STEP = 40\n",
    "KERNEL_COV_BANDWIDTH = 20\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc', \n",
    "    verbose=0,\n",
    "    patience=10,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Percentage of data points to be used in the Test dataset\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "# Percentage of datapoints used between training and validation\n",
    "VALIDATION_SPLIT = 0.3 # How to divide the training dataset for validation\n",
    "\n",
    "EPOCHS = 100 # We have so many datapoints that 20 epochs are enough to stabilize the training\n",
    "BATCH_SIZE = 1000 # to get a few identified ASM we need at a few hundreds since the\n",
    "# frequency of ASM is 1.38%\n",
    "# A batch size of 1000 will run into a memory error on TF 2.7\n",
    "\n",
    "# Regularlization L1 and L2 (defaults are l1 = 0.01 and l2 = 0.01)\n",
    "L1_R = 0\n",
    "L2_R = 1e-3\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Parameters common to neural network models\n",
    "ACTIVATION_FUNCTION = 'tanh' # 'tanh' # or 'relu' or 'gelu (Gaussian Error Linear Unit)'\n",
    "NB_NODES_PERCEPTRON = 60\n",
    "NB_LAYERS_PERCEPTRON = 5\n",
    "NB_NODES_AFTER_CNN = 2\n",
    "\n",
    "# CNN parameters\n",
    "CNN_FILTERS = 16\n",
    "CNN_KERNEL = 100 # Must be smaller than the genomic region (250). The av distance between CpG is 37 bp and the std dev of the distances between cpgs is 24 bp\n",
    "LEARNING_RATE = 3e-4 \n",
    "\n",
    "# Learning rate was taken from this\n",
    "# http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Parameters common to RNN\n",
    "\n",
    "RNN_UNITS = 128 # 64 orginally\n",
    "\n",
    "#--------------------------------------------------\n",
    "# SPECIFIC TO RANDOM FOREST ALGORITHM\n",
    "use_raw_df_for_forest_models = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22515ca-30c7-4a83-a982-6eb67b884f37",
   "metadata": {},
   "source": [
    "## ML evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38b202-fe69-489c-aae0-638bcd40b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='sensitivity'),\n",
    "      keras.metrics.AUC(name='auc')\n",
    "      ]\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'auc', 'precision', 'sensitivity']\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def display_results(df_results):\n",
    "  print(\"Loss\", np.round(df_results[0], 3))\n",
    "  print(\"True positives\", np.round(df_results[1], 3))\n",
    "  print(\"False positives\", np.round(df_results[2], 3))\n",
    "  print(\"True negatives\", np.round(df_results[3], 3))\n",
    "  print(\"False negatives\", np.round(df_results[4], 3))\n",
    "  print(\"Accuracy\", np.round(df_results[5], 3))\n",
    "  print(\"Precision\", np.round(df_results[6], 3))\n",
    "  print(\"Sensitivity\", np.round(df_results[7], 3))\n",
    "  print(\"AUC\", np.round(df_results[8], 3))\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.5,80])\n",
    "  plt.ylim([0,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e5515-6785-4461-b139-ced8487e2eee",
   "metadata": {},
   "source": [
    "## Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440a700-3f8d-443b-87f4-3036f2a0f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_RAW_FROM_BUCKET == True:\n",
    "    !gsutil ls gs://$DEEPASM_BUCKET/$GENOMIC_INTERVAL*bp/encode_training_data_with_genomic_picture/*.json > list_to_download.txt\n",
    "    files_to_download_df = pd.read_csv('list_to_download.txt', header=None)\n",
    "    print(\"Number of files to download:\", files_to_download_df.shape[0])\n",
    "\n",
    "    imported_df = pd.DataFrame()\n",
    "    \n",
    "    if TEST_RUN == True:\n",
    "        range_files = 1\n",
    "    else: # Download all the files\n",
    "        range_files = files_to_download_df.shape[0]\n",
    "\n",
    "    for index_file in range(range_files): \n",
    "        file_name_bucket = files_to_download_df[0][index_file]\n",
    "        local_file_name = \"training_\" + str(index_file) + \".json\"\n",
    "        \n",
    "        # Download the file from bucket\n",
    "        !gsutil cp $file_name_bucket $local_file_name\n",
    "        \n",
    "        print(\"Appending file...\")\n",
    "        imported_df = imported_df.append(pd.read_json(local_file_name, lines = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a257f-e990-4f89-bec0-0011061db225",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the imported dataset:\", imported_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80fcd0-5fd3-4c31-9073-25ef8c137027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported_df['nb_reads'].describe()\n",
    "imported_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44574e-a6e1-4e5b-b254-bcf6959d60a0",
   "metadata": {},
   "source": [
    "## Prepare the features\n",
    "\n",
    "Note: we do not randomize the rows because the scripts preceding this notebook already sampled the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f096d-c8fd-4270-b822-97fd20c94331",
   "metadata": {},
   "source": [
    "### Copy & clean dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1bbac4-bc09-4786-b688-b8976eeac8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy of the dataframe\n",
    "# raw_df = imported_df.copy()\n",
    "# raw_df = raw_df.head(NB_ROWS_RAW_DATASET)\n",
    "# # Randomize the rows\n",
    "# raw_df = raw_df.sample(frac = 1, ignore_index = True)\n",
    "\n",
    "# USED FOR TESTING\n",
    "df_with_asm = imported_df[imported_df['asm_snp'] == 1].head(20).reset_index(drop=True)\n",
    "df_without_asm = imported_df[imported_df['asm_snp'] == 0].head(20).reset_index(drop=True)\n",
    "                     \n",
    "raw_df = df_with_asm.append(df_without_asm).reset_index(drop=True)                     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d48e94-5264-4e01-913a-222b72bde9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the dataset: \", raw_df.shape)\n",
    "print(\"Number of samples (12 expected):\", len(raw_df['sample'].unique()))\n",
    "print(\"Chromosomes found (24 expected):\", raw_df['chr'].unique())\n",
    "\n",
    "# We remove the sample column\n",
    "raw_df = raw_df.drop('sample', axis = 1)\n",
    "\n",
    "# We remove chromosomes X and Y because ASM cannot be reliabily evaluated in these chromosomes\n",
    "raw_df = raw_df[raw_df['chr'] != 'X']\n",
    "raw_df = raw_df[raw_df['chr'] != 'Y']\n",
    "\n",
    "# We remove the chr column\n",
    "raw_df = raw_df.drop('chr', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a306c-f998-48b3-812b-0900d382663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY\n",
    "raw_df = raw_df[['asm_snp', 'nb_reads', 'nb_cpg_found',  'genomic_picture', 'cpg_cov', 'cpg_pos', 'cpg_fm']].reset_index(drop=True)\n",
    "raw_df\n",
    "#raw_df['genomic_picture'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0c078-541c-4563-a68a-4dd87bb34e5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Calculate the distance between CpGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2362ad-6e00-4d94-961d-8081ada1d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the distance between CpGs (~3min)\n",
    "def dist_cpg(cpg_pos):\n",
    "  distances = []\n",
    "  for index in range(len(cpg_pos)):\n",
    "    if index >= len(cpg_pos)-1:\n",
    "      return distances\n",
    "    else:\n",
    "      distances.append(cpg_pos[index + 1] - cpg_pos[index])\n",
    "  return distances\n",
    "\n",
    "# Apply the function \"distance\" to the array of CpG positions\n",
    "raw_df['cpg_dist'] = raw_df['cpg_pos'].apply(lambda x: dist_cpg(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc6f7c-ccef-4681-a609-dbde5ac5e379",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Convert arrays into numerical features\n",
    "\n",
    "To do this, we use kernel estimates as well as simpler metrics like mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653f9e8-b3a9-4077-8bd9-52d25fd22626",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Kernel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5452c4c-7ff6-43a5-97ec-da1ccbdee6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FRACTIONAL METHYLATION\n",
    "\n",
    "# Values for fractional methylation (between 0 and 1)\n",
    "values_for_kernel_fm = asarray([value for value in range(0, KERNEL_FM_NB_VALUES+1)])\n",
    "values_for_kernel_fm = values_for_kernel_fm / KERNEL_FM_NB_VALUES\n",
    "print(\"X-axis values used for the FM kernel estimate:\", values_for_kernel_fm)\n",
    "values_for_kernel_fm = values_for_kernel_fm.reshape((len(values_for_kernel_fm), 1))\n",
    "\n",
    "# Build Kernel model\n",
    "kernel_fm_model = KernelDensity(bandwidth=KERNEL_FM_BANDWIDTH, kernel='gaussian')\n",
    "\n",
    "# Function to be applied to each array in the columns read_fm and cpg_fm\n",
    "def estimate_kernels_fm(x):\n",
    "  sample = np.reshape(x, (len(x), 1))\n",
    "  kernel_fm_model.fit(sample)\n",
    "  probabilities = kernel_fm_model.score_samples(values_for_kernel_fm)\n",
    "  probabilities = exp(probabilities)\n",
    "  return np.round(probabilities, 4)\n",
    "\n",
    "# Try function\n",
    "#estimate_kernels_fm(raw_df['cpg_fm'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6d384-d6a5-4ad9-bc40-74584337290f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVERAGE AND CPG DISTANCE\n",
    "\n",
    "# Values for fractional methylation (between 0 and 1)\n",
    "values_for_kernel_cov = asarray([value for value in range(0, KERNEL_COV_NB_MAX, KERNEL_COV_NB_STEP)])\n",
    "print(\"Values used in kernel estimate:\", values_for_kernel_cov)\n",
    "values_for_kernel_cov = values_for_kernel_cov.reshape((len(values_for_kernel_cov), 1))\n",
    "\n",
    "# Build Kernel model\n",
    "kernel_cov_model = KernelDensity(bandwidth=KERNEL_COV_BANDWIDTH, kernel='gaussian')\n",
    "\n",
    "# Function to be applied to each array in the columns read_fm and cpg_fm\n",
    "def estimate_kernels_cov(x):\n",
    "  sample = np.reshape(x, (len(x), 1))\n",
    "  kernel_fm_model.fit(sample)\n",
    "  probabilities = kernel_fm_model.score_samples(values_for_kernel_cov)\n",
    "  probabilities = exp(probabilities)\n",
    "  return np.round(probabilities, 4)\n",
    "\n",
    "# Try function\n",
    "#estimate_kernels_cov(raw_df['cpg_cov'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed3b36-3216-458e-83f5-4e24c38c814e",
   "metadata": {},
   "source": [
    "#### Test kernel estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775a79c-f61b-48fb-bd6e-555c3d12e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_to_plot = 'cpg_fm' # cpg_fm or read_fm or cpg_dist or cpg_cov\n",
    "\n",
    "n_extract = 10\n",
    "extract_asm = raw_df[raw_df['asm_snp'] == 1].sample(n=n_extract, ignore_index = True)\n",
    "extract_noasm = raw_df[raw_df['asm_snp'] == 0].sample(n=n_extract, ignore_index = True)\n",
    "n_x = round(np.sqrt(n_extract))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb3ee5-1333-43a5-99f0-a7481f682a7d",
   "metadata": {},
   "source": [
    "##### Plots for regions with ASM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309fc8e-8acb-4955-bb8e-53fa1b602fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (10, 10)\n",
    "fig, axs = plt.subplots(n_x, n_x, sharey=True, sharex=True, tight_layout=True)\n",
    "\n",
    "for k in range(n_x):\n",
    "  for m in range(n_x):\n",
    "\n",
    "    # Print data distribution\n",
    "    data_distribution = extract_asm[variable_to_plot][k+m]\n",
    "    axs[k,m].hist(data_distribution, density = True, bins = 10)\n",
    "\n",
    "    # Print kernel density\n",
    "    if 'fm' in variable_to_plot:\n",
    "        #print(\"Using the FM kernel estimates\")\n",
    "        kernel_probabilities = estimate_kernels_fm(data_distribution)\n",
    "        values = values_for_kernel_fm\n",
    "    else:\n",
    "        #print(\"Using the COV kernel estimates\")\n",
    "        kernel_probabilities = estimate_kernels_cov(data_distribution)\n",
    "        values = values_for_kernel_cov\n",
    "    axs[k,m].plot(values[:], kernel_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eef467-301c-4c3a-8c14-65c63bd6704e",
   "metadata": {},
   "source": [
    "##### Plots for regions without ASM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fe19d-1890-45e8-9429-bd26f9503dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (10, 10)\n",
    "fig, axs = plt.subplots(n_x, n_x, sharey=True, sharex=True, tight_layout=True)\n",
    "\n",
    "for k in range(n_x):\n",
    "  for m in range(n_x):\n",
    "\n",
    "    # Print data distribution\n",
    "    data_distribution = extract_noasm[variable_to_plot][k+m]\n",
    "    axs[k,m].hist(data_distribution, density = True, bins = 10)\n",
    "\n",
    "    # Print kernel density\n",
    "    if 'fm' in variable_to_plot:\n",
    "        #print(\"Using the FM kernel estimates\")\n",
    "        kernel_probabilities = estimate_kernels_fm(data_distribution)\n",
    "        values = values_for_kernel_fm\n",
    "    else:\n",
    "        #print(\"Using the COV kernel estimates\")\n",
    "        kernel_probabilities = estimate_kernels_cov(data_distribution)\n",
    "        values = values_for_kernel_cov\n",
    "    axs[k,m].plot(values[:], kernel_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474e90d-b2cf-4b02-9627-ebb593bc1a2b",
   "metadata": {},
   "source": [
    "#### Calculate the mean, std, and kernel estimates of arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85aea5-083a-47ca-93cc-7a554a3f7cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arrays(df, column_name):\n",
    "  \"\"\"Inputs: dataframe and a column name that contains arrays\"\"\"\n",
    "\n",
    "  # Mean and Standard deviation\n",
    "  std_name = \"std_\" + column_name\n",
    "  av_name = \"mean_\" + column_name\n",
    "\n",
    "  print(\"Calculating the standard deviation\")\n",
    "  df[std_name] = df[column_name].apply(lambda x: np.round(np.std(x), 4))\n",
    "  print(\"Calculating the average\")\n",
    "  df[av_name] = df[column_name].apply(lambda x: np.round(np.mean(x), 4))\n",
    "  \n",
    "  # Kernel density estimates\n",
    "  kernel_name = \"kernel_\" + column_name\n",
    "  if (column_name == 'cpg_cov' or column_name == 'cpg_dist'):\n",
    "    print(\"Calculating the proba distribution for cov or dist\")\n",
    "    df[kernel_name] = df[column_name].apply(lambda x: estimate_kernels_cov(x))\n",
    "  else:\n",
    "    print(\"Calculating the proba distribution for fractional methylation\")\n",
    "    df[kernel_name] = df[column_name].apply(lambda x: estimate_kernels_fm(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d5288-af28-40f5-8bb5-9abf3d30014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "for col in ['read_fm', 'cpg_fm', 'cpg_dist']:\n",
    "    clear_output(wait=True)\n",
    "    print(\"Column: \", col)\n",
    "    convert_arrays(raw_df, col)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522320d-cfe7-457d-9d50-e6394d71a68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_kernel_array(col):\n",
    "    # Col must be a column of kernel estimate arrays\n",
    "    print(\"Processing:\", col)\n",
    "    kernel_name_list = []\n",
    "    if 'fm' in col:\n",
    "        values = values_for_kernel_fm\n",
    "    else:\n",
    "        values = values_for_kernel_cov\n",
    "    # Create a list of the new column names\n",
    "    for k in range(0, values.shape[0]):\n",
    "        kernel_name = col + \"_kernel_\" + str(k)\n",
    "        kernel_name_list = kernel_name_list + [kernel_name]\n",
    "    print(kernel_name_list)\n",
    "    \n",
    "    # Create the additional columns\n",
    "    kernel_estimates_column = \"kernel_\" + col\n",
    "    raw_df[kernel_name_list] = pd.DataFrame(raw_df[kernel_estimates_column].tolist(), index= raw_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052717c-c317-4b71-bce8-fe5729896820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "for col in ['read_fm', 'cpg_fm', 'cpg_dist']:\n",
    "    export_kernel_array(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889362e-ba69-4a65-8de1-92be70ff8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns that we no longer need\n",
    "for col in ['read_fm', 'cpg_dist',\n",
    "            'kernel_cpg_dist', 'kernel_cpg_fm', 'kernel_read_fm']:\n",
    "    raw_df.drop(col, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb4754-b931-4004-949b-80652daa233a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Convert epigenetic signals into dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988cff6a-e3f5-4320-8a95-a2f8607bc5ab",
   "metadata": {},
   "source": [
    "#### Plot histograms for the values of epigenetic signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f3c8c-f04a-4f7d-b0a0-c012a9ace7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_extract = raw_df[['dnase','encode_ChiP_V2', 'tf_motifs']]\n",
    "#df_extract\n",
    "#hist = raw_df[['dnase','encode_ChiP_V2', 'tf_motifs']].hist(density = True, bins = 3)\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(12,12)})\n",
    "#sns.pairplot(raw_df[['asm_snp','dnase','encode_ChiP_V2', 'tf_motifs']], hue = 'asm_snp', diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b08f5-8e0a-4cdd-9542-944b91096f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79f67df-b84f-4456-99a7-93e12d2916ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_epi_signal(epi_signal):\n",
    "  print(\"Processing signal\", epi_signal)\n",
    "  unique_values = raw_df[epi_signal].unique()\n",
    "  print(unique_values)\n",
    "  min_epi_value = 0 # It's always zero (no signal) for all signals\n",
    "  median_epi_value = np.median(unique_values)\n",
    "  print(\"Median epi value:\", median_epi_value)\n",
    "  epi_signal_null = epi_signal + \"_null\"\n",
    "  epi_signal_low = epi_signal + \"_low\"\n",
    "  epi_signal_high = epi_signal + \"_high\"\n",
    "  raw_df[epi_signal_null] = raw_df[epi_signal].apply(lambda x: 1 if x == min_epi_value else 0)\n",
    "  raw_df[epi_signal_low] = raw_df[epi_signal].apply(lambda x: 1 if (x > min_epi_value and x <= median_epi_value) else 0)\n",
    "  raw_df[epi_signal_high] = raw_df[epi_signal].apply(lambda x: 1 if x > median_epi_value else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43bdee0-cd0f-46fd-b54a-301b1b3975cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to all epigenetic signals\n",
    "for epi_signal in ['dnase', 'encode_ChiP_V2', 'tf_motifs']:\n",
    "  convert_epi_signal(epi_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fceac6-13e4-44f9-80e7-cad08a4f7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the raw epigenetic signals\n",
    "# for epi_signal in ['dnase', 'encode_ChiP_V2', 'tf_motifs']:\n",
    "#   raw_df.drop(epi_signal, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229e490-b543-4f31-b69f-70cc5b3e8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47106530-61f6-42eb-9cfa-f9b3700256c9",
   "metadata": {},
   "source": [
    "## Create a 2D image of the genomic region (20 reads x 3 consecutive CpGs)\n",
    "\n",
    "We select 3 CpGs whose FM is between 0.4 and 0.6 if possible. Then we get all 20 reads that cover them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f0815-a743-45b8-9816-b1c87ded3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cpgs_for_pic(df_row, nb_cpg, min_cov_cpg):\n",
    "    # df_row is a typical row of the raw_df dataset\n",
    "    # We find the nb_cpg CpGs whose FM is the closest to 0.5\n",
    "    \n",
    "    # We first sort the FM by its distance from 0.5\n",
    "    ranked_fm = [round(abs(x-0.5),3) for x in df_row['cpg_fm']]\n",
    "    new_pos_sorted = [i for _, i in sorted(zip(ranked_fm, df_row['cpg_pos']))]\n",
    "    new_cov_sorted = [i for _, i in sorted(zip(ranked_fm, df_row['cpg_cov']))]\n",
    "    #print(ranked_fm)\n",
    "    #print(new_pos_sorted)\n",
    "    \n",
    "    filtered_cpg = []\n",
    "    \n",
    "    for k in range(0, len(new_pos_sorted)):\n",
    "        #print(new_cov_sorted[k])\n",
    "        if new_cov_sorted[k] >= min_cov_cpg:\n",
    "            #print(\"covered enough\")\n",
    "            filtered_cpg = filtered_cpg + [new_pos_sorted[k]]\n",
    "    \n",
    "    # print(ranked_fm)\n",
    "    # print(new_pos_sorted)\n",
    "    # print(new_cov_sorted)\n",
    "    \n",
    "    # We return the first 3 elements of the list and we sort them\n",
    "    return sorted(filtered_cpg[:min(len(df_row['cpg_pos']),nb_cpg)])\n",
    "        \n",
    "# Quick example\n",
    "find_cpgs_for_pic(raw_df.iloc[28], 3, 15)\n",
    "\n",
    "#print(\"HI\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6ddab-48dc-4fec-84c8-bf48ab66a7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe\n",
    "# Find 3 CpGs at least 10x covered as close as possible to a FM of 0.5\n",
    "raw_df['cpgs_for_pic'] = raw_df.apply(lambda x: find_cpgs_for_pic(x, 3, 10), axis = 1)\n",
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b91cc9-4426-443e-ade5-4cb6343ba418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the FM and CpG pos columns\n",
    "raw_df.drop(['cpg_pos', 'cpg_fm', 'cpg_cov', 'nb_cpg_found'], inplace = True, axis = 1)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0386274-c213-4dc9-a40a-0991ee250bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_df['genomic_picture'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3abf2-e076-44f4-b19a-122188cc15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a read, create a dictionary of 1/ read FM and 2/ array of methylation status\n",
    "def find_methylation_array_from_one_read(genomic_dictionary, cpg_array):\n",
    "    # genomic dictionary contains read ID, its FM, an array of CpG pos and an array of CpG methylation status\n",
    "    # cpg_array is the list of 3 CpGs we need to find the methylation array for\n",
    "    \n",
    "    #print(genomic_dictionary)\n",
    "    #print(cpg_array)\n",
    "    \n",
    "    # Initialize the methylation array\n",
    "    meth_array = []\n",
    "    \n",
    "    # When importing the arrays of CpG pos and FM, there were converted into strings\n",
    "    genomic_dictionary['pos_array'] = list(map(int, genomic_dictionary['pos_array']))\n",
    "    genomic_dictionary['meth_array'] = list(map(int, genomic_dictionary['meth_array']))\n",
    "        \n",
    "    if int(genomic_dictionary['nb_cpg']) >= len(cpg_array): # no need to go through the dic if it does not cover at least 3 CpGs\n",
    "        for k in range(0,len(cpg_array)):\n",
    "            #print(cpg_array[k])\n",
    "            if cpg_array[k] in genomic_dictionary['pos_array']:\n",
    "                cpg_pos_in_array = genomic_dictionary['pos_array'].index(cpg_array[k])\n",
    "                meth_array = meth_array + [genomic_dictionary['meth_array'][cpg_pos_in_array]]\n",
    "        \n",
    "        # Create a new dictionary to return\n",
    "        new_dic = {'read_fm': round(genomic_dictionary['read_fm'],3), \n",
    "                   'meth_array': meth_array}\n",
    "    else:\n",
    "        new_dic = []\n",
    "    \n",
    "    # if did not find all CpGs\n",
    "    if meth_array != []:\n",
    "        if len(meth_array) < len(cpg_array):\n",
    "            new_dic = []\n",
    "    \n",
    "    # If we did not find ANY CpG\n",
    "    if meth_array == []:\n",
    "        new_dic = []\n",
    "    \n",
    "    return new_dic\n",
    "        \n",
    "# Quick test\n",
    "test = {'read_id': 'E00247:448:H3HNYCCXY:2:1219:8471:23970_1:N:0:TATAAT',\n",
    "        'read_fm': 0.667,\n",
    "        'nb_cpg': '4',\n",
    "        'pos_array': ['3', '1', '9'],\n",
    "        'meth_array': ['1', '0', '0']}\n",
    "find_methylation_array_from_one_read(test, [3,9])\n",
    "\n",
    "#### Quick test 2\n",
    "#test1 = {'read_id': 'E00247:448:H3HNYCCXY:2:1201:11698:14441_1:N:0:TATAAT', 'read_fm': 0.8180000000000001, 'nb_cpg': '11', 'pos_array': [47, 115, 150, 157, 101, 175, 105, 138, 167, 113, 185], 'meth_array': [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n",
    "#test2 = [16, 200, 229]\n",
    "#find_methylation_array_from_one_read(test1, test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdeb163-7ec4-461a-9fc9-622aff354016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the array (3, 20, 1)\n",
    "def create_2d_pic(row_df, min_cov):\n",
    "    \n",
    "    array_of_new_dics = []\n",
    "    \n",
    "    # row_df is a row in the raw_df dataframe\n",
    "    for k in range(0, len(row_df['genomic_picture'])):\n",
    "        #print(row_df['genomic_picture'][k])\n",
    "        #print(row_df['cpgs_for_pic'])\n",
    "        #print(find_methylation_array_from_one_read(row_df['genomic_picture'][k], row_df['cpgs_for_pic']))\n",
    "        #print(\"**************\")\n",
    "        array_of_new_dics = array_of_new_dics + [find_methylation_array_from_one_read(row_df['genomic_picture'][k], row_df['cpgs_for_pic'])]\n",
    "    \n",
    "    array_of_new_dics = [x for x in array_of_new_dics if x != []]\n",
    "    print(array_of_new_dics)\n",
    "    \n",
    "#     # Remove empty lists\n",
    "#     result = [x for x in result if x != []]\n",
    "    \n",
    "#     if np.any(result):\n",
    "#         #print(\"Coverage:\", len(result))\n",
    "#         if len(result) >= min_cov:\n",
    "            \n",
    "#             # We sample 20 reads at random\n",
    "#             result_min_cov = random.sample(result, min_cov)\n",
    "\n",
    "#             # Sort the array by the read FM\n",
    "#             sorted_result = sorted(result_min_cov, key = lambda d: d['read_fm'], reverse = True)\n",
    "#             #sorted_result = result\n",
    "\n",
    "#             # Create the array of arrays in the same order\n",
    "#             genomic_2d_picture = []\n",
    "#             for k in range(0, len(sorted_result)):\n",
    "#                 genomic_2d_picture = genomic_2d_picture + [sorted_result[k]['meth_array']]\n",
    "\n",
    "#             # Convert into array\n",
    "#             genomic_2d_picture = np.array(genomic_2d_picture)\n",
    "#         else:\n",
    "#             #print(\"Not enough coverage\")\n",
    "#             genomic_2d_picture = []\n",
    "        \n",
    "#     else:\n",
    "#         #print(\"EMPTY\")\n",
    "#         genomic_2d_picture = []\n",
    "    \n",
    "#     return genomic_2d_picture\n",
    "\n",
    "# Quick test\n",
    "result_test = create_2d_pic(raw_df.iloc[1], 3)\n",
    "print(result_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db111e71-0c7f-428b-b24c-24a686d3b03e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a 2D image of the genomic region (20 reads x 250 bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d67e3-ad27-4052-a22f-fe5659c89209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_1d_genomic_window(genomic_dictionary, genomic_interval, nb_cpg_in_region):\n",
    "    # genomic dictionary contains read ID, its FM, an array of CpG pos and an array of CpG methylation status\n",
    "    # This creates an array of array. Its length is the length of the genomic window (250bp) and each array\n",
    "    # contains [a,b] where a indicates if a CpG was found (0 or 1) and b is the methylation status of the CpG (0 or 1)\n",
    "    \n",
    "    # When importing the arrays of CpG pos and FM, there were converted into strings\n",
    "    genomic_dictionary['pos_array'] = list(map(int, genomic_dictionary['pos_array']))\n",
    "    genomic_dictionary['meth_array'] = list(map(int, genomic_dictionary['meth_array']))\n",
    "    \n",
    "    #print(genomic_dictionary)\n",
    "    \n",
    "    # We initialize the array\n",
    "    result = []\n",
    "    \n",
    "    if int(genomic_dictionary['nb_cpg']) >= nb_cpg_in_region:\n",
    "        for genomic_position in range(1,genomic_interval+1):\n",
    "            #print(genomic_position)\n",
    "            if genomic_position in genomic_dictionary['pos_array']:\n",
    "                #print(\"Found this CpG on the read\")\n",
    "                cpg_pos_in_array = genomic_dictionary['pos_array'].index(genomic_position)\n",
    "                result = result + [[1,genomic_dictionary['meth_array'][cpg_pos_in_array]]]\n",
    "            else:\n",
    "                result = result + [[0,0]]\n",
    "            #print(result)\n",
    "    \n",
    "        # Convert list into array\n",
    "        result = np.array(result)\n",
    "        new_dic = {'read_id': genomic_dictionary['read_id'], \n",
    "                   'read_fm': genomic_dictionary['read_fm'], \n",
    "                   'nb_cpg': genomic_dictionary['nb_cpg'],\n",
    "                    'genomic_1d_window': result}\n",
    "    else:\n",
    "        new_dic = []\n",
    "    \n",
    "    return new_dic\n",
    "\n",
    "\n",
    "# Quick test\n",
    "test = {'read_id': 'E00247:448:H3HNYCCXY:2:1219:8471:23970_1:N:0:TATAAT',\n",
    "        'read_fm': 0.667,\n",
    "        'nb_cpg': '3',\n",
    "        'pos_array': ['1', '3', '9'],\n",
    "        'meth_array': ['1', '1', '0']}\n",
    "\n",
    "result_test = create_1d_genomic_window(test, 10, 3)\n",
    "print(result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3571521-1a37-400e-93ae-3f73cf732e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_genomic_window(row_df, genomic_interval, min_cov):\n",
    "    # row_df is a row in the raw_df data.\n",
    "    # The genomic array is an array of genomic dictionaries (see function above).\n",
    "    # This will create a 2D image: an array of of nested arrays. nested arrays are arrays of arrays. \n",
    "    # They are 250 length and contain [a,b] where a indicates if a CpG was found (0 or 1) and b is the methylation status of the CpG (0 or 1)\n",
    "    # The number of 250-arrays is equal to the number of reads for that genomic window.\n",
    "    \n",
    "    genomic_array = row_df['genomic_picture']\n",
    "    nb_cpg_in_region = row_df['nb_cpg_found']\n",
    "    \n",
    "    required_nb_cpg = round(0.3*nb_cpg_in_region)\n",
    "    \n",
    "    #print(nb_cpg_in_region)\n",
    "    #print(\"Requiring\", required_nb_cpg, \" cpgs\")\n",
    "    \n",
    "    # Create the genomic 1D window for each read in the array\n",
    "    result = np.array([create_1d_genomic_window(x, genomic_interval, required_nb_cpg) for x in genomic_array])\n",
    "    \n",
    "    # Remove empty lists\n",
    "    result = [x for x in result if x != []]\n",
    "    #print(result.shape)\n",
    "    #print(\"result\", result)\n",
    "    # If the result is not empty:\n",
    "    if np.any(result):\n",
    "        #print(\"Coverage:\", len(result))\n",
    "        if len(result) >= min_cov:\n",
    "            result_min_cov = random.sample(result, min_cov)\n",
    "\n",
    "            # Sort the array by the read FM\n",
    "            sorted_result = sorted(result_min_cov, key = lambda d: d['read_fm'], reverse = True)\n",
    "            #sorted_result = result\n",
    "\n",
    "            # Create the array of arrays in the same order\n",
    "            genomic_2d_picture = []\n",
    "            for k in range(0, len(sorted_result)):\n",
    "                genomic_2d_picture = genomic_2d_picture + [sorted_result[k]['genomic_1d_window']]\n",
    "\n",
    "            # Convert into array\n",
    "            genomic_2d_picture = np.array(genomic_2d_picture)\n",
    "        else:\n",
    "            #print(\"Not enough coverage\")\n",
    "            genomic_2d_picture = []\n",
    "        \n",
    "    else:\n",
    "        #print(\"EMPTY\")\n",
    "        genomic_2d_picture = []\n",
    "    \n",
    "    return genomic_2d_picture\n",
    "\n",
    "#tmp1 = {'read_id': 'read_id_121391', 'nb_cpg': '3', 'read_fm': 0.33, 'pos_array':[2,5,7], 'meth_array':[0,1,0]}\n",
    "#tmp2 = {'read_id': 'read_id_12', 'nb_cpg': '3', 'read_fm': 0.66, 'pos_array':[1,2,3], 'meth_array':[1,1,0]}\n",
    "#test = [tmp1, tmp2]\n",
    "#print(type(result_test))\n",
    "\n",
    "result_test = create_2d_genomic_window(raw_df.iloc[0], 10, 10)\n",
    "#print(\"Final result\", result_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851518fa-d233-463a-8a0d-7c4ab6ea9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into a list of dataframes of XX rows. Otherwise it stalls\n",
    "nb_rows_in_splits = 100\n",
    "nb_dataframe_pieces = max(1, round(raw_df.shape[0]/nb_rows_in_splits))\n",
    "raw_df_pieces = np.array_split(raw_df, nb_dataframe_pieces)\n",
    "print(\"The dataframe has been split into\", nb_dataframe_pieces, \"pieces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6a5f5-7c63-4d54-a46b-77f9b9c64560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe which we split before because it takes a lot of memory.\n",
    "for df_piece in range(nb_dataframe_pieces): # nb_dataframe_pieces\n",
    "        clear_output(wait=True)\n",
    "        print(\"processing the piece at position:\", df_piece)\n",
    "        raw_df_pieces[df_piece]['genomic_2d_picture'] = raw_df_pieces[df_piece].apply(lambda x: create_2d_genomic_window(x, GENOMIC_INTERVAL, 20), axis = 1)\n",
    "        raw_df_pieces[df_piece] = raw_df_pieces[df_piece][raw_df_pieces[df_piece]['genomic_2d_picture'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b98ee6-5c87-4c6d-98c9-711acbc47d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "prepared_df = pd.DataFrame()\n",
    "\n",
    "for df_piece in range(nb_dataframe_pieces):\n",
    "    clear_output(wait=True)\n",
    "    print(\"processing the piece at position:\", df_piece)\n",
    "    tmp_df = raw_df_pieces[df_piece]\n",
    "    tmp_df.drop('genomic_picture', axis = 1, inplace = True)\n",
    "    prepared_df = prepared_df.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25275027-6e9a-4f4c-bc4d-f1a8bc1e2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.shape\n",
    "#prepared_df[prepared_df['genomic_2d_picture'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae6224-d8e0-4444-8d19-6ef29d7742e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a 1D \"image\" of the genomic region\n",
    "\n",
    "We create an 1D-image (length: genomic region interval) with 3 information per \"pixel\": CpG presence (0 or 1), CpG coverage, CpG fractional methylation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f9a36-983c-47e6-831d-5b169d1108ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30e5e77-f178-4d1b-b8b5-0d9754bd0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of positions, fractional methylation, and coverage for CpGs (~2h30)\n",
    "def create_genomic_array(df):\n",
    "    genomic_positions = []\n",
    "    genomic_fm = []\n",
    "    for position in range(df['region_inf'], df['region_sup'] + 1):\n",
    "        if position in df['cpg_pos']:\n",
    "            new_pos = 1\n",
    "            pos_index = df['cpg_pos'].index(position)\n",
    "            new_fm = df['cpg_fm'][pos_index]\n",
    "        else:\n",
    "            new_pos = 0\n",
    "            new_fm = 0\n",
    "        genomic_positions = genomic_positions + [new_pos]\n",
    "        genomic_fm = genomic_fm + [new_fm]\n",
    "    return np.transpose([genomic_positions, genomic_fm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706ea3d-e4a0-4398-bc26-ee64f817daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "d = {'cpg_pos': [[2],[21,22]], \n",
    "              'region_inf':[1,20], \n",
    "              'region_sup':[4,23],\n",
    "              'cpg_fm': [[0.0],[1.0,0.0]]\n",
    "             }\n",
    "example_df = pd.DataFrame(data = d)\n",
    "example_df['genomic_pic'] = example_df.apply(lambda x: create_genomic_array(x), axis = 1)\n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6018cf7e-bdf4-4be4-af9f-42d14e7723c0",
   "metadata": {},
   "source": [
    "### Split the dataset for calculating the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06096995-690f-4aab-8bc5-259e884992ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into a list of dataframes of 10 rows. Otherwise it stalls\n",
    "nb_dataframe_pieces = max(1, round(raw_df.shape[0]/10))\n",
    "raw_df_pieces = np.array_split(raw_df, nb_dataframe_pieces)\n",
    "print(\"The dataframe has been split into\", nb_dataframe_pieces, \"pieces\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4e0f2-9355-4174-8d14-cf1333af9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataframe which we split before because it takes a lot of memory.\n",
    "for df_piece in range(nb_dataframe_pieces): # nb_dataframe_pieces\n",
    "        clear_output(wait=True)\n",
    "        print(\"processing the piece at position:\", df_piece)\n",
    "        raw_df_pieces[df_piece]['genomic_matrix'] = raw_df_pieces[df_piece].apply(lambda x: create_genomic_array(x), \n",
    "                                                                                      axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e3c7b-0731-4766-95db-0713df166b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes\n",
    "prepared_df = pd.DataFrame()\n",
    "\n",
    "for df_piece in range(nb_dataframe_pieces):\n",
    "    clear_output(wait=True)\n",
    "    print(\"processing the piece at position:\", df_piece)\n",
    "    tmp_df = raw_df_pieces[df_piece]\n",
    "    for var in ['cpg_pos', 'cpg_fm', 'region_inf', 'region_sup']:\n",
    "        tmp_df.drop(var, axis = 1, inplace = True)\n",
    "    prepared_df = prepared_df.append(tmp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23fd340-7827-4182-b371-554436af51f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save dataframe with features on Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf90a2f-dc29-4a84-8c3d-500e3bf49c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns\n",
    "prepared_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdade84-79fc-452e-9ab7-11317804c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the date/time\n",
    "now = datetime.today()\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "print(dt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abf4ba-fd30-4102-b419-2713aeaa79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the variable names to the bucket\n",
    "sys.stdout = open(\"variables.txt\", \"w\")\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d939f-8b48-4670-bba5-9877e03b3f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export variable names to to Cloud Storage\n",
    "dt_string = str(GENOMIC_INTERVAL) + \"bp_\" + str(NB_ROWS_RAW_DATASET) + \"rows_\" + dt_string\n",
    "!gsutil cp variables.txt gs://$DEEPASM_BUCKET/notebook/$dt_string/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67397ca8-78ff-44ac-8f7f-9114182fc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_PROCESSED_DATA == True:\n",
    "    \n",
    "    nb_pieces_export = max(1, round(raw_df.shape[0]/2000))\n",
    "    prepared_df_pieces = np.array_split(prepared_df, nb_pieces_export)\n",
    "    print(\"The dataframe has been split into\", nb_pieces_export, \"pieces\")\n",
    "    \n",
    "    for df_piece in range(nb_pieces_export):\n",
    "        print(\"processing the piece at position:\", df_piece)\n",
    "        df_to_export = prepared_df_pieces[df_piece]\n",
    "        print(\"Size of dataframe:\", df_to_export.shape)\n",
    "\n",
    "        print(\"Saving the file as HDF5...\")\n",
    "        file_name = \"prepared_df_\" + str(df_piece) + \".h5\"\n",
    "        print(\"File name:\", file_name)\n",
    "        df_to_export.to_hdf(file_name, key = 'df', mode = 'w')\n",
    "\n",
    "        print(\"Exporting file to bucket...\")\n",
    "        !gsutil cp $file_name gs://$DEEPASM_BUCKET/notebook/$dt_string/\n",
    "else:\n",
    "        print(\"Not exporting the scaled DF per variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4449d8-ef60-4d41-b099-19781a2817f5",
   "metadata": {},
   "source": [
    "## Importing prepared features from bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31230ac-36bd-47da-9055-b840367e4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPORT_PROCESSED_DATA == True:\n",
    "    display(\"Downloading from the bucket...\")\n",
    "    # Obtain path from GCP Cloud Storage\n",
    "    bucket_path = \"gs://\" + PROCESSED_DATA_PATH\n",
    "    file_path = bucket_path + \"/*.h5\"\n",
    "\n",
    "    # Find all the H5 files with the same normalization method.\n",
    "    !gsutil ls $file_path > list_to_download.txt\n",
    "    files_to_download_df = pd.read_csv('list_to_download.txt', header=None)\n",
    "\n",
    "    prepared_df = pd.DataFrame()\n",
    "\n",
    "    for index_file in range(files_to_download_df.shape[0]):\n",
    "        clear_output(wait=True)\n",
    "        display(\"Processing file:\", index_file)\n",
    "        file_name_bucket = bucket_path + \"/prepared_df_\" + str(index_file) + \".h5\"\n",
    "        display(file_name_bucket)\n",
    "        file_name_local = \"prepared_df_\" +  str(index_file) + \".h5\"\n",
    "        !gsutil cp $file_name_bucket $file_name_local\n",
    "        tmp = pd.read_hdf(file_name_local)\n",
    "        prepared_df = prepared_df.append(tmp)\n",
    "else:\n",
    "    display(\"Not downloading from the bucket\")\n",
    "    prepared_df = pd.DataFrame()\n",
    "    !ls *.h5 > list_files.txt\n",
    "    files_to_append = pd.read_csv('list_files.txt', header=None)\n",
    "    for file_number in range(files_to_append.shape[0]):\n",
    "        clear_output(wait=True)\n",
    "        file_name_local = \"prepared_df_\" +  str(file_number) + \".h5\"\n",
    "        print(\"Processing:\", file_name_local)\n",
    "        tmp = pd.read_hdf(file_name_local)\n",
    "        prepared_df = prepared_df.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa078a19-6777-47f9-a3f5-02f4bdcc9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# TEMPORARY########################\n",
    "##################################\n",
    "files_to_append = pd.read_csv('list_files.txt', header=None)\n",
    "prepared_df = pd.DataFrame()\n",
    "for file_number in range(files_to_append.shape[0]):\n",
    "        clear_output(wait=True)\n",
    "        file_name_local = \"prepared_df_\" +  str(file_number) + \".h5\"\n",
    "        print(\"Processing:\", file_name_local)\n",
    "        tmp = pd.read_hdf(file_name_local)\n",
    "        prepared_df = prepared_df.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653db181-b4d3-42f9-87d3-af0df13d3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size:\", prepared_df.shape)\n",
    "print(\"Columns of dataset:\", prepared_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae104cf-3df7-419e-bb0b-f553cfdcc4fa",
   "metadata": {},
   "source": [
    "## Plot a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ffcb64-5d28-4c37-8084-4fdb38a63b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_primary_color(read_info):\n",
    "    # read_info is a list of 250 arrays. Each subarray is [a,b] where a = 0 or 1 (CpG) and b = 0 or 1 (methylation)\n",
    "    # We return a list of array with 3 elements. The third one is always zero\n",
    "    augmented_read = []\n",
    "    for element in range(0, len(read_info)):\n",
    "        #print(element)\n",
    "        # RGB codes: (0,0,0) = black (no CpG), (0,0,1) = dark blue (CpG not methylated) ; \n",
    "        # (0,1,1) = light blue (CpG methylated)\n",
    "        #augmented_read = augmented_read + [[0.0, float(read_info[element][1]), float(read_info[element][0])]]\n",
    "        \n",
    "        # RGB codes: (1,1,1) = white (no CpG), (1,0,0) = red (CpG not methylated) [1,0] ; \n",
    "        # (0,0,1) = blue (CpG methylated) [1,1]\n",
    "        if read_info[element][0] == 0: # No CpG\n",
    "            pixel = [1.0, 1.0, 1.0]\n",
    "        else:\n",
    "            if read_info[element][1] == 0: # CpG not methylated\n",
    "                pixel = [1.0, 0.0, 0.0]\n",
    "            else:\n",
    "                pixel = [0.0, 0.0, 1.0]\n",
    "        augmented_read = augmented_read + [pixel]\n",
    "    return augmented_read\n",
    "\n",
    "# Quick test\n",
    "result = add_primary_color([[0,0],[1,0],[1,1]])\n",
    "result\n",
    "#add_primary_color(prepared_df['genomic_2d_picture'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5821205-ef77-4b51-bd8c-dbe3eed4bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Black: no CpG\n",
    "# Dark blue: CpG NOT methylated\n",
    "# Light blue: CpG METHYLATED\n",
    "\n",
    "def convert_to_picture(gen2d):\n",
    "    # gen2d is a list of lists. Each list is structured as above\n",
    "    \n",
    "    # Convert into array\n",
    "    #tmp = np.array(gen2d)\n",
    "    gen_pic =  [add_primary_color(x) for x in gen2d]\n",
    "    #gen_pic = map(add_primary_color, gen2d)\n",
    "    return gen_pic\n",
    "\n",
    "# Quick test\n",
    "read1 = [[1,0],[0,0],[1,1], [0,0]]\n",
    "read2 = [[0,0],[1,0],[1,1], [1,0]]\n",
    "test = [read1, read2]\n",
    "result = convert_to_picture(test)\n",
    "print(result)\n",
    "imgplot = plt.imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375d320-6a13-4105-a05a-a3504c54d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8a8f6-26b3-479c-aa53-327b6e06719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepared_df = prepared_df[len(prepared_df['genomic_2d_picture']) > 0 ]\n",
    "df_with_asm = prepared_df[prepared_df['asm_snp'] == 1].sample(4).reset_index(drop=True)\n",
    "df_without_asm = prepared_df[prepared_df['asm_snp'] == 0].sample(4).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17edff-3087-4e3a-bd91-97838cca766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT REGIONS WITHOUT ASM\n",
    "mpl.rcParams['figure.figsize'] = (20, 3)\n",
    "fig, axs = plt.subplots(2, 2, sharey=False, sharex=False, tight_layout=True)\n",
    "\n",
    "for k in range(0,2):\n",
    "    for m in range(0,2):\n",
    "        sequence = df_without_asm['genomic_2d_picture'][k+m]\n",
    "        axs[k,m].imshow(convert_to_picture(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e07c18-bec3-4649-8276-42d424028517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT REGIONS WITH ASM\n",
    "mpl.rcParams['figure.figsize'] = (20, 3)\n",
    "fig, axs = plt.subplots(2, 2, sharey=False, sharex=False, tight_layout=True)\n",
    "\n",
    "for k in range(0,2):\n",
    "    for m in range(0,2):\n",
    "        sequence = df_with_asm['genomic_2d_picture'][k+m]\n",
    "        axs[k,m].imshow(convert_to_picture(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f11de7-362c-499d-8cc0-03b7f6bd8748",
   "metadata": {},
   "source": [
    "## Class weights\n",
    "\n",
    "There are approximately 100x more regions without ASM than with ASM. We'll have to use weights in our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b93e1b-580a-42df-88cd-2273b892649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(prepared_df['asm_snp'])\n",
    "total = neg + pos\n",
    "print('Number of regions assessed for ASM: {}\\nRegions with ASM found: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb591cd-0295-4ee8-8334-07199035fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight_asm = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f31db6-6e8b-4050-b1ca-10876e376d8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c0908-86bd-40d6-ab7d-66e4f519b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcd565-8237-48da-8926-79d00290eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_continous_columns = ['asm_snp', 'sample_category', 'dnase', \n",
    "                                    'dnase_high', 'dnase_null', 'dnase_low', \n",
    "                                    'encode_ChiP_V2_low', 'encode_ChiP_V2_null', 'encode_ChiP_V2_high',\n",
    "                                    'tf_motifs_null', 'tf_motifs_low', 'tf_motifs_null',\n",
    "                                    'genomic_matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be36d41-5ae6-4a59-974e-56b5621582b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe and remove non-continous variables\n",
    "normalized_df = prepared_df.copy()\n",
    "normalized_df = normalized_df.drop(columns = non_continous_columns)\n",
    "\n",
    "# Apply StandardScaler to continous variables\n",
    "columns = normalized_df.columns\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(normalized_df)\n",
    "normalized_df = pd.DataFrame(data = scaler.transform(normalized_df), index = normalized_df.index)\n",
    "normalized_df.columns = columns\n",
    "\n",
    "# Concatenate with other columns\n",
    "normalized_df = pd.concat([normalized_df, \n",
    "                           prepared_df[non_continous_columns]], axis = 1)\n",
    "\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1626493-61fd-4c9f-8f2b-df05048c1d96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perform a PCA to reduce the number of the features to be used in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938e1dd-8e0b-48db-8db2-1cb7b0803706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do a test on normalized features\n",
    "# features_df = normalized_df.copy()\n",
    "\n",
    "# # Not normalized features\n",
    "# #features_df = prepared_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b9ca48-1ce7-4cff-99c5-20cee3f0be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = pd.DataFrame(abs(features_df.corr()))\n",
    "# sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f3dcc3-da1c-496d-89c9-d012558b6cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = pd.DataFrame(abs(features_df.corr()['asm_snp'])).sort_values(by = 'asm_snp')\n",
    "# corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7144bbf-b15f-41cd-97ed-4453a42d71eb",
   "metadata": {},
   "source": [
    "### Remove variables with no correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a3440-74f2-4467-871b-1864750f9494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars_no_corr = list(corr_matrix[pd.isna(corr_matrix['asm_snp']) == True].index)\n",
    "# print(vars_no_corr)\n",
    "# features_df = features_df.drop(columns = vars_no_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea96ea-7ca8-40f0-8e38-c252a8863fb4",
   "metadata": {},
   "source": [
    "### Remove variables that are poorly correlated with ASM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b029f39-714d-4d67-984b-43627207f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars_low_corr = list(corr_matrix[corr_matrix['asm_snp'] < MIN_CORR].index)\n",
    "# print(vars_low_corr)\n",
    "# features_df = features_df.drop(columns = vars_low_corr, axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eee5ed-4858-4277-987a-153e3e8672ea",
   "metadata": {},
   "source": [
    "### Run a PCA for the remaining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa7b62-051b-4cf9-ba28-64fdecd8a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove the column of genomic matrix and labels\n",
    "# features_df_pca = features_df.copy()\n",
    "# features_df_pca = features_df_pca.drop(columns = ['asm_snp', 'genomic_matrix'])\n",
    "# print(\"Number of features:\", len(features_df_pca.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff97713-e8bc-4ae6-a489-ce44c5a84508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=10)\n",
    "# principalComponents = pca.fit_transform(features_df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9ff45-f710-41d7-b5ad-2702828a8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_pca = []\n",
    "# for k in range(1,11):\n",
    "#     name = 'pca_' + str(k)\n",
    "#     columns_pca = columns_pca + [name]\n",
    "# print(columns_pca)\n",
    "# features_df_pca = pd.DataFrame(data = principalComponents, columns = columns_pca, index = features_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd6768-7341-4029-9ca8-749bf206d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_df_pca = pd.concat([features_df_pca, features_df[['asm_snp', 'genomic_matrix']]], axis = 1)\n",
    "# features_df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd508432-1839-437a-a88a-aaa42b129199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = pd.DataFrame(abs(features_df_pca.corr()))\n",
    "# sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38b188-003e-424b-9579-eda7edd3e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = pd.DataFrame(abs(features_df_pca.corr()['asm_snp'])).sort_values(by = 'asm_snp')\n",
    "# corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d39f59-bad8-4ed9-90fd-27d746d1e0b8",
   "metadata": {},
   "source": [
    "## Split the dataset for training and testing\n",
    "\n",
    "We use the sklearn `train_test_split` function. The validation set will be carved out from the training set when training the model. The validation set will be  used during the model fitting to evaluate the loss and any metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9ea43-d317-4aa5-b269-55195151d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3c4b5-c0b7-4414-8348-c0605eea67b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_split = prepared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889b72d-d572-4689-9275-c5f1b56fd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df_for_split['genomic_2d_picture'][0])\n",
    "#df_for_split['genomic_2d_picture'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eccb60e-3f5a-43bc-89a4-a822c8c39d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_feature, test_image_feature = train_test_split(df_for_split, test_size=TEST_SPLIT)\n",
    "train_labels = np.array(train_image_feature.pop('asm_snp'))\n",
    "test_labels = np.array(test_image_feature.pop('asm_snp'))\n",
    "train_image_feature = np.array(train_image_feature['genomic_2d_picture'].tolist())\n",
    "test_image_feature = np.array(test_image_feature['genomic_2d_picture'].tolist())\n",
    "display(\"Size of the TRAIN dataset for images:\", train_image_feature.shape)\n",
    "display(\"Size of the TEST dataset for images:\", test_image_feature.shape)\n",
    "display(\"Size of the TRAIN LABELS dataset:\", train_labels.shape)\n",
    "display(\"Size of the TEST LABELS dataset:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde7afa-f9f7-4f27-a627-91be3074dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(df_for_split, test_size=TEST_SPLIT)\n",
    "\n",
    "# Form np arrays of labels\n",
    "train_labels = np.array(train_df.pop('asm_snp'))\n",
    "test_labels = np.array(test_df.pop('asm_snp'))\n",
    "\n",
    "# Np arrays of features for CNN/RNN\n",
    "train_image_feature = np.array(train_df['genomic_matrix'].tolist())\n",
    "test_image_feature = np.array(test_df['genomic_matrix'].tolist())\n",
    "\n",
    "# Remove the matrix for the datasets\n",
    "train_df.drop('genomic_matrix', axis = 1, inplace = True)\n",
    "test_df.drop('genomic_matrix', axis = 1, inplace = True)\n",
    "\n",
    "# np arrays for  linear/perceptron\n",
    "train_scalar_features = np.array(train_df)\n",
    "test_scalar_features = np.array(test_df)\n",
    "\n",
    "# Check size of arrays for CNN (X,250,3)\n",
    "display(\"Image features\")\n",
    "display(\"Size of the TRAIN dataset for images:\", train_image_feature.shape)\n",
    "display(\"Size of the TEST dataset for images:\", test_image_feature.shape)\n",
    "\n",
    "# # Check size of arrays for scalar features (X, 39)\n",
    "display(\"SCALAR FEATURES:\")\n",
    "display(\"Size of the TRAIN datase:\", train_scalar_features.shape)\n",
    "display(\"Size of the TEST dataset:\", test_scalar_features.shape)\n",
    "\n",
    "# # Check size of arrays for the labels\n",
    "display(\"LABELS:\")\n",
    "display(\"Size of the TRAIN LABELS dataset:\", train_labels.shape)\n",
    "display(\"Size of the TEST LABELS dataset:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb63bc-781b-4ede-9cdb-7fe3e0860e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615cef5c-2b03-4553-b66b-2d36a60bfd91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16a918-449b-445f-b312-c81ead250b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_logistic_regression_model(output_bias = None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Normalize the features\n",
    "    # model.add(\n",
    "    #     layers.BatchNormalization(\n",
    "    #         axis=-1,\n",
    "    #         momentum = 0.99,\n",
    "    #         epsilon = 0.001,\n",
    "    #         input_dim = train_scalar_features.shape[1])\n",
    "    # )\n",
    "    model.add(\n",
    "        layers.Normalization(\n",
    "            axis = 1,\n",
    "            input_dim = train_scalar_features.shape[1])\n",
    "    )\n",
    "    \n",
    "    # Linear model\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "                1,  # number of classes\n",
    "                activation='sigmoid', #'sigmoid' 'softmax'\n",
    "                kernel_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = L1_R, \n",
    "                    l2 = L2_R),\n",
    "                bias_initializer=output_bias\n",
    "                )\n",
    "    )\n",
    " \n",
    "  \n",
    "    model.compile(\n",
    "        optimizer = 'sgd' , # sgd = stochastic gradient descent, rmsprop\n",
    "        loss= 'binary_crossentropy', # 'mse' 'categorical_crossentropy', 'binary_crossentropy'\n",
    "        metrics = METRICS)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1b309-16d9-4b5e-85ab-bde22b42dba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer = layers.Normalization()\n",
    "# normalizer.adapt(train_scalar_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee45aaf-045a-46d5-8f7c-1202f013c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Normalization layer and set its internal state using the training data\n",
    "# #normalizer = layers.Normalization()\n",
    "# #normalizer.adapt(train_scalar_features)\n",
    "\n",
    "# input_shape = train_scalar_features.shape[1:]\n",
    "# print(input_shape)\n",
    "# inputs = keras.Input(shape=input_shape)\n",
    "# x = normalizer(inputs)\n",
    "# outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "# linear_model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765c748-85de-468a-ae52-1ba6590a26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_model.compile(\n",
    "#         optimizer = 'sgd' , # sgd = stochastic gradient descent, rmsprop\n",
    "#         loss= 'binary_crossentropy', # 'mse' 'categorical_crossentropy', 'binary_crossentropy'\n",
    "#         metrics = METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec731b-3677-4cc9-bc5f-28aa4079e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = make_logistic_regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243dcae0-a04f-4fbe-b46b-2f06f39bab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.summary()\n",
    "keras.utils.plot_model(linear_model, \"linear_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089d144-77ad-4cf6-92bd-01278e2d7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_training = linear_model.fit(\n",
    "    train_scalar_features,\n",
    "    train_labels,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [EARLY_STOPPING],\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    class_weight = class_weight_asm,\n",
    "    verbose = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143fd5e-3a70-4c8a-ab63-f3e4fdd8c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(linear_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c44784-07aa-4f88-91d5-aade135911f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on the test dataset\n",
    "linear_results = linear_model.evaluate(test_scalar_features, test_labels, batch_size= BATCH_SIZE,\n",
    "                                           verbose=1)\n",
    "\n",
    "display_results(linear_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94734896-b3e3-4f7b-9a29-b932984703e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Forest models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ba5104-d4d9-4aaf-bb4b-9a9c6cec04b0",
   "metadata": {},
   "source": [
    "### Simple Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d4daa-2218-4f95-bb0f-ee33b0424c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree_model = tfdf.keras.RandomForestModel(\n",
    "    task=tfdf.keras.Task.CLASSIFICATION)\n",
    "\n",
    "simple_tree_model.compile(metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0b62f-aa87-4cf6-bb3d-d9b1f059f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree_model.fit(x=train_scalar_features, \n",
    "             y = train_labels, \n",
    "             batch_size=BATCH_SIZE,\n",
    "             callbacks = [EARLY_STOPPING],\n",
    "             class_weight=class_weight_asm, \n",
    "             validation_split = VALIDATION_SPLIT,\n",
    "             verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf42da2-3031-4706-af79-1cfaea164975",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree_results = simple_tree_model.evaluate(\n",
    "    test_scalar_features, \n",
    "    test_labels, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    return_dict=True)\n",
    "\n",
    "simple_tree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4083ba-e019-4380-b8cf-02241ab5df25",
   "metadata": {},
   "source": [
    "### Gradient Boosted Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7dad3-27a3-4511-bafa-8bd4a80d7f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gbt_model():\n",
    "    boosted_tree_model = tfdf.keras.GradientBoostedTreesModel(\n",
    "        #features = specify_feature_usages(df_for_split),\n",
    "        growing_strategy=GROWING_STRATEGY,\n",
    "        num_trees=NUM_TREES,\n",
    "        max_depth=MAX_DEPTH,\n",
    "        min_examples=MIN_EXAMPLES,\n",
    "        subsample=SUBSAMPLE,\n",
    "        task=tfdf.keras.Task.CLASSIFICATION,\n",
    "        loss=\"DEFAULT\",\n",
    "    )\n",
    "\n",
    "    boosted_tree_model.compile(metrics=METRICS)\n",
    "    return boosted_tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bf7ee-5a31-4c62-8ce0-96e585e89ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_tree_model = create_gbt_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3318ab-c493-46cf-a3c3-30a380c588f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_tree_model.fit(x = train_scalar_features, \n",
    "              y = train_labels, \n",
    "              batch_size=BATCH_SIZE, \n",
    "              validation_split = VALIDATION_SPLIT,\n",
    "              callbacks = [EARLY_STOPPING],\n",
    "              class_weight=class_weight_asm, \n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37be835-9571-4671-82e2-44699b6f5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_tree_results = boosted_tree_model.evaluate(\n",
    "    test_scalar_features, \n",
    "    test_labels, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    return_dict=True)\n",
    "boosted_tree_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b066fd87-a5f8-4225-9385-f1b8c21d1b01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e55a34e-c704-4f19-8ebc-2eb81070e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_perceptron_model(output_bias = None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "  \n",
    "    model = keras.Sequential()\n",
    "  \n",
    "#     # Normalize the features\n",
    "#     model.add(\n",
    "#         layers.Normalization(\n",
    "#             axis = 1,\n",
    "#             input_dim = train_scalar_features.shape[-1])\n",
    "#     )\n",
    "        \n",
    "  # Initial perceptron layer\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            NB_NODES_PERCEPTRON, \n",
    "            activation=ACTIVATION_FUNCTION,\n",
    "            input_dim = train_scalar_features.shape[-1],\n",
    "            kernel_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = L1_R, \n",
    "                    l2 = L2_R)\n",
    "        )\n",
    "    )\n",
    "  \n",
    "    # Range of neuron layers\n",
    "    for layer_number in range(0, NB_LAYERS_PERCEPTRON-1): \n",
    "        model.add(layers.Dense(\n",
    "            NB_NODES_PERCEPTRON, \n",
    "            activation = ACTIVATION_FUNCTION,\n",
    "            kernel_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = L1_R, \n",
    "                    l2 = L2_R)))\n",
    "  \n",
    "        # Dropout layer in between layers\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # We add a sigmoid to create the probability function of the ASM event.\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            1, \n",
    "            activation='sigmoid',\n",
    "            bias_initializer=output_bias,\n",
    "            kernel_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = L1_R, \n",
    "                    l2 = L2_R)))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = LEARNING_RATE),\n",
    "        loss = keras.losses.BinaryCrossentropy(),\n",
    "        metrics = METRICS)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d240b81b-547d-4b53-bd34-7caff787d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_model = make_perceptron_model()\n",
    "perceptron_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a8496-1b8c-49bf-af90-ab57cf6651da",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_training = perceptron_model.fit(\n",
    "    train_scalar_features,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [EARLY_STOPPING],\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    class_weight=class_weight_asm,\n",
    "    verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bda20-b408-47a8-b3e2-d47bd4dd1c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(perceptron_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cfbe0-f510-4126-8765-92540accf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_results = perceptron_model.evaluate(test_scalar_features, \n",
    "                                               test_labels, \n",
    "                                               batch_size= BATCH_SIZE,\n",
    "                                               verbose=1)\n",
    "display_results(perceptron_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34cef7-a896-444a-8472-fd4988a704f8",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93948497-6b99-4032-abf4-3e8c34904cb4",
   "metadata": {},
   "source": [
    "train_image_feature.shape[1], train_image_feature.shape[2]## CNN model with the genomic picture as sole input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3feb5-6e93-47dd-b699-0762c0f3111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_feature.shape[3]#, train_image_feature.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a3648-bb78-4ca9-a3a4-39cc9ef83e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_simple_cnn_model(output_bias = None):\n",
    "\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    # Start the model\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Normalize the features\n",
    "    # model.add(\n",
    "    #     layers.BatchNormalization(\n",
    "    #         axis=-1,\n",
    "    #         momentum = 0.99,\n",
    "    #         epsilon = 0.001,\n",
    "    #         input_shape=(train_image_feature.shape[1], train_image_feature.shape[2])\n",
    "    #     )\n",
    "    # )  \n",
    "    \n",
    "    # Add a convolutional layer\n",
    "    model.add(layers.Conv2D(\n",
    "        filters = 32, \n",
    "        kernel_size = (5,5), # (3,3) or (5,5)\n",
    "        activation = ACTIVATION_FUNCTION,\n",
    "        input_shape=(train_image_feature.shape[1], train_image_feature.shape[2], train_image_feature.shape[3]),\n",
    "        kernel_regularizer = keras.regularizers.L1L2(\n",
    "                            l1 = L1_R, \n",
    "                            l2 = L2_R)))\n",
    "\n",
    "    # Pooling\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  \n",
    "    # Flattening\n",
    "    model.add(layers.Flatten())\n",
    "  \n",
    "    # Output layer (Sigmoid)\n",
    "    model.add(layers.Dense(1, activation='sigmoid', \n",
    "                         kernel_regularizer = keras.regularizers.L1L2(\n",
    "                            l1 = L1_R, \n",
    "                            l2 = L2_R),\n",
    "                         bias_initializer=output_bias))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = LEARNING_RATE),\n",
    "        loss = 'binary_crossentropy',\n",
    "        metrics = METRICS)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d6a6e-9ff5-41eb-8803-44b42db2d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_model = make_simple_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcb8fe-de9c-4e31-92a7-6c74f50e08f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_model = keras.Sequential()\n",
    "simple_cnn_model.add(layers.Conv2D(32, (3, 3), input_shape = (20, 250, 2)))\n",
    "simple_cnn_model.add(layers.Activation('relu'))\n",
    "simple_cnn_model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "simple_cnn_model.add(layers.Conv2D(32, (3, 3)))\n",
    "simple_cnn_model.add(layers.Activation('relu'))\n",
    "simple_cnn_model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "simple_cnn_model.add(layers.Conv2D(64, (2, 2)))\n",
    "simple_cnn_model.add(layers.Activation('relu'))\n",
    "simple_cnn_model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "simple_cnn_model.add(layers.Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "simple_cnn_model.add(layers.Dense(64))\n",
    "simple_cnn_model.add(layers.Activation('relu'))\n",
    "simple_cnn_model.add(layers.Dropout(0.5))\n",
    "simple_cnn_model.add(layers.Dense(1))\n",
    "simple_cnn_model.add(layers.Activation('sigmoid'))\n",
    "\n",
    "simple_cnn_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b53c2f-c161-4e7f-bd50-0209960cc713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "# keras.utils.plot_model(model, \"cnn_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a41d72-5ca6-42ca-87c5-c953ddde7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_model.summary()\n",
    "keras.utils.plot_model(simple_cnn_model, \"cnn_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b71e7-d194-44d8-96f9-29a4b6be7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_training = simple_cnn_model.fit(\n",
    "    train_image_feature,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [EARLY_STOPPING],\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    class_weight=class_weight_asm,\n",
    "    verbose = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbe8d23-6562-496b-88f8-ce2940716b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(simple_cnn_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ebb40-563d-4eb7-8dff-b9325311b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on the test dataset\n",
    "simple_cnn_results = simple_cnn_model.evaluate(\n",
    "    test_image_feature, \n",
    "    test_labels, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    verbose=1)\n",
    "\n",
    "display_results(simple_cnn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59acce-8448-4ba7-a628-cd913dbfc62c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70faf688-173b-4b2e-882f-47eb3f71443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_simple_rnn_model():\n",
    "\n",
    "    nb_dim_in_genomic_seq = train_image_feature.shape[2] # Should be 3\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Normalize the features\n",
    "    model.add(\n",
    "        layers.BatchNormalization(\n",
    "            axis=-1,\n",
    "            momentum = 0.99,\n",
    "            epsilon = 0.001,\n",
    "            input_shape=(train_image_feature.shape[1], train_image_feature.shape[2])\n",
    "        )\n",
    "    )  \n",
    "    \n",
    "    model.add(layers.LSTM(RNN_UNITS, input_shape=(None, nb_dim_in_genomic_seq), go_backwards = True))\n",
    "\n",
    "    # Output layer (Sigmoid)\n",
    "    model.add(layers.Dense(1, activation='sigmoid', \n",
    "                         kernel_regularizer = keras.regularizers.L1L2(\n",
    "                            l1 = L1_R, \n",
    "                            l2 = L2_R)\n",
    "                         ))\n",
    "\n",
    "    model.compile(\n",
    "      optimizer = keras.optimizers.Adam(learning_rate =LEARNING_RATE),\n",
    "      loss = 'binary_crossentropy',\n",
    "      metrics = METRICS)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3372d-f038-4344-b4c1-8757e5cad58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_model = make_simple_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b5625-732d-40cc-8e7b-ea0588cf4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_model.summary()\n",
    "keras.utils.plot_model(simple_rnn_model, \"cnn_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bee2d1-7c8a-4b27-aa24-412f7cf1a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rnn_training = simple_rnn_model.fit(\n",
    "    train_image_feature,\n",
    "    train_labels,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [EARLY_STOPPING],\n",
    "    validation_split = VALIDATION_SPLIT,\n",
    "    class_weight=class_weight_asm,\n",
    "    verbose = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396c444-e534-484c-a0f4-94a9b03dea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(simple_rnn_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905674c1-98d1-4d77-9884-4d0f26b09785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on the test dataset\n",
    "simple_rnn_results = simple_rnn_model.evaluate(\n",
    "    test_image_feature, \n",
    "    test_labels, \n",
    "    batch_size = BATCH_SIZE,\n",
    "    verbose=1)\n",
    "\n",
    "display_results(simple_rnn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6597e7ea-b54e-4733-bd98-bdaf014a46c2",
   "metadata": {},
   "source": [
    "## Save model results into CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8eed8e-1bef-4e96-a29c-a6a576232ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS FOR WHICH WE NEED TO RECORD THE RESULTS\n",
    "\n",
    "models = ['linear', 'perceptron', 'simple_cnn', 'simple_rnn', 'simple_tree', 'boosted_tree']\n",
    "#models = ['simple_rnn']\n",
    "\n",
    "# Loss is better than AUC for monitoring\n",
    "PARAM_TO_CHANGE = \"baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a19f2-0a4e-47dd-a372-44c1ef7869c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize DF if it does not exist\n",
    "try: \n",
    "    model_results.head()\n",
    "    print(\"does not exist\")\n",
    "except AttributeError:\n",
    "    pass\n",
    "except NameError:\n",
    "    model_results = pd.DataFrame([\n",
    "                \"param_testing\", \"data_path\", \"nb_data_points\", \"genomic_region\", \"test_split\", \"val_split\", \n",
    "                \"stop_monitor\", \"stop_restore_w\", \n",
    "                \"epochs\", \"batch_size\",\n",
    "                \"L1_R\", \"L2_R\",\n",
    "                'activation_function', 'nb_nodes_perceptron',\n",
    "                'nb_layers_perceptron', 'nb_nodes_after_cnn',\n",
    "                'cnn_filters', 'cnn_kernel', 'RNN_units',\n",
    "                'growing_strat', 'num_trees', 'min_examples', 'max_depth', 'subsample', 'sampling_method',\n",
    "                'nb_params',\n",
    "                'loss', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', \n",
    "                'sensitivity', 'AUC'], columns = ['parameters'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f38cfd-aa26-4037-993f-c624e4f67afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame([\n",
    "                \"param_testing\", \"data_path\", \"nb_data_points\", \"genomic_region\", \"test_split\", \"val_split\", \n",
    "                \"stop_monitor\", \"stop_restore_w\", \n",
    "                \"epochs\", \"batch_size\",\n",
    "                \"L1_R\", \"L2_R\",\n",
    "                'activation_function', 'nb_nodes_perceptron',\n",
    "                'nb_layers_perceptron', 'nb_nodes_after_cnn',\n",
    "                'cnn_filters', 'cnn_kernel', 'RNN_units',\n",
    "                'growing_strat', 'num_trees', 'min_examples', 'max_depth', 'subsample', 'sampling_method',\n",
    "                'nb_params',\n",
    "                'loss', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', \n",
    "                'sensitivity', 'AUC'], columns = ['parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c1239-2544-404b-920b-154ee451d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "  print(\"model:\", model) \n",
    "  name_results = model + \"_results\"\n",
    "  name_model = model + \"_model\"\n",
    "\n",
    "  command_nb_params = \"nb_params = \" + name_model + \".count_params()\"\n",
    "  exec(command_nb_params)\n",
    "\n",
    "  # Parameters common to all models\n",
    "  common_param = pd.DataFrame([\n",
    "     PARAM_TO_CHANGE,\n",
    "     PROCESSED_DATA_PATH,\n",
    "     normalized_df.shape[0],\n",
    "     GENOMIC_INTERVAL,\n",
    "      TEST_SPLIT,\n",
    "     VALIDATION_SPLIT,\n",
    "     EARLY_STOPPING.monitor,\n",
    "     EARLY_STOPPING.restore_best_weights,\n",
    "     EPOCHS,\n",
    "     BATCH_SIZE,\n",
    "     L1_R,\n",
    "     L2_R,\n",
    "     ACTIVATION_FUNCTION,\n",
    "     NB_NODES_PERCEPTRON,\n",
    "     NB_LAYERS_PERCEPTRON,\n",
    "     NB_NODES_AFTER_CNN,\n",
    "     CNN_FILTERS,\n",
    "     CNN_KERNEL,\n",
    "     RNN_UNITS,\n",
    "     GROWING_STRATEGY,\n",
    "     NUM_TREES,\n",
    "     MIN_EXAMPLES,\n",
    "     MAX_DEPTH,\n",
    "     SUBSAMPLE,\n",
    "     SAMPLING_METHOD,\n",
    "     nb_params])\n",
    "\n",
    "  # Create dataframe from the model results\n",
    "  if \"tree\" in model:\n",
    "      #print(\"Found a tree\")\n",
    "      command_df_new_results = \"tmp = pd.DataFrame(np.round(list(\" + name_results + \".values()), 3))\"\n",
    "  else:\n",
    "      #print(\"Not a tree\")\n",
    "      command_df_new_results = \"tmp = pd.DataFrame(np.round(\" + name_results + \", 3))\"\n",
    "  exec(command_df_new_results)\n",
    "  #print(tmp)\n",
    "\n",
    "  # Append the two dataframes\n",
    "  new_column = common_param.append(tmp, ignore_index = True)\n",
    "\n",
    "  # Rename the dataframe\n",
    "  new_column.columns = [model]\n",
    "  #print(new_column)\n",
    "\n",
    "  # Add the results to the dataframe of results\n",
    "  model_results = model_results.merge(new_column, left_index = True, right_index = True)\n",
    " \n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0818d9d3-5ac2-4b14-94ea-7c786eb0c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.to_csv('model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da651f4c-53ed-4576-a78a-81a7d2cbbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tree_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8381b-990e-4332-8a2b-bd331684fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb47ebb-0f73-4a33-8dff-59cd994615df",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(simple_tree_results.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c2e7a-7402-49fa-9b24-d94cc5d680a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(np.round(simple_tree_results, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e1f0f-990f-4203-be19-ee8f96730c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
