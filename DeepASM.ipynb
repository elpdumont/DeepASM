{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71a3685-b41b-4dd7-8987-fb26729209a6",
   "metadata": {},
   "source": [
    "# DeepASM\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9eb55e93-2391-4791-bca2-a69b4ec7a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Python packages for data, stats, and visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Kernel functions\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from numpy import asarray\n",
    "from matplotlib import pyplot\n",
    "from numpy import exp\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA, KernelPCA, NMF, TruncatedSVD\n",
    "from sklearn.manifold import TSNE, LocallyLinearEmbedding, SpectralEmbedding\n",
    "\n",
    "\n",
    " \n",
    "# Figure parameters\n",
    "mpl.rcParams['figure.figsize'] = (10, 10)\n",
    "mpl.rcParams['axes.titlesize'] = 15\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "683ebf71-130f-4ea0-8eaa-c27478ebb990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) \n",
      "[GCC 9.4.0]\n",
      "TensorFlow version: 2.7.0\n",
      "Keras version: 2.7.0\n",
      "Numpy version: 1.19.5\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# Print different versions\n",
    "print(sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abddb628-6484-4f52-9386-ced2fb0f456a",
   "metadata": {},
   "source": [
    "## GCP Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb33deb-7b19-4795-acd3-4511eb464de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw data from bucket. False if you want to import the processed dataset\n",
    "IMPORT_RAW_FROM_BUCKET = True\n",
    "\n",
    "# Export data after it's been prepared\n",
    "EXPORT_PROCESSED_DATA = False\n",
    "\n",
    "# Bucket name where the training datasets are\n",
    "DEEPASM_BUCKET=\"deepasm\"\n",
    "\n",
    "# GCP variable\n",
    "import os\n",
    "projectid = 'hackensack-tyco'\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = 'hackensack-tyco'\n",
    "cloud_bucket = \"gs://deepasm/colab\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb144c7-b3d9-4ca7-836b-027939589d48",
   "metadata": {},
   "source": [
    "## Model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d49f672e-93d9-485a-9914-010650158ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS FOR WHICH WE NEED TO RECORD THE RESULTS\n",
    "\n",
    "models = ['linear', 'perceptron', 'simple_cnn',  'cnn', 'simple_rnn', 'rnn']\n",
    "#models = ['simple_rnn']\n",
    "\n",
    "# Loss is better than AUC for monitoring\n",
    "PARAM_TO_CHANGE = \"keep_chr\"\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Parameters common to all models\n",
    "\n",
    "# Number of rows to take into the dataset after import\n",
    "NB_ROWS_RAW_DATASET = int(0.2e6) # The maximum is 5e6. We use 200k to test the code\n",
    "\n",
    "# Minimum correlation factor\n",
    "MIN_CORR = 0.03\n",
    "\n",
    "# Size of the genomic window\n",
    "GENOMIC_INTERVAL = 1000\n",
    "\n",
    "# Kernel values for probability estimates\n",
    "KERNEL_FM_NB_VALUES = 10\n",
    "KERNEL_FM_BANDWIDTH = 0.1\n",
    "KERNEL_COV_NB_MAX = 200\n",
    "KERNEL_COV_NB_STEP = 40\n",
    "KERNEL_COV_BANDWIDTH = 20\n",
    "\n",
    "# Normalization method\n",
    "norm_method = \"z_score\" # \"min_max\" or \"z_score\"\n",
    "\n",
    "# Keep the chromosomes in the model (correlate poorly with ASM)\n",
    "KEEP_CHR_IN_MODEL = False\n",
    "\n",
    "# Early stopping\n",
    "EARLY_STOPPING = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=0,\n",
    "    patience=5,\n",
    "    mode='auto',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "# Percentage of data points to be used in the Test dataset\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "# Percentage of datapoints used between training and validation\n",
    "VALIDATION_SPLIT = 0.3 # How to divide the training dataset for validation\n",
    "\n",
    "EPOCHS = 100 # We have so many datapoints that 20 epochs are enough to stabilize the training\n",
    "BATCH_SIZE = 1000 # to get a few identified ASM we need at a few hundreds since the\n",
    "# frequency of ASM is 1.38%\n",
    "# A batch size of 1000 will run into a memory error on TF 2.7\n",
    "\n",
    "# Regularlization L1 and L2 (defaults are l1 = 0.01 and l2 = 0.01)\n",
    "L1_R = 0\n",
    "L2_R = 1e-3\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Parameters common to neural network models\n",
    "ACTIVATION_FUNCTION = 'relu' # 'tanh' # or 'relu' or 'gelu (Gaussian Error Linear Unit)'\n",
    "NB_NODES_PERCEPTRON = 10\n",
    "NB_LAYERS_PERCEPTRON = 5\n",
    "NB_NODES_AFTER_CNN = 2\n",
    "CNN_FILTERS = 8\n",
    "CNN_KERNEL = 100 # Must be smaller than the genomic region (250). The av distance between CpG is 37 bp and the std dev of the distances between cpgs is 24 bp\n",
    "LEARNING_RATE = 3e-4 \n",
    "\n",
    "# Learning rate was taken from this\n",
    "# http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines\n",
    "\n",
    "#--------------------------------------------------\n",
    "# Parameters common to RNN\n",
    "\n",
    "RNN_UNITS = 64 # 64 orginally\n",
    "\n",
    "#--------------------------------------------------\n",
    "# SPECIFIC TO RANDOM FOREST ALGORITHM\n",
    "use_raw_df_for_forest_models = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22515ca-30c7-4a83-a982-6eb67b884f37",
   "metadata": {},
   "source": [
    "## ML evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f38b202-fe69-489c-aae0-638bcd40b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 03:09:04.493537: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-08 03:09:06.651648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38444 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='sensitivity'),\n",
    "      keras.metrics.AUC(name='auc')\n",
    "      ]\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'auc', 'precision', 'sensitivity']\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def display_results(df_results):\n",
    "  print(\"Loss\", np.round(df_results[0], 3))\n",
    "  print(\"True positives\", np.round(df_results[1], 3))\n",
    "  print(\"False positives\", np.round(df_results[2], 3))\n",
    "  print(\"True negatives\", np.round(df_results[3], 3))\n",
    "  print(\"False negatives\", np.round(df_results[4], 3))\n",
    "  print(\"Accuracy\", np.round(df_results[5], 3))\n",
    "  print(\"Precision\", np.round(df_results[6], 3))\n",
    "  print(\"Sensitivity\", np.round(df_results[7], 3))\n",
    "  print(\"AUC\", np.round(df_results[8], 3))\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.xlim([-0.5,80])\n",
    "  plt.ylim([0,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e5515-6785-4461-b139-ced8487e2eee",
   "metadata": {},
   "source": [
    "## Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d440a700-3f8d-443b-87f4-3036f2a0f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the file as: training_0.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000000.json...\n",
      "- [1 files][257.9 MiB/257.9 MiB]                                                \n",
      "Operation completed over 1 objects/257.9 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_1.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000001.json...\n",
      "/ [1 files][258.0 MiB/258.0 MiB]                                                \n",
      "Operation completed over 1 objects/258.0 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_2.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000002.json...\n",
      "- [1 files][257.3 MiB/257.3 MiB]                                                \n",
      "Operation completed over 1 objects/257.3 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_3.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000003.json...\n",
      "- [1 files][258.0 MiB/258.0 MiB]                                                \n",
      "Operation completed over 1 objects/258.0 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_4.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000004.json...\n",
      "/ [1 files][258.4 MiB/258.4 MiB]                                                \n",
      "Operation completed over 1 objects/258.4 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_5.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000005.json...\n",
      "- [1 files][257.5 MiB/257.5 MiB]                                                \n",
      "Operation completed over 1 objects/257.5 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_6.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000006.json...\n",
      "| [1 files][257.6 MiB/257.6 MiB]                                                \n",
      "Operation completed over 1 objects/257.6 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_7.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000007.json...\n",
      "/ [1 files][257.8 MiB/257.8 MiB]                                                \n",
      "Operation completed over 1 objects/257.8 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_8.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000008.json...\n",
      "/ [1 files][257.6 MiB/257.6 MiB]                                                \n",
      "Operation completed over 1 objects/257.6 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_9.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000009.json...\n",
      "/ [1 files][256.8 MiB/256.8 MiB]                                                \n",
      "Operation completed over 1 objects/256.8 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_10.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000010.json...\n",
      "/ [1 files][258.0 MiB/258.0 MiB]                                                \n",
      "Operation completed over 1 objects/258.0 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_11.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000011.json...\n",
      "/ [1 files][258.9 MiB/258.9 MiB]                                                \n",
      "Operation completed over 1 objects/258.9 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_12.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000012.json...\n",
      "- [1 files][257.4 MiB/257.4 MiB]                                                \n",
      "Operation completed over 1 objects/257.4 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_13.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000013.json...\n",
      "/ [1 files][258.0 MiB/258.0 MiB]                                                \n",
      "Operation completed over 1 objects/258.0 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_14.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000014.json...\n",
      "/ [1 files][258.5 MiB/258.5 MiB]                                                \n",
      "Operation completed over 1 objects/258.5 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_15.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000015.json...\n",
      "- [1 files][257.6 MiB/257.6 MiB]                                                \n",
      "Operation completed over 1 objects/257.6 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_16.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000016.json...\n",
      "/ [1 files][258.0 MiB/258.0 MiB]                                                \n",
      "Operation completed over 1 objects/258.0 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_17.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000017.json...\n",
      "/ [1 files][257.4 MiB/257.4 MiB]                                                \n",
      "Operation completed over 1 objects/257.4 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_18.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000018.json...\n",
      "- [1 files][257.5 MiB/257.5 MiB]                                                \n",
      "Operation completed over 1 objects/257.5 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_19.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000019.json...\n",
      "/ [1 files][257.5 MiB/257.5 MiB]                                                \n",
      "Operation completed over 1 objects/257.5 MiB.                                    \n",
      "Appending file...\n",
      "Downloading the file as: training_20.json\n",
      "Copying gs://deepasm/1000bp/encode_training_data/encode_training-000000000020.json...\n",
      "/ [1 files][257.9 MiB/257.9 MiB]                                                \n",
      "Operation completed over 1 objects/257.9 MiB.                                    \n",
      "Appending file...\n"
     ]
    }
   ],
   "source": [
    "if IMPORT_RAW_FROM_BUCKET == True:\n",
    "    !gsutil ls gs://$DEEPASM_BUCKET/$GENOMIC_INTERVAL*bp/encode_training_data/*.json > list_to_download.txt\n",
    "    files_to_download_df = pd.read_csv('list_to_download.txt', header=None)\n",
    "\n",
    "    imported_df = pd.DataFrame()\n",
    "\n",
    "    for index_file in range(files_to_download_df.shape[0]):\n",
    "        file_name_bucket = files_to_download_df[0][index_file]\n",
    "        local_file_name = \"training_\" + str(index_file) + \".json\"\n",
    "        \n",
    "        # Download the file from bucket\n",
    "        !gsutil cp $file_name_bucket $local_file_name\n",
    "        \n",
    "        print(\"Appending file...\")\n",
    "        imported_df = imported_df.append(pd.read_json(local_file_name, lines = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f83a257f-e990-4f89-bec0-0011061db225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the imported dataset: (4509754, 23)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the imported dataset:\", imported_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44574e-a6e1-4e5b-b254-bcf6959d60a0",
   "metadata": {},
   "source": [
    "## Prepare the features\n",
    "\n",
    "Note: we do not randomize the rows because the scripts preceding this notebook already sampled the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f096d-c8fd-4270-b822-97fd20c94331",
   "metadata": {},
   "source": [
    "### Copy & clean dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b1bbac4-bc09-4786-b688-b8976eeac8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the dataframe\n",
    "raw_df = imported_df.copy()\n",
    "raw_df = raw_df.head(NB_ROWS_RAW_DATASET)\n",
    "\n",
    "# We drop columns that are not going to be features\n",
    "raw_df.drop('sample', axis = 1, inplace = True)\n",
    "raw_df.drop('snp_id', axis = 1, inplace = True)\n",
    "raw_df.drop('snp_pos', axis = 1, inplace = True)\n",
    "raw_df.drop('wilcoxon_corr_pvalue', axis = 1, inplace = True)\n",
    "raw_df.drop('asm_region_effect', axis = 1, inplace = True)\n",
    "\n",
    "# We remove the chromosomes X and Y (no ASM)\n",
    "raw_df = raw_df.loc[raw_df['chr'] != 'X']\n",
    "raw_df = raw_df.loc[raw_df['chr'] != 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26e8e447-ea95-4b96-a467-19606d3f0f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataset:  (197237, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the dataset: \", raw_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0c078-541c-4563-a68a-4dd87bb34e5f",
   "metadata": {},
   "source": [
    "### Calculate the distance between CpGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1f2362ad-6e00-4d94-961d-8081ada1d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the distance between CpGs (~3min)\n",
    "def dist_cpg(cpg_pos):\n",
    "  distances = []\n",
    "  for index in range(len(cpg_pos)):\n",
    "    if index >= len(cpg_pos)-1:\n",
    "      return distances\n",
    "    else:\n",
    "      distances.append(cpg_pos[index + 1] - cpg_pos[index])\n",
    "  return distances\n",
    "\n",
    "# Apply the function \"distance\" to the array of CpG positions\n",
    "raw_df['cpg_dist'] = raw_df['cpg_pos'].apply(lambda x: dist_cpg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe7a2cbb-37a8-4e15-b420-e61802e539c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asm_snp</th>\n",
       "      <th>sample_category</th>\n",
       "      <th>chr</th>\n",
       "      <th>region_inf</th>\n",
       "      <th>region_sup</th>\n",
       "      <th>region_nb_cpg</th>\n",
       "      <th>nb_cpg_found</th>\n",
       "      <th>nb_reads</th>\n",
       "      <th>dnase</th>\n",
       "      <th>encode_ChiP_V2</th>\n",
       "      <th>tf_motifs</th>\n",
       "      <th>global_cpg_fm</th>\n",
       "      <th>tot_nb_cpg</th>\n",
       "      <th>tot_nb_reads</th>\n",
       "      <th>read_fm</th>\n",
       "      <th>cpg_fm</th>\n",
       "      <th>cpg_cov</th>\n",
       "      <th>cpg_pos</th>\n",
       "      <th>cpg_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>119313001</td>\n",
       "      <td>119314000</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632</td>\n",
       "      <td>24030712</td>\n",
       "      <td>420944333</td>\n",
       "      <td>[0, 0, 1, 0.5, 0.667, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>[0.463, 0.981, 1, 0.796, 0.927, 0.943000000000...</td>\n",
       "      <td>[54, 53, 58, 54, 55, 53]</td>\n",
       "      <td>[119313416, 119313421, 119313464, 119313512, 1...</td>\n",
       "      <td>[5, 43, 48, 2, 448]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211213001</td>\n",
       "      <td>211214000</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632</td>\n",
       "      <td>24030712</td>\n",
       "      <td>420944333</td>\n",
       "      <td>[1, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0.25, 0, 0...</td>\n",
       "      <td>[0.182, 0.2, 0.23800000000000002, 0.2380000000...</td>\n",
       "      <td>[22, 10, 21, 21, 19, 21, 15, 14, 23, 10, 34, 1...</td>\n",
       "      <td>[211213024, 211213259, 211213269, 211213313, 2...</td>\n",
       "      <td>[235, 10, 44, 14, 31, 35, 8, 4, 44, 106, 58, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17661001</td>\n",
       "      <td>17662000</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>181</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632</td>\n",
       "      <td>24030712</td>\n",
       "      <td>420944333</td>\n",
       "      <td>[1, 0, 0, 0, 1, 1, 1, 1, 0.75, 1, 1, 0.667, 1,...</td>\n",
       "      <td>[0.857, 0.794, 0.967, 0.862, 0.677, 0.903, 0.9...</td>\n",
       "      <td>[28, 34, 61, 29, 31, 31, 32, 31, 33, 44, 40, 4...</td>\n",
       "      <td>[17661167, 17661172, 17661178, 17661234, 17661...</td>\n",
       "      <td>[5, 6, 56, 20, 4, 70, 32, 28, 239, 146, 6, 34,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>55314001</td>\n",
       "      <td>55315000</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.632</td>\n",
       "      <td>24030712</td>\n",
       "      <td>420944333</td>\n",
       "      <td>[1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "      <td>[0.444, 0.975, 0.97, 0.714, 1, 1, 1, 0.976, 0....</td>\n",
       "      <td>[27, 40, 33, 28, 31, 38, 38, 41, 29, 33, 65]</td>\n",
       "      <td>[55314039, 55314070, 55314084, 55314091, 55314...</td>\n",
       "      <td>[31, 14, 7, 4, 43, 2, 4, 4, 412, 195]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>81647001</td>\n",
       "      <td>81648000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>209</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.632</td>\n",
       "      <td>24030712</td>\n",
       "      <td>420944333</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0.667, 0, 1, 0.25,...</td>\n",
       "      <td>[0.191, 0.13, 0.396, 0.396, 0.341, 0.245, 0.42...</td>\n",
       "      <td>[47, 46, 48, 48, 44, 53, 63, 36]</td>\n",
       "      <td>[81647176, 81647266, 81647334, 81647369, 81647...</td>\n",
       "      <td>[90, 68, 35, 8, 153, 78, 208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>24299001</td>\n",
       "      <td>24300000</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>95</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.647</td>\n",
       "      <td>25049829</td>\n",
       "      <td>535812384</td>\n",
       "      <td>[0, 0, 0.5, 0.5, 0.14300000000000002, 0, 0, 0....</td>\n",
       "      <td>[0.222, 0.467, 0.063, 0.08700000000000001, 0, ...</td>\n",
       "      <td>[27, 15, 16, 23, 30, 28, 27, 13, 19, 24, 26, 1...</td>\n",
       "      <td>[24299132, 24299154, 24299221, 24299227, 24299...</td>\n",
       "      <td>[22, 67, 6, 8, 55, 15, 25, 92, 6, 30, 15, 178,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>134148001</td>\n",
       "      <td>134149000</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>202</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.646</td>\n",
       "      <td>22540136</td>\n",
       "      <td>527040401</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 0.917, 0.9570000000000001, 0.9570000000000...</td>\n",
       "      <td>[36, 12, 47, 23, 30, 14, 62, 60, 31, 40, 35, 3...</td>\n",
       "      <td>[134148115, 134148133, 134148135, 134148146, 1...</td>\n",
       "      <td>[18, 2, 11, 40, 38, 18, 55, 121, 47, 23, 40, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>50708001</td>\n",
       "      <td>50709000</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.646</td>\n",
       "      <td>22540136</td>\n",
       "      <td>527040401</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0.333, 0, 0, 0.5, 0, 0, 0, ...</td>\n",
       "      <td>[0.86, 0.015, 0.016, 0, 0.022, 0.5660000000000...</td>\n",
       "      <td>[50, 65, 63, 48, 46, 53, 55, 43, 47]</td>\n",
       "      <td>[50708082, 50708248, 50708388, 50708405, 50708...</td>\n",
       "      <td>[166, 140, 17, 47, 173, 64, 213, 26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>74537001</td>\n",
       "      <td>74538000</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.646</td>\n",
       "      <td>22540136</td>\n",
       "      <td>527040401</td>\n",
       "      <td>[1, 1, 0, 0.333, 0, 0, 0.5, 0.333, 0, 1, 1, 0....</td>\n",
       "      <td>[0, 0.13, 0.87, 0.973, 0, 0.043000000000000003...</td>\n",
       "      <td>[20, 23, 23, 37, 17, 23, 23, 11, 23, 34, 20, 3...</td>\n",
       "      <td>[74537027, 74537068, 74537092, 74537147, 74537...</td>\n",
       "      <td>[41, 24, 55, 9, 112, 4, 12, 93, 63, 22, 44, 81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>56046001</td>\n",
       "      <td>56047000</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.646</td>\n",
       "      <td>22540136</td>\n",
       "      <td>527040401</td>\n",
       "      <td>[1, 0.667, 1, 0.8, 1, 1, 1, 0.75, 1, 0.667, 0....</td>\n",
       "      <td>[1, 0.929, 0.905, 1, 1, 1, 0.8260000000000001,...</td>\n",
       "      <td>[10, 14, 21, 14, 12, 10, 23, 18, 20, 15, 21, 1...</td>\n",
       "      <td>[56046069, 56046109, 56046497, 56046532, 56046...</td>\n",
       "      <td>[40, 388, 35, 6, 20, 28, 8, 31, 89, 54, 25, 21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197237 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        asm_snp  sample_category chr  region_inf  region_sup  region_nb_cpg  \\\n",
       "0             0                1   1   119313001   119314000              8   \n",
       "1             0                1   1   211213001   211214000             16   \n",
       "2             0                1   4    17661001    17662000             16   \n",
       "3             0                1   5    55314001    55315000             11   \n",
       "4             0                1   1    81647001    81648000              8   \n",
       "...         ...              ...  ..         ...         ...            ...   \n",
       "199995        0                1  20    24299001    24300000             17   \n",
       "199996        0                1   9   134148001   134149000             28   \n",
       "199997        0                1  11    50708001    50709000             11   \n",
       "199998        0                1  17    74537001    74538000             23   \n",
       "199999        0                1  18    56046001    56047000             28   \n",
       "\n",
       "        nb_cpg_found  nb_reads  dnase  encode_ChiP_V2  tf_motifs  \\\n",
       "0                  6       141      0               0          0   \n",
       "1                 14       182      0               0          0   \n",
       "2                 15       181      2               0          1   \n",
       "3                 11       141      1               0          2   \n",
       "4                  8       209      1               0          0   \n",
       "...              ...       ...    ...             ...        ...   \n",
       "199995            16        95      3               1         12   \n",
       "199996            28       202      3               6          8   \n",
       "199997             9       266      0               0          5   \n",
       "199998            21       130      1               9         20   \n",
       "199999            18        78      4               0         17   \n",
       "\n",
       "        global_cpg_fm  tot_nb_cpg  tot_nb_reads  \\\n",
       "0               0.632    24030712     420944333   \n",
       "1               0.632    24030712     420944333   \n",
       "2               0.632    24030712     420944333   \n",
       "3               0.632    24030712     420944333   \n",
       "4               0.632    24030712     420944333   \n",
       "...               ...         ...           ...   \n",
       "199995          0.647    25049829     535812384   \n",
       "199996          0.646    22540136     527040401   \n",
       "199997          0.646    22540136     527040401   \n",
       "199998          0.646    22540136     527040401   \n",
       "199999          0.646    22540136     527040401   \n",
       "\n",
       "                                                  read_fm  \\\n",
       "0       [0, 0, 1, 0.5, 0.667, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
       "1       [1, 0, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0.25, 0, 0...   \n",
       "2       [1, 0, 0, 0, 1, 1, 1, 1, 0.75, 1, 1, 0.667, 1,...   \n",
       "3       [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...   \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.667, 0, 1, 0.25,...   \n",
       "...                                                   ...   \n",
       "199995  [0, 0, 0.5, 0.5, 0.14300000000000002, 0, 0, 0....   \n",
       "199996  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "199997  [0, 1, 0, 1, 0, 0, 0.333, 0, 0, 0.5, 0, 0, 0, ...   \n",
       "199998  [1, 1, 0, 0.333, 0, 0, 0.5, 0.333, 0, 1, 1, 0....   \n",
       "199999  [1, 0.667, 1, 0.8, 1, 1, 1, 0.75, 1, 0.667, 0....   \n",
       "\n",
       "                                                   cpg_fm  \\\n",
       "0       [0.463, 0.981, 1, 0.796, 0.927, 0.943000000000...   \n",
       "1       [0.182, 0.2, 0.23800000000000002, 0.2380000000...   \n",
       "2       [0.857, 0.794, 0.967, 0.862, 0.677, 0.903, 0.9...   \n",
       "3       [0.444, 0.975, 0.97, 0.714, 1, 1, 1, 0.976, 0....   \n",
       "4       [0.191, 0.13, 0.396, 0.396, 0.341, 0.245, 0.42...   \n",
       "...                                                   ...   \n",
       "199995  [0.222, 0.467, 0.063, 0.08700000000000001, 0, ...   \n",
       "199996  [1, 0.917, 0.9570000000000001, 0.9570000000000...   \n",
       "199997  [0.86, 0.015, 0.016, 0, 0.022, 0.5660000000000...   \n",
       "199998  [0, 0.13, 0.87, 0.973, 0, 0.043000000000000003...   \n",
       "199999  [1, 0.929, 0.905, 1, 1, 1, 0.8260000000000001,...   \n",
       "\n",
       "                                                  cpg_cov  \\\n",
       "0                                [54, 53, 58, 54, 55, 53]   \n",
       "1       [22, 10, 21, 21, 19, 21, 15, 14, 23, 10, 34, 1...   \n",
       "2       [28, 34, 61, 29, 31, 31, 32, 31, 33, 44, 40, 4...   \n",
       "3            [27, 40, 33, 28, 31, 38, 38, 41, 29, 33, 65]   \n",
       "4                        [47, 46, 48, 48, 44, 53, 63, 36]   \n",
       "...                                                   ...   \n",
       "199995  [27, 15, 16, 23, 30, 28, 27, 13, 19, 24, 26, 1...   \n",
       "199996  [36, 12, 47, 23, 30, 14, 62, 60, 31, 40, 35, 3...   \n",
       "199997               [50, 65, 63, 48, 46, 53, 55, 43, 47]   \n",
       "199998  [20, 23, 23, 37, 17, 23, 23, 11, 23, 34, 20, 3...   \n",
       "199999  [10, 14, 21, 14, 12, 10, 23, 18, 20, 15, 21, 1...   \n",
       "\n",
       "                                                  cpg_pos  \\\n",
       "0       [119313416, 119313421, 119313464, 119313512, 1...   \n",
       "1       [211213024, 211213259, 211213269, 211213313, 2...   \n",
       "2       [17661167, 17661172, 17661178, 17661234, 17661...   \n",
       "3       [55314039, 55314070, 55314084, 55314091, 55314...   \n",
       "4       [81647176, 81647266, 81647334, 81647369, 81647...   \n",
       "...                                                   ...   \n",
       "199995  [24299132, 24299154, 24299221, 24299227, 24299...   \n",
       "199996  [134148115, 134148133, 134148135, 134148146, 1...   \n",
       "199997  [50708082, 50708248, 50708388, 50708405, 50708...   \n",
       "199998  [74537027, 74537068, 74537092, 74537147, 74537...   \n",
       "199999  [56046069, 56046109, 56046497, 56046532, 56046...   \n",
       "\n",
       "                                                 cpg_dist  \n",
       "0                                     [5, 43, 48, 2, 448]  \n",
       "1       [235, 10, 44, 14, 31, 35, 8, 4, 44, 106, 58, 1...  \n",
       "2       [5, 6, 56, 20, 4, 70, 32, 28, 239, 146, 6, 34,...  \n",
       "3                   [31, 14, 7, 4, 43, 2, 4, 4, 412, 195]  \n",
       "4                           [90, 68, 35, 8, 153, 78, 208]  \n",
       "...                                                   ...  \n",
       "199995  [22, 67, 6, 8, 55, 15, 25, 92, 6, 30, 15, 178,...  \n",
       "199996  [18, 2, 11, 40, 38, 18, 55, 121, 47, 23, 40, 1...  \n",
       "199997               [166, 140, 17, 47, 173, 64, 213, 26]  \n",
       "199998  [41, 24, 55, 9, 112, 4, 12, 93, 63, 22, 44, 81...  \n",
       "199999  [40, 388, 35, 6, 20, 28, 8, 31, 89, 54, 25, 21...  \n",
       "\n",
       "[197237 rows x 19 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc6f7c-ccef-4681-a609-dbde5ac5e379",
   "metadata": {},
   "source": [
    "### Convert arrays into numerical features\n",
    "\n",
    "To do this, we use kernel estimates as well as simpler metrics like mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6653f9e8-b3a9-4077-8bd9-52d25fd22626",
   "metadata": {},
   "source": [
    "#### Kernel function for Fractional Methylation (FM) for CpG and reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5452c4c-7ff6-43a5-97ec-da1ccbdee6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values used in kernel estimate: [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.9267, 1.8709, 0.5784, 0.2146, 0.2302, 0.2894, 0.2103, 0.1486,\n",
       "       0.1677, 0.3284, 0.4871])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values for fractional methylation (between 0 and 1)\n",
    "values_for_kernel_fm = asarray([value for value in range(0, KERNEL_FM_NB_VALUES+1)])\n",
    "values_for_kernel_fm = values_for_kernel_fm / KERNEL_FM_NB_VALUES\n",
    "print(\"X-axis values used for the FM kernel estimate:\", values_for_kernel_fm)\n",
    "values_for_kernel_fm = values_for_kernel_fm.reshape((len(values_for_kernel_fm), 1))\n",
    "\n",
    "# Build Kernel model\n",
    "kernel_fm_model = KernelDensity(bandwidth=KERNEL_FM_BANDWIDTH, kernel='gaussian')\n",
    "\n",
    "# Function to be applied to each array in the columns read_fm and cpg_fm\n",
    "def estimate_kernels_fm(x):\n",
    "  sample = np.reshape(x, (len(x), 1))\n",
    "  kernel_fm_model.fit(sample)\n",
    "  probabilities = kernel_fm_model.score_samples(values_for_kernel_fm)\n",
    "  probabilities = exp(probabilities)\n",
    "  return np.round(probabilities, 4)\n",
    "\n",
    "# Try function\n",
    "estimate_kernels_fm(raw_df['read_fm'][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c2085-399f-4de8-9e4b-55b267e182f9",
   "metadata": {},
   "source": [
    "#### Kernel function for CpG coverage and the distance between CpGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97c6d384-d6a5-4ad9-bc40-74584337290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values used in kernel estimate: [  0  40  80 120 160]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values for fractional methylation (between 0 and 1)\n",
    "values_for_kernel_cov = asarray([value for value in range(0, KERNEL_COV_NB_MAX, KERNEL_COV_NB_STEP)])\n",
    "print(\"Values used in kernel estimate:\", values_for_kernel_cov)\n",
    "values_for_kernel_cov = values_for_kernel_cov.reshape((len(values_for_kernel_cov), 1))\n",
    "\n",
    "# Build Kernel model\n",
    "kernel_cov_model = KernelDensity(bandwidth=KERNEL_COV_BANDWIDTH, kernel='gaussian')\n",
    "\n",
    "# Function to be applied to each array in the columns read_fm and cpg_fm\n",
    "def estimate_kernels_cov(x):\n",
    "  sample = np.reshape(x, (len(x), 1))\n",
    "  kernel_fm_model.fit(sample)\n",
    "  probabilities = kernel_fm_model.score_samples(values_for_kernel_cov)\n",
    "  probabilities = exp(probabilities)\n",
    "  return np.round(probabilities, 4)\n",
    "\n",
    "# Try function\n",
    "estimate_kernels_cov(raw_df['cpg_cov'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2574d-900f-44a9-871e-5ea49c7afb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
